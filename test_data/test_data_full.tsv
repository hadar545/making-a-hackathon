4	"standard poodle
Mexican hairless
timber wolf, grey wolf, gray wolf, Canis lupus
white wolf, Arctic wolf, Canis lupus tundrarum
red wolf, maned wolf, Canis rufus, Canis niger
"
2	"                    test_horovod_python_path = ['horovod', 'horovod/python']
                    test_env['HOROVOD_SPARK_PYTHONPATH'] = os.pathsep.join(test_horovod_python_path)

                with override_env(test_env):
"
2	"  agents:
    queue: cpu
- label: ':docker: Build test-cpu-openmpi-py3_6-tf1_14_0-keras2_2_4-torch1_2_0-mxnet1_4_1-pyspark2_4_0'
  plugins:
"
4	"English springer, English springer spaniel
Welsh springer spaniel
cocker spaniel, English cocker spaniel, cocker
Sussex spaniel
Irish water spaniel
"
1	"        """"""Make masks for masked self-attention.

        Args:
            olens (LongTensor or List): Batch of lengths (B,).
"
4	"turnstile
typewriter keyboard
umbrella
unicycle, monocycle
upright, upright piano
"
4	"#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
"
4	"            padding_type=self.padding_type,
            name='',
            conv_name='_conv_head',
"
4	"sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita
lorikeet
coucal
bee eater
hornbill
"
6	"    x1 = torch.randn(8, 16)
    x2 = torch.randn(4, 16)
    batch1 = torch.tensor([0, 0, 0, 0, 1, 1, 1, 1])
    batch2 = torch.tensor([0, 0, 1, 1])
"
2	"  agents:
    queue: 2x-gpu-g4
- label: ':tensorflow: Test TensorFlow Eager MNIST (test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c "" \$(cat /mpirun_command) python /horovod/examples/tensorflow_mnist_eager.py""
  plugins:
"
4	"    return img


def crop_image(img, target_size, center):
    width, height = img.size
"
4	"            trainable (bool): Set parameters in program to be trainable.
            pretrained (bool) : Whether to load pretrained model.
"
6	"            :obj:`num_bases` denotes the number of bases to use.
            (default: :obj:`None`)
"
4	"                    dtype='int64',
                    lod_level=0)
                emb_2 = fluid.embedding(
                    input=text_2,
"
4	"barrow, garden cart, lawn cart, wheelbarrow
baseball
basketball
bassinet
bassoon
"
1	"    for key, data in dataset:
        if key == ""a"":
            assert data[""data5""].shape == (100, 80,)
"
1	"        z_cache = self.decoder.init_state(x)
        while True:
            # update index
            idx += 1
"
6	"
        self.lin = Linear(out_channels, out_channels)

        self.reset_parameters()

"
4	"swimming trunks, bathing trunks
swing
switch, electric switch, electrical switch
syringe
"
4	"            attr=scale_param, shape=input.shape[1:2], dtype=dtype)
        offset = helper.create_parameter(
            attr=offset_param, shape=input.shape[1:2], dtype=dtype)
"
2	"        assert len(rank_results) == 2
        for rank, timestamps in rank_results.items():
            expected = 2 if rank == 0 else 0
            assert len(timestamps) == expected, rank
"
2	"    best_val_rmspe = min(history['val_exp_rmspe'])
    print('Best RMSPE: %f' % best_val_rmspe)
"
4	"cock
hen
ostrich, Struthio camelus
brambling, Fringilla montifringilla
goldfinch, Carduelis carduelis
"
2	"## [Unreleased] - YYYY-MM-DD

### Added

"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
4	"            help=""Return top k results."")

    def add_module_input_arg(self):
        """"""
        Add the command input options.
"
4	"cougar, puma, catamount, mountain lion, painter, panther, Felis concolor
lynx, catamount
leopard, Panthera pardus
snow leopard, ounce, Panthera uncia
"
2	"

"
2	"            self.assertEqual({'host-hash-1': 1, 'host-hash-2': 2}, slots)

"
4	"        if has_se:
            num_squeezed_channels = max(
                1, int(block_args.input_filters * block_args.se_ratio))
            conv = self.se_block(conv, num_squeezed_channels, oup, name)

"
2	"            if p in self._handles and self._handles[p][0] is not None:
                if self._allreduce_delay[p] <= 0:
                    raise AssertionError(
                        ""Gradients were computed more than ""
                        ""backward_passes_per_step times before call ""
"
3	"      backend: numpy
      compress_level: 1
      index_filename: wrap-npidx
      metric: euclidean
  space: l2
"
2	"                val_data_schema, val_rows, val_data_total_byte_size = _load_metadata_from_fs(fs,
                                                                                             val_data_meta_path)

"
4	"thunder snake, worm snake, Carphophis amoenus
ringneck snake, ring-necked snake, ring snake
hognose snake, puff adder, sand viper
"
4	"stupa, tope
submarine, pigboat, sub, U-boat
suit, suit of clothes
sundial
"
4	"black stork, Ciconia nigra
spoonbill
flamingo
little blue heron, Egretta caerulea
American egret, great white heron, Egretta albus
"
6	"            path = download_url(self.processed_url, self.raw_dir)
            extract_zip(path, self.raw_dir)
            os.unlink(path)
"
4	"### 查看代码

"
2	"            assert updated_slot_info.local_rank == slot_info.local_rank, rank
            assert updated_slot_info.cross_size == 1, rank
            assert updated_slot_info.cross_rank == 0, rank
"
1	"            fig.clf()

"
4	"kit fox, Vulpes macrotis
Arctic fox, white fox, Alopex lagopus
grey fox, gray fox, Urocyon cinereoargenteus
tabby, tabby cat
tiger cat
"
2	"    # Newly created optimizers will not have their state initialized, so
    # do that initialization here
    if len(state_dict['state']) == 0:
        for group in optimizer.param_groups:
"
1	"                        line = next(f)
                    except StopIteration:
"
6	"            modules = [Linear(in_channels, self.F_out)]
            for _ in range(pre_layers - 1):
                modules += [ReLU()]
                modules += [Linear(self.F_out, self.F_out)]
            self.post_nns.append(Sequential(*modules))
"
6	"        if isinstance(edge_index, Tensor):
            edge_index, edge_weight = gcn_norm(  # yapf: disable
                edge_index, edge_weight, x.size(self.node_dim),
                add_self_loops=False, dtype=x.dtype)
"
4	"        x_squeezed = conv2d(
            x_squeezed,
            num_filters=oup,
            filter_size=1,
"
4	"bottlecap
bow
bow tie, bow-tie, bowtie
"
4	"            each['org_im_width'], each['org_im_height'] = each['org_im'].size
            component.append(each)
    if images is not None:
"
4	"        conv = conv[:, :, 1:, 1:]

    if norm is not None:
        conv = norm_layer(input=conv, norm_type=norm, name=name + ""_norm"")
    if act == 'relu':
"
4	"African chameleon, Chamaeleo chamaeleon
Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis
African crocodile, Nile crocodile, Crocodylus niloticus
American alligator, Alligator mississipiensis
"
4	"            num_filters=out_channels,
            filter_size=3,
            stride=2,
            bn_act=None,
            bn_mom=self._bn_mom,
"
2	"        }
        with override_env(service_env):
            with spark_task_service(index=1) as (service, client, key):
"
2	"    """"""
    def __init__(self, model, optimizer=None, **kwargs):
"
2	"                                         **kwargs)

    def save(self):
        self._saved_model_state = copy.deepcopy(self.model.state_dict())
"
1	"                    MultiHeadedAttention(
                        attention_heads, attention_dim, src_attention_dropout_rate
                    ),
"
4	"            paths (list[str]): The paths of images.
            batch_size (int): batch size.
"
4	"bikini, two-piece
binder, ring-binder
"
2	"        def fn(*args):
            try:
                target(*args)
            except:
"
2	"
    def register_reset_callbacks(self, callbacks):
        """"""Register callbacks that will be invoked following a reset event (worker added or removed).

        For example, a common use of a reset callback would be to update the learning rate scale with the
"
1	"                        raise RuntimeError(f""{uid} is not found in the files"")
                    sps = line.rstrip().split(maxsplit=1)
                    if len(sps) != 2:
                        raise RuntimeError(
                            f""This line doesn't include a space:""
"
2	"                    if int(time.time()) - start > args.discovery_wait:
                        raise TimeoutError('Timed out waiting for notifications from driver.')
                    time.sleep(0.1)
"
1	"        --seed ${seed} \
        --train-json ${feat_tr_dir}/data.json \
"
2	"        else:
            logging.error('command failed: {}'.format(command))

"
4	"        depth_divisor=8,
        min_depth=None)
"
2	"        actual_env = execute_kwargs.get('env')
        actual_stdout = execute_kwargs.get('stdout')
        actual_stderr = execute_kwargs.get('stderr')
"
4	"meerkat, mierkat
tiger beetle
ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle
"
2	"    stop = threading.Event()
    events = events or []
    for event in events:
        on_event(event, exit_event.set, stop=stop, silent=True)
"
1	"        #               \        /
        #                 Linear
"
2	"    torch.manual_seed(1234)

    prev_zero = False
"
2	"
    def reset(self, size):
"
4	"# -*- coding:utf-8 -*-
# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
"
2	"      image-repository: 823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite
      cache-from: test-cpu-gloo-py3_8-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark3_0_0:823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite:SLUG-test-cpu-gloo-py3_8-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark3_0_0-latest
"
4	"power drill
prayer rug, prayer mat
printer
prison, prison house
projectile, missile
"
1	"    )
    parser.add_argument(
"
4	"

"
4	"        if id_skip and block_args.stride == 1 and input_filters == output_filters:
            if drop_connect_rate:
                conv = self._drop_connect(conv, drop_connect_rate, is_test)
"
2	"                    self._wait_cond.wait(max(req.delay, WAIT_FOR_COMMAND_MIN_DELAY))
                return WaitForCommandExitCodeResponse(self._command_exit_code)

"
2	"            # Ensure that at least one previously active host is still assigned, otherwise there is no
            # way to sync the state to the new workers
            prev_hosts = self._host_assignments.keys()
            next_hosts = host_assignments.keys()
            if not prev_hosts & next_hosts:
"
3	"            resp = requests.post(
                f'http://0.0.0.0:{f.port_expose}/api/index',
                json={
                    'data': [
"
2	"            return x*x + self.param[0]*x + self.param[1]
        else:
            return 10*x*x*x + self.param[0]*x*x + self.param[1]*x + self.param[2]

def train(args):
"
4	"
    def get_padding(filter_size, stride=1, dilation=1):
        padding = ((stride - 1) + dilation * (filter_size - 1)) // 2
        return padding
"
4	"red fox, Vulpes vulpes
kit fox, Vulpes macrotis
"
6	"    sys.modules[cls_name] = mod
    spec.loader.exec_module(mod)
    return getattr(mod, cls_name)

"
4	"buckeye, horse chestnut, conker
coral fungus
"
3	"        pass


class MyTestCase(JinaTestCase):
"
1	"         data/${train_set} ${dict} > ${feat_tr_dir}/data.json
    data2json.sh --feat ${feat_dt_dir}/feats.scp --nlsyms ${nlsyms} \
         data/${train_dev} ${dict} > ${feat_dt_dir}/data.json
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 15
  retry:
"
5	"    def _isYukonSequence(self, c1, c2):
        # print('Yukon_AC_RowStack._isYukonSequence()', c1, c2)
"
2	"        with pytest.raises(ValueError, match=""^args must be a tuple, not <(class|type) 'int'>, ""
                                             ""for a single argument use \\(arg,\\)$""):
            on_event(event, fn, args=1)
        fn.assert_not_called()

"
4	"def reader(images=None, paths=None):
    """"""
    Preprocess to yield image.

    Args:
"
2	"  command: bash -c ""\$(cat /mpirun_command) python /horovod/examples/tensorflow2_keras_mnist.py""
  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-openmpi-py3_6-tf2_0_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
"
4	"        """"""
        cpu_config = AnalysisConfig(self.default_pretrained_model_path)
        cpu_config.disable_glog_info()
        cpu_config.disable_gpu()
"
3	"            self.assertTrue(rate < 0.1)

"
4	"#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
4	"macaw
sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita
lorikeet
"
2	"        self.task_index = task_index
        """"""Task index of other task service.""""""

        self.all_task_addresses = all_task_addresses
"
2	"    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
    @mock.patch('horovod.run.gloo_run._get_min_start_hosts', return_value=1)
    def test_all_ranks_failure(self, mock_get_min_start_hosts):
        discovery_schedule = [
"
2	"       test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0 \
       test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0 \
       test-gpu-openmpi-py3_6-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_6_0-pyspark2_4_0 \

"
2	"            for grandchild in child.children(recursive=True):
                try:
                    grandchild.kill()
                except psutil.NoSuchProcess:
"
4	"        ops = block_string.split('_')
        options = {}
        for op in ops:
            splits = re.split(r'(\d.*)', op)
            if len(splits) >= 2:
"
4	"    def encode(blocks_args):
        """"""
        Encodes a list of BlockArgs to a list of strings.
        :param blocks_args: a list of BlockArgs namedtuples of block args
        :return: a list of strings, each string is a notation of block
"
1	"        choices=(""full"", ""part""),
        help=""""""if the ngram is set as a part scorer, similar with CTC scorer,
                ngram scorer only scores topK hypethesis.
                if the ngram is set as full scorer, ngram scorer scores all hypthesis
                the decoding speed of part scorer is musch faster than full one"""""",
"
4	"library
lifeboat
"
1	"            self.path.parent.mkdir(parents=True, exist_ok=True)
            self.fd = self.path.open(""w"", encoding=""utf-8"")

        self.keys.add(key)
"
1	"# Copyright 2020 Johns Hopkins University (Xuankai Chang)
#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
"
3	"            vecs = self._load_gzip(self.index_abspath)
            if vecs is None:
"
4	"table lamp
tank, army tank, armored combat vehicle, armoured combat vehicle
"
4	"import time
from collections import OrderedDict

"
2	"    def wrapper(state, *args, **kwargs):
        notification_manager.init()
        notification_manager.register_listener(state)
"
2	"
            rendezvous_addr = rendezvous_addr or os.environ.get(HOROVOD_GLOO_RENDEZVOUS_ADDR)
"
2	"parser.add_argument('--epoch-wait', type=int, default=0,
                    help='number of seconds each epoch takes')
parser.add_argument('--logfile', default='/tmp/logfile.txt',
                    help='log file to record results (one line per epoch)')
"
6	"    t = '(OptPairTensor, SparseTensor, OptTensor, Size) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert jit((x1, x2), adj.t()).tolist() == out1.tolist()
    assert jit((x1, None), adj.t()).tolist() == out2.tolist()
"
4	"flat-coated retriever
curly-coated retriever
golden retriever
Labrador retriever
"
2	"                        ""Gradients were computed more than ""
                        ""backward_passes_per_step times before call ""
                        ""to step(). Increase backward_passes_per_step to ""
"
4	"carousel, carrousel, merry-go-round, roundabout, whirligig
carpenter's kit, tool kit
carton
"
1	"class DynamicConvolution2D(nn.Module):
    """"""Dynamic 2-Dimentional Convolution layer.

    This implementation is based on
"
2	"
        # Horovod: print summary logs on the first worker.
        verbose = 2 if hvd.rank() == 0 else 0
"
4	"    elif act == 'leaky_relu':
        conv = fluid.layers.leaky_relu(
            conv, alpha=relufactor, name=name + '_leaky_relu')
"
1	"            lpz = None

        h = enc_output.squeeze(0)

"
2	"        if not gloo_built():
            self.skipTest(""Gloo is not available"")
"
4	"
res (list\[dict\]): 分类结果，列表的每一个元素均为字典，其中 key 为识别动物的类别，value为置信度。

"
6	"
        edge_index = knn(x[0], x[1], self.k, b[0], b[1],
                         num_workers=self.num_workers)

"
4	"        w_start = np.random.randint(0, width - size + 1)
        h_start = np.random.randint(0, height - size + 1)
    w_end = w_start + size
    h_end = h_start + size
"
2	"                                    '{expected_env} '
                                    '{extra_mpi_args} '
                                    '-x NCCL_DEBUG=INFO '
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
"
4	"stingray
cock
"
2	"        self._reset_callbacks.extend(callbacks)

    def on_reset(self):
"
4	"            name='',
            conv_name='_conv_head',
            bn_name='_bn1')
"
6	"            m = torch.matmul(x, self.weight[i])
            # propagate_type: (x: Tensor, edge_weight: OptTensor)
"
2	"  agents:
    queue: cpu
"
2	"        raise ValueError('default values must be defined for these properties: {}'
                         .format(unknown_keys))
"
4	"            scale_name = name + ""_scale""
            offset_name = name + ""_offset""
        scale_param = fluid.ParamAttr(
"
6	"from typing import Union, Tuple
from torch_geometric.typing import PairTensor, Adj, OptTensor, Size

"
4	"oxcart
oxygen mask
packet
paddle, boat paddle
paddlewheel, paddle wheel
"
3	"    font-weight: 500;
    padding-left: 30px;
    padding-right: 30px;
}

"
4	"microwave, microwave oven
military uniform
milk can
minibus
miniskirt, mini
"
4	"            bn_mom=self._bn_mom,
            bn_eps=self._bn_eps,
"
1	"    :param int idim: dimension of inputs
    :param int odim: dimension of outputs
    :param Namespace args: argument Namespace containing options
    """"""

"
6	"    iou = test(test_loader)
    print('Epoch: {:02d}, Test IoU: {:.4f}'.format(epoch, iou))
"
4	"wine bottle
wing
wok
wooden spoon
"
2	"

@cache.use_cache()
def _driver_fn(all_host_names, local_host_names, settings):
    """"""
"
3	"from pkg_resources import resource_stream

from .. import __binary_delimiter__
"
2	"        return hook

    def synchronize(self):
        missing_p = self._requires_update - set(self._handles.keys())
"
4	"Lakeland terrier
Sealyham terrier, Sealyham
"
4	"coffee mug
coffeepot
"
1	"        x = F.conv1d(x, weight, padding=self.padding_size, groups=self.wshare).view(
            B, C, T
        )
        if self.use_bias:
"
6	"    pass

"
2	"from horovod.spark.driver.host_discovery import SparkDriverHostDiscovery
from horovod.spark.driver.rsh import rsh
from horovod.spark.task import get_available_devices, gloo_exec_fn, mpirun_exec_fn
"
4	"BlockArgs = collections.namedtuple('BlockArgs', [
    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',
    'expand_ratio', 'id_skip', 'stride', 'se_ratio'
])
"
4	"import paddle.fluid as fluid


"
4	"        name='b6',
        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
"
6	"        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)

        out = torch.cat([x, out], dim=-1)
        outs = [nn(out[:, i]) for i, nn in enumerate(self.post_nns)]
"
1	"        :param torch.Tensor xs: input tensor
        :param torch.Tensor masks: input mask
        :return: position embedded tensor and mask
        :rtype Tuple[torch.Tensor, torch.Tensor]:
        """"""
"
2	"from socket import AF_INET
from psutil import net_if_addrs


"
0	"        # XXX OFFSET
        # -- restrict sane
        self.assertEqual(
"
4	"        results = self.classify(
            paths=[args.input_path],
"
4	"agama
frilled lizard, Chlamydosaurus kingi
"
6	"def train(epoch):
    model.train()

"
1	"            np.zeros((lpz.size(0), len(y_int)), dtype=np.int16) - 1
        )  # state path

        logdelta[0, 0] = lpz[0][y_int[0]]
        logdelta[0, 1] = lpz[0][y_int[1]]
"
4	"groom, bridegroom
scuba diver
rapeseed
daisy
yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum
"
4	"        if use_gpu:
            gpu_config = AnalysisConfig(self.default_pretrained_model_path)
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: cpu
"
1	"            normalize_before=encoder_normalize_before,
            concat_after=encoder_concat_after,
"
2	"            .getOrCreate()

        try:
            yield session
"
4	"                name_prefix = '@HUB_{}@'.format(self.name)
                inputs = {'image': name_prefix + image.name}
                outputs = {
                    'classification': name_prefix + output.name,
"
2	"
        # Horovod: adjust learning rate based on number of processes.
        scaled_lr = K.get_value(model.optimizer.lr) * hvd.size()
        K.set_value(model.optimizer.lr, scaled_lr)
"
4	"        initializer=fluid.initializer.UniformInitializer(
            low=-init_range, high=init_range))
    bias_attr = fluid.ParamAttr(
"
4	"        dtype = helper.input_dtype()
        epsilon = 1e-5
        mean = fluid.layers.reduce_mean(input, dim=[2, 3], keep_dim=True)
        var = fluid.layers.reduce_mean(
"
4	"    img = resize_short(img, target_size=256)
    img = crop_image(img, target_size=DATA_DIM, center=True)
    if img.mode != 'RGB':
        img = img.convert('RGB')
    img = np.array(img).astype('float32').transpose((2, 0, 1)) / 255
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
"
4	"        override_params=override_params,
        use_se=use_se)
    return model


"
4	"        assert isinstance(block_string, str)

"
4	"           stride=1,
           stddev=0.02,
           padding=0,
           groups=None,
           name=""conv2d"",
"
3	"                self.num_reqs,
                self.batch_unit
"
4	"killer whale, killer, orca, grampus, sea wolf, Orcinus orca
dugong, Dugong dugon
sea lion
Chihuahua
Japanese spaniel
"
2	"                SparkTaskClient(next_task_index, next_task_addresses,
                                self._key, self._verbose,
                                match_intf=True)
"
2	"            slots = {'host-2': 2}
            discovery.set(slots)

        def exec_command(slot_info, events):
            manager = WorkerNotificationManager()
"
2	"  agents:
    queue: cpu
- label: ':tensorflow: Test TensorFlow MNIST (test-cpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
"
4	"wig
window screen
"
4	"vase
vault
velvet
"
6	"        # Instantiate a class from the rendered JIT module representation.
        cls = class_from_module_repr(cls_name, jit_module_repr)
"
3	"
def input_fn():
    d = Document()
"
4	"            default=1,
            help=""Return top k results."")
"
4	"            padding_type=self.padding_type,
            name='',
            conv_name='_conv_head',
            bn_name='_bn1')

"
4	"vase
vault
velvet
vending machine
vestment
"
4	"            groups=num_groups,
            act=conv_act,
            padding_type=padding_type,
            use_cudnn=use_cudnn,
            name=conv_name,
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
6	"
    t = '(Tensor, Tensor, OptTensor, Size) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert jit(x1, edge_index).tolist() == out.tolist()
"
6	"
    def __repr__(self):
        return (f'{self.__class__.__name__}({self.in_channels}, '
"
2	"    ``model.named_parameters()``, or ``model.parameters()``.

    Arguments:
"
4	"classifier = hub.Module(name=""efficientnetb6_imagenet"")

result = classifier.classify(images=[cv2.imread('/PATH/TO/IMAGE')])
# or
# result = classifier.classify(paths=['/PATH/TO/IMAGE'])
"
4	"park bench
parking meter
"
6	"        data = data.to(device)
        optimizer.zero_grad()
        out = model(data.x, data.edge_index, data.edge_attr, data.batch)
        loss = (out.squeeze() - data.y).abs().mean()
        loss.backward()
"
4	"brass, memorial tablet, plaque
brassiere, bra, bandeau
breakwater, groin, groyne, mole, bulwark, seawall, jetty
breastplate, aegis, egis
broom
"
2	"        sys.argv[1:] = args
        yield
"
2	"
            if _SYNC_BN_V2 or _SYNC_BN_V3:
                count_all_sum = count_all.sum()
"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
"
2	"
    def __init__(self, state, batches_per_commit=1):
"
2	"#
#     http://www.apache.org/licenses/LICENSE-2.0
#
"
4	"                   use_se=True):
    model = EfficientNet(
"
4	"bee
ant, emmet, pismire
"
4	"green mamba
sea snake
horned viper, cerastes, sand viper, horned asp, Cerastes cornutus
"
4	"    percent = float(target_size) / min(img.size[0], img.size[1])
    resized_width = int(round(img.size[0] * percent))
"
4	"    return model

"
4	"white stork, Ciconia ciconia
black stork, Ciconia nigra
spoonbill
flamingo
"
4	"wool, woolen, woollen
worm fence, snake fence, snake-rail fence, Virginia fence
"
2	"  explicit HorovodReturnScalarOp(OpKernelConstruction* context)
      : OpKernel(context) {}
"
4	"            name=name + ""_weights"",
            initializer=fluid.initializer.NormalInitializer(
                loc=0.0, scale=math.sqrt(2.0 / n)))
        if use_bias == True:
            bias_attr = fluid.ParamAttr(
"
4	"        padding = get_padding(filter_size, stride)
    else:
"
1	"    group.add_argument(
        ""--train_config"", type=str, help=""Training configuration file."",
"
4	"orangutan, orang, orangutang, Pongo pygmaeus
gorilla, Gorilla gorilla
chimpanzee, chimp, Pan troglodytes
gibbon, Hylobates lar
"
2	"            hvd.init()
            res = hvd.allgather(torch.tensor([hvd.rank()])).tolist()
            return res, hvd.rank()

"
4	"leopard, Panthera pardus
snow leopard, ounce, Panthera uncia
"
2	"        yield
    finally:
"
2	"        self._discovery_thread = threading.Thread(target=self._discover_hosts)
        self._discovery_thread.daemon = True
        self._discovery_thread.start()

    def start(self, np, create_worker_fn):
"
3	"      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
"
6	"        else:
            out.append([type_repr])
    return [(x, return_type_repr) for x in product(*out)]
"
4	"wing
wok
wooden spoon
"
4	"            s = s[0]
        oup = block_args.input_filters * block_args.expand_ratio  # number of output channels

"
4	"hourglass
iPod
iron, smoothing iron
jack-o'-lantern
"
3	"      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
"
2	"        self._save_model()

        def broadcast_object_with_session(obj):
            return broadcast_object(obj, session=backend.get_session())

"
6	"from torch_geometric.nn import radius_graph
from torch_geometric.data import download_url
from torch_geometric.data.makedirs import makedirs
"
4	"hare
Angora, Angora rabbit
hamster
porcupine, hedgehog
fox squirrel, eastern fox squirrel, Sciurus niger
"
0	"
            layout.label(text=""UV Mapping"")

"
4	"                   override_params=None,
                   use_se=True):
    model = EfficientNet(
        name='b3',
        is_test=is_test,
"
4	"def efficientnet(width_coefficient=None,
                 depth_coefficient=None,
                 dropout_rate=0.2,
                 drop_connect_rate=0.2):
"
1	"
    Examples:
        >>> with DatadirWriter(""output"") as writer:
"
1	"                    torch.cat([att_w, att_w_], dim=1)
                    for att_w, att_w_ in zip(att_ws, att_ws_)
"
4	"# -*- coding:utf-8 -*-
# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
"
2	"                                .format(work_dir_env_set, python_path_is_set, hvd_python_path_is_set)
                            print('testing with {}'.format(msg))
"
1	"                    ax_.xaxis.set_major_locator(MaxNLocator(integer=True))
                    ax_.yaxis.set_major_locator(MaxNLocator(integer=True))

            fig.tight_layout(rect=[0, 0.03, 1, 0.95])
            fig.savefig(output_dir / f""att_ws/{key}.png"")
"
4	"```

## 服务部署

"
2	"        self.hosts = hosts

"
3	"        return _index

"
4	"                   override_params=None,
                   use_se=True):
    model = EfficientNet(
"
3	"            return self._compiled_if

    def _test_eval_if(self):
"
2	"SPARK_CONF_MAX_INT_MINUS_ONE = '2147483646'

# required for elastic fault-tolerance and auto-scale
# Horovod has retry counters and limits, no need to limit Spark's retries
"
1	"    for key, data in dataset:
        if key == ""a"":
            assert data[""data4""].shape == (100, 80,)
        if key == ""b"":
"
1	"        return retval

"
4	"Arctic fox, white fox, Alopex lagopus
grey fox, gray fox, Urocyon cinereoargenteus
tabby, tabby cat
tiger cat
Persian cat
"
4	"hourglass
iPod
iron, smoothing iron
jack-o'-lantern
"
4	"pirate, pirate ship
pitcher, ewer
plane, carpenter's plane, woodworking plane
"
2	"            'docker0': [('172.122.10.1', 34588)],
            'eth0': [('11.111.33.73', 34588)]
        }
    :type driver_addresses: map
"
2	"  command: bash -c "" /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd test_static_run.py""
  plugins:
  - docker-compose#v2.6.0:
"
4	"        :return: a list of BlockArgs namedtuples of block args
        """"""
"
2	"def run(fn, args=(), kwargs={}, num_proc=None, start_timeout=None,
        use_mpi=None, use_gloo=None, extra_mpi_args=None,
        env=None, stdout=None, stderr=None, verbose=1, nics=None):
    """"""
"
6	"    out21 = conv((x1, x2), edge_index)
    out22 = conv((x1, x2), edge_index, value)
    out23 = conv((x1, None), edge_index, size=(4, 2))
    out24 = conv((x1, None), edge_index, value, size=(4, 2))
    assert out21.size() == (2, 32)
"
4	"            bias_attr = False
    elif init == 'google':
"
2	"        if settings.verbose >= 2:
            print('Waiting for hosts to perform host-to-host interface checking.')
        driver.wait_for_task_to_task_address_updates(settings.start_timeout)
        if settings.verbose >= 2:
            print('Host-to-host interface checking successful.')
"
4	"    print(res[0]['data'])

"
1	"        speech=torch.randn(2, 3, 5),
        speech_lengths=torch.tensor([3, 1], dtype=torch.long),
"
6	"    edge_index = torch.tensor([[0, 0, 0, 1, 2, 3], [1, 2, 3, 0, 0, 0]])
    row, col = edge_index
"
4	"oscilloscope, scope, cathode-ray oscilloscope, CRO
overskirt
"
4	"        padding = get_padding(filter_size, stride)
    else:
        padding = padding

"
2	"      c->set_output(0, c->Scalar());
      return Status::OK();
"
2	"

"
2	"# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
"
4	"kelpie
komondor
Old English sheepdog, bobtail
"
3	"}

#right-side #first, #right-side #second, #right-side #third, #right-side #fourth {
"
4	"black grouse
ptarmigan
"
4	"            filter_size=1,
            bn_act=None,
            padding_type=self.padding_type,
            bn_mom=self._bn_mom,
"
4	"projectile, missile
projector
puck, hockey puck
punching bag, punch bag, punching ball, punchball
purse
"
2	"                print('host changes: {} -> {}'.format(current_hosts, next_hosts))
                start = int(time.time())
"
4	"                   padding_type='SAME',
                   override_params=None,
                   use_se=True):
    model = EfficientNet(
        name='b2',
"
4	"            attr=scale_param, shape=input.shape[1:2], dtype=dtype)
        offset = helper.create_parameter(
            attr=offset_param, shape=input.shape[1:2], dtype=dtype)
"
2	"        thread.join(1.0)
        self.assertFalse(thread.is_alive())
"
4	"            'o%d' % block.output_filters
        ]
        if 0 < block.se_ratio <= 1:
"
1	"
if [ ${stage} -le 4 ] && [ ${stop_stage} -ge 4 ]; then
    echo ""stage 4: Network Training""
"
1	"    if args.ngram_model:
        from espnet.nets.scorers.ngram import NgramFullScorer
        from espnet.nets.scorers.ngram import NgramPartScorer

"
2	"    queue: 4x-gpu-g4
- label: ':pytest: Run PyTests (test-mixed-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
"
4	"racer, race car, racing car
racket, racquet
radiator
radio, wireless
"
4	"strainer
streetcar, tram, tramcar, trolley, trolley car
stretcher
"
4	"picket fence, paling
pickup, pickup truck
pier
piggy bank, penny bank
pill bottle
"
1	"        return len(next(iter(self.loader_dict.values())))

    def __iter__(self):
        return iter(next(iter(self.loader_dict.values())))

"
2	"batch_size = 32
data = tf.random_uniform([batch_size, 2])
"
2	"                execute.assert_called_once_with(expected_cmd, env=expected_env, stdout=None, stderr=None)

"
4	"                add_vars_prefix(context_prog, name_prefix)
                add_vars_prefix(startup_prog, name_prefix)
"
3	"        .. highlight:: yaml
        .. code-block:: yaml

            !Encode2
"
4	"    else:
        param_attr = fluid.ParamAttr(
"
2	"# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"
4	"    b2.context()
    import cv2
    test_image = [
        cv2.imread(
"
1	"            output_state_seq.append(y_int[state_seq[t, 0]])

        return output_state_seq


"
2	"        fn.assert_not_called()

    def test_safe_shell_exec_captures_stdout(self):
        self.do_test_safe_shell_exec('echo hello', 0, 'hello\n', '')
"
4	"                except:
                    pass
"
2	"

class ElasticTorchTests(BaseElasticTests, unittest.TestCase):
"
4	"toy terrier
Rhodesian ridgeback
Afghan hound, Afghan
basset, basset hound
"
2	"

"
4	"        return res

    def save_inference_model(self,
"
4	"amphibian, amphibious vehicle
analog clock
apiary, bee house
"
1	")



"
2	"                settings.key,
                settings.verbose) for index in range(
                num_hosts)]
        # Notify all the drivers that the initial registration is complete.
"
4	"import cv2
import numpy as np
"
4	"    else:
        raise NotImplementedError(""activation: [%s] is not support"" % act)

"
6	"from typing import Optional, Callable, Union
from torch_geometric.typing import OptTensor, PairOptTensor, PairTensor, Adj

"
0	"    bmesh.ops.remove_doubles(bm, verts=bm.verts, dist=0.0001)
    res = bmesh.ops.inset_region(
        bm, faces=[mid], use_even_offset=True, thickness=prop.frame_thickness
    )
"
4	"## 命令行预测

```
hub run efficientnetb4_imagenet --input_path ""/PATH/TO/IMAGE""
"
3	"from ..clients.python import ProgressBar
from ..helper import colored, get_readable_size
from ..logging import get_logger
from ..logging.profile import TimeContext
"
2	"
    """"""
    Performs an actual horovod.spark.run test using MPI or Gloo.
    """"""
"
4	"        batch_norm_epsilon=1e-3,
        dropout_rate=dropout_rate,
"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
"
6	"        super(MyConv, self).__init__(aggr='add')

        if isinstance(in_channels, int):
            in_channels = (in_channels, in_channels)
"
4	"jaguar, panther, Panthera onca, Felis onca
lion, king of beasts, Panthera leo
tiger, Panthera tigris
cheetah, chetah, Acinonyx jubatus
brown bear, bruin, Ursus arctos
"
4	"gibbon, Hylobates lar
siamang, Hylobates syndactylus, Symphalangus syndactylus
guenon, guenon monkey
patas, hussar monkey, Erythrocebus patas
baboon
"
4	"
import paddle.fluid as fluid
from efficientnetb3_imagenet.layers import conv2d, init_batch_norm_layer, init_fc_layer

"
4	"# 打印预测结果
print(r.json()[""results""])
```

"
4	"def EfficientNetB7(is_test=False,
                   padding_type='SAME',
"
2	"        # TODO(travis): this should be a max allreduce to account for changes in rank 0
        prev_timestamp, self._last_updated_timestamp = self._bcast_object((prev_timestamp, last_updated_timestamp))

"
4	"perfume, essence
Petri dish
photocopier
pick, plectrum, plectron
"
4	"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
"
2	"  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
"
4	"marimba, xylophone
mask
matchstick
maypole
maze, labyrinth
"
2	"  agents:
    queue: cpu
- label: ':muscle: Test MXNet MNIST (test-cpu-gloo-py3_8-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark3_0_0)'
  command: horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet_mnist.py
"
3	"

"
2	"                                  F.greatest(F.lit(0), F.least(F.lit(25 * 7), F.datediff(df.Date, df.Promo2Since))))
                           .otherwise(0))
        df = df.withColumn('Promo2Weeks', (df.Promo2Days / 7).cast(T.IntegerType()))

        # Check that we did not lose any rows through inner joins.
"
2	"
        self.assertEqual(0, results[0]['start_rank'])
"
2	"
    spark_thread = in_thread(target=run_spark, daemon=False)
"
2	"
import horovod.tensorflow as hvd
"
2	"  - ecr#v1.2.0:
      login: true
"
6	"    t = '(Tensor, SparseTensor) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert torch.allclose(conv(x, adj.t()), out, atol=1e-6)
"
4	"            if num_data > 1:
                text_2 = fluid.data(
                    name='text_2',
                    shape=[-1, max_seq_len],
"
2	"
        count_handle = allgather_async(count.unsqueeze(0), name='sync_batch_norm.count')
"
4	"        self.arg_config_group.add_argument(
            '--top_k',
            type=ast.literal_eval,
            default=1,
            help=""Return top k results."")
"
2	"
parser.add_argument('--model', type=str, default='ResNet50',
                    help='model to benchmark')
parser.add_argument('--batch-size', type=int, default=32,
                    help='input batch size')
"
3	"        f = Flow().add(
            yaml_path='_forward',
            replicas=2)
        with f:
"
1	"

if [ -z ${tag} ]; then
"
3	"                elif isinstance(ret, list):  #: 1-to-many
                    d.chunks.remove(c)  # remove the current one?
                    for c_dict in ret:
                        self.set_chunk(d.chunks.add(), c_dict)
"
4	"daisy
yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum
corn
acorn
hip, rose hip, rosehip
"
3	"        def _input_fn():
            return iter(['data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAA2ElEQVR4nADIADf/AxWcWRUeCEeBO68T3u1qLWarHqMaxDnxhAEaLh0Ssu6ZGfnKcjP4CeDLoJok3o4aOPYAJocsjktZfo4Z7Q/WR1UTgppAAdguAhR+AUm9AnqRH2jgdBZ0R+kKxAFoAME32BL7fwQbcLzhw+dXMmY9BS9K8EarXyWLH8VYK1MACkxlLTY4Eh69XfjpROqjE7P0AeBx6DGmA8/lRRlTCmPkL196pC0aWBkVs2wyjqb/LABVYL8Xgeomjl3VtEMxAeaUrGvnIawVh/oBAAD///GwU6v3yCoVAAAAAElFTkSuQmCC',
"
2	"parser = argparse.ArgumentParser(description='PyTorch Elastic Test',
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
"
4	"

```python
"
4	"screen, CRT screen
screw
screwdriver
seat belt, seatbelt
sewing machine
"
4	"
        if block_args.expand_ratio != 1:
            conv = self.conv_bn_layer(
                inputs,
                num_filters=oup,
"
4	"spoonbill
flamingo
little blue heron, Egretta caerulea
American egret, great white heron, Egretta albus
bittern
"
4	"from __future__ import division
from __future__ import print_function
"
4	"        override_params=override_params,
        use_se=use_se)
    return model
"
4	"        'efficientnet-b2': (1.1, 1.2, 260, 0.3),
        'efficientnet-b3': (1.2, 1.4, 300, 0.3),
        'efficientnet-b4': (1.4, 1.8, 380, 0.4),
        'efficientnet-b5': (1.6, 2.2, 456, 0.4),
        'efficientnet-b6': (1.8, 2.6, 528, 0.5),
"
4	"    orig_shape = x.shape
    if len(x.shape) > 1:
        tmp = np.max(x, axis=1)
"
4	"    init_range = 1.0 / math.sqrt(n)

"
2	"        test_compile(build_ext, 'test_torch_rocm',
                     include_dirs=include_dirs + include_paths(cuda=True),
                     extra_compile_preargs=extra_compile_args,
"
4	"            kernel_size=int(options['k']),
            num_repeat=int(options['r']),
"
4	"

def EfficientNetB6(is_test=False,
                   padding_type='SAME',
"
4	"Japanese spaniel
Maltese dog, Maltese terrier, Maltese
Pekinese, Pekingese, Peke
Shih-Tzu
"
0	""""""" Adapted from Script Watcher Addon
https://github.com/wisaac407/blender-script-watcher
""""""
import os
"
2	"# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
"
1	"    https://github.com/pytorch/fairseq/tree/master/fairseq

"
2	"    def test_safe_shell_exec_interrupts_on_parent_shutdown(self):
        sleep = 20
        parent_script = os.path.join(os.path.dirname(__file__), 'data/run_safe_shell_exec.py')
        child_script = os.path.join(os.path.dirname(__file__), 'data/sleep.py')

"
2	"        if log:
            logging.info('Spark is logging to %s', log.group(1))
            return log.group(1)
        else:
            logging.warning('could not find log file in: %s', line)
"
0	"    else:
        f.close()

"
4	"                      num_groups=1,
                      padding_type=""SAME"",
"
3	"        if vecs is not None:
            return self.build_advanced_index(vecs)
        else:
"
4	"tripod
triumphal arch
trolleybus, trolley coach, trackless trolley
trombone
"
4	"
    def add_module_input_arg(self):
        """"""
"
4	"    ])
    print(res)
"
4	"import math
import warnings

"
6	"
        if self.add_self_loops:
"
4	"    * feature\_map (paddle.fluid.framework.Variable): 特征匹配，全连接层前面的那个张量。
* context\_prog(fluid.Program): 计算图，用于迁移学习。
"
1	"    n_unks=$(grep tokenid ${feat_align_dir}/data.json | \
                sed -e 's/.*: ""\(.*\)"".*/\1/' | \
                awk -v unk_id=${unk_id} '
"
2	"  retry:
    automatic: true
  agents:
    queue: cpu
"
1	"                odim, args.adim, args.dropout_rate, ctc_type=args.ctc_type, reduce=False
            )
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
"
6	"            raise ValueError((f'Split {split} found, but expected either '
                              'train, val, trainval or test'))

"
2	"
    """"""
    Test that horovod.spark.run_elastic works properly in a simple setup.
    """"""
    @pytest.mark.skipif(LooseVersion(torch.__version__) < LooseVersion('1.0.0'),
"
4	"                                             self._global_params),
                num_repeat=round_repeats(block_arg.num_repeat,
"
4	"bolete
ear, spike, capitulum
toilet tissue, toilet paper, bathroom tissue

"
4	"barrow, garden cart, lawn cart, wheelbarrow
baseball
basketball
bassinet
"
4	"measuring cup
medicine chest, medicine cabinet
megalith, megalithic structure
"
6	"                out = out * (torch.log(deg + 1) / self.avg_deg['log'])
            elif scaler == 'attenuation':
                out = out * (self.avg_deg['log'] / torch.log(deg + 1))
            elif scaler == 'linear':
                out = out * (deg / self.avg_deg['lin'])
"
4	"                    'classification': name_prefix + output.name,
                    'feature_map': name_prefix + feature_map.name
                }
                add_vars_prefix(context_prog, name_prefix)
"
4	"pickelhaube
picket fence, paling
pickup, pickup truck
pier
piggy bank, penny bank
"
4	"#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
1	"from pathlib import Path
from typing import Union
import warnings

from typeguard import check_argument_types
"
3	"_STATUS_STATUSCODE.containing_type = _STATUS

"
6	"        data.adj_t = SparseTensor(row=col, col=row, value=value,
                                  sparse_sizes=(N, N), is_sorted=True)
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
4	"            gpu_config.enable_use_gpu(
                memory_pool_init_size_mb=1000, device_id=0)
            self.gpu_predictor = create_paddle_predictor(gpu_config)
"
4	"    def shortcut(self, input, data_residual):
        return fluid.layers.elementwise_add(input, data_residual)

"
3	"from typing import Any

import numpy as np
"
1	"dict=
nlsyms=

"
3	"    def build_advanced_index(self, vecs: 'np.ndarray'):
        return vecs

"
2	"        return self._workers[state]

"
2	"      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
"
2	"        """"""Saves state to host memory.""""""
        raise NotImplementedError()

    def restore(self):
        """"""Restores the last committed state, undoing any uncommitted modifications.""""""
"
6	"                edge_weight: OptTensor = None, size: Size = None) -> Tensor:
        """"""""""""
        if isinstance(x, Tensor):
            x: OptPairTensor = (x, x)
"
4	"    def context(self,
                trainable=True,
                pretrained=True,
                override_params=None,
                phase='train'):
"
4	"garden spider, Aranea diademata
black widow, Latrodectus mactans
tarantula
wolf spider, hunting spider
"
4	"                                         self._global_params))

            # The first block needs to take care of stride and filter size increase.
            drop_connect_rate = self._global_params.drop_connect_rate
            if drop_connect_rate:
"
4	"bluetick
black-and-tan coonhound
Walker hound, Walker foxhound
"
3	"             .add(name='d1', image='jinaai/jina:test-pip', yaml_path='_logforward', entrypoint='jina pod')
             .add(name='d2', image='jinaai/jina:test-pip', yaml_path='_logforward', entrypoint='jina pod')
             .add(name='d3', image='jinaai/jina:test-pip', yaml_path='_logforward',
"
4	"        name=name + '_offset',
        initializer=fluid.initializer.Constant(value=0.0))
"
6	"@pytest.mark.parametrize('separate_gaussians', [True, False])
def test_gmm_conv(separate_gaussians):
    x1 = torch.randn(4, 8)
    x2 = torch.randn(2, 16)
    edge_index = torch.tensor([[0, 1, 2, 3], [0, 0, 1, 1]])
"
4	"
    def get_expected_image_width(self):
        return 224

    def get_expected_image_height(self):
"
4	"marmot
beaver
guinea pig, Cavia cobaya
"
2	"            config.inter_op_parallelism_threads = 1
            config.intra_op_parallelism_threads = 1
"
4	"cliff dwelling
cloak
clog, geta, patten, sabot
cocktail shaker
coffee mug
"
2	"
log('Model: %s' % args.model)
log('Batch size: %d' % args.batch_size)
device = 'GPU' if args.cuda else 'CPU'
log('Number of %ss: %d' % (device, hvd.size()))
"
4	"            for image_id in range(batch_size):
                try:
                    batch_data.append(all_data[handle_id + image_id])
                except:
"
3	"    background: #32C8CD;
    border-radius: 10px;
    padding: 10px;
    color: white;
"
2	"    automatic: true
  agents:
    queue: cpu
- label: ':muscle: Test MXNet MNIST (test-cpu-openmpi-py3_6-tf1_14_0-keras2_2_4-torch1_2_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c "" OMP_NUM_THREADS=1 \$(cat /mpirun_command) python /horovod/examples/mxnet_mnist.py""
"
6	"    assert conv2.__repr__() == 'SignedConv(32, 48, first_aggr=False)'

    out1 = conv1(x, edge_index, edge_index)
    assert out1.size() == (4, 64)
"
4	"European fire salamander, Salamandra salamandra
common newt, Triturus vulgaris
eft
"
4	"            description=""Run the {} module."".format(self.name),
            prog='hub run {}'.format(self.name),
"
2	"            self.skipTest(""Not compiled with HOROVOD_GPU_OPERATIONS"")

        hvd.init()
        rank = hvd.rank()
"
2	"        extra_mpi_args = '<extra args go here>'
        with is_built(gloo_is_built=False, mpi_is_built=True):
            self._do_test_spark_run(num_proc=2, use_mpi=True, use_gloo=False,
                                    extra_mpi_args=extra_mpi_args,
"
4	"        'efficientnet-b7': (2.0, 3.1, 600, 0.5),
    }
    return params_dict[model_name]
"
2	"        assert current_hosts.count_available_slots() == 4

"
4	"submarine, pigboat, sub, U-boat
suit, suit of clothes
"
0	"        self.clear_objects()

        prop.width = 10
        prop.length = 10
        res = builder.build(context, prop)
"
4	"            dirname=self.default_pretrained_model_path, executor=exe)

        fluid.io.save_inference_model(
"
1	"                )

            # add eos in the final loop to avoid that there are no ended hyps
"
3	"    margin-left: -2px;
    margin-top: 143px;
"
3	"        self.logger.info('%s %s' % (self._greetings, data))
        return np.random.random([data.shape[0], 3])
"
4	"leaf beetle, chrysomelid
dung beetle
rhinoceros beetle
weevil
"
4	"            out = postprocess(
                data_out=predictor_output[0].as_ndarray(),
"
1	"        writer[""def""] = array2
    target = NpyScpReader(tmp_path / ""feats.scp"")
    desired = {""abc"": array1, ""def"": array2}

    for k in desired:
"
4	"    params_dict = {
        # Coefficients:   width,depth,resolution,dropout
        'efficientnet-b0': (1.0, 1.0, 224, 0.2),
"
6	"    x1 = torch.randn(4, 8)
    x2 = torch.randn(2, 16)
    edge_index = torch.tensor([[0, 1, 2, 3], [0, 0, 1, 1]])
    row, col = edge_index
"
1	"    ):
        """"""Construct Dynamic Convolution layer.""""""
        super(DynamicConvolution, self).__init__()
"
4	"                loc=0.0, scale=stddev))
        if use_bias == True:
"
4	"
    def get_pretrained_images_std(self):
        im_std = np.array([0.229, 0.224, 0.225]).reshape(1, 3)
"
2	"    Tests for .buildkite directory
    """"""

    def __init__(self, *args, **kwargs):
        super(BuildKiteTests, self).__init__(*args, **kwargs)
"
5	"from pysol_cards.random import CUSTOM_BIT, MS_LONG_BIT  # noqa: E402,I100

"
2	"            self.assertEqual(alloc_info.local_rank, alloc_info.rank)

"
2	"  command: bash -c "" /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd test_static_run.py""
  plugins:
  - docker-compose#v2.6.0:
"
2	"            assert results[2]['size'] == slots
            assert results[2]['hostname'] == '127.0.0.1'
            assert results[2]['rendezvous'] == 3

    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
"
1	"                logging.info(""end detected at %d"", i)
                break

"
4	"otterhound, otter hound
Saluki, gazelle hound
Scottish deerhound, deerhound
Weimaraner
"
4	"black-footed ferret, ferret, Mustela nigripes
otter
skunk, polecat, wood pussy
badger
"
2	"                    yield Row(**fields)
            return fn
"
0	"if len(args) != 1 :
    raise getopt.GetoptError(""expecting exactly one arg, the tag to build a release for"")

upto = args[0]
earliest = git(""rev-list"", ""--reverse"", upto).split(b""\n"")[0].strip().decode()
"
4	"    'dropout_rate',
    'num_classes',
    'width_coefficient',
"
2	"
    handler = create_rendezvous_handler(driver)
    global_rendezv_port = rendezvous.start(handler)
"
4	"        width = self.module.get_expected_image_width()
        height = self.module.get_expected_image_height()
        mean = self.module.get_pretrained_images_mean()
        std = self.module.get_pretrained_images_std()

"
2	"      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
"
4	"        is_test=is_test,
        padding_type=padding_type,
"
4	"cliff, drop, drop-off
coral reef
geyser
lakeside, lakeshore
promontory, headland, head, foreland
"
4	"sea lion
Chihuahua
Japanese spaniel
Maltese dog, Maltese terrier, Maltese
"
4	"            num_filters=out_channels,
            filter_size=1,
            bn_act='swish',
            bn_mom=self._bn_mom,
            bn_eps=self._bn_eps,
"
4	"hook, claw
hoopskirt, crinoline
"
4	"# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
"
2	"      run: test-cpu-openmpi-py3_6-tf1_14_0-keras2_2_4-torch1_2_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
4	"        place = fluid.CPUPlace()
        exe = fluid.Executor(place)

        program, feeded_var_names, target_vars = fluid.io.load_inference_model(
            dirname=self.default_pretrained_model_path, executor=exe)
"
2	"        self.assertEqual(0, results[0]['start_rank'])
        self.assertEqual(3, results[0]['size'])
"
0	"# from which to generate the archive. For example, the command
#
#     ./make-dist v1.0
"
4	"water tower
whiskey jug
whistle
"
3	"        f = Flow(rest_api=True).add(
            yaml_path='_forward',
"
1	"wget -c -N https://ftp.espci.fr/pub/sigma/WSJ05K_Test/WSJ0_5K_Transcripts.txt -P ${feat_local_dir}

mkdir -p data/train
"
4	"cab, hack, taxi, taxicab
caldron, cauldron
candle, taper, wax light
cannon
"
2	"API. If a process spawned by multiprocessing is hard killed while holding semaphores, then they will not be properly
cleaned up and will be effectively leaked.

"
4	"lumbermill, sawmill
magnetic compass
mailbag, postbag
mailbox, letter box
maillot
"
4	"                input_filters=round_filters(block_args.input_filters,
                                            self._global_params),
                output_filters=round_filters(block_args.output_filters,
                                             self._global_params),
                num_repeat=round_repeats(block_args.num_repeat,
"
4	"                key, value = splits[:2]
                options[key] = value
"
4	"convertible
corkscrew, bottle screw
cornet, horn, trumpet, trump
cowboy boot
cowboy hat, ten-gallon hat
"
6	"                                         num_steps=4, sample_coverage=10,
                                         log=False)
"
4	"tricycle, trike, velocipede
trimaran
tripod
"
2	"               '-m', 'horovod.spark.task.mpirun_exec_fn',
               codec.dumps_base64(driver.addresses()),
"
4	"frying pan, frypan, skillet
fur coat
garbage truck, dustcart
"
4	"                inputs = {'image': name_prefix + image.name}
                outputs = {
                    'classification': name_prefix + output.name,
                    'feature_map': name_prefix + feature_map.name
"
4	"assault rifle, assault gun
backpack, back pack, knapsack, packsack, rucksack, haversack
"
4	"                   override_params=None,
                   use_se=True):
"
4	"            padding_type=self.padding_type,
            bn_eps=self._bn_eps,
            name='',
            conv_name='_conv_stem',
"
1	"import pytest
import torch
"
2	"        """"""Reset objects and variables following a reset event (before synchronization).""""""
        pass


"
2	"                self._wait_hosts_cond.wait(tmout.remaining())
                tmout.check_time_out_for('minimum number of slots to become available')
        finally:
            self._wait_hosts_cond.release()
"
3	"    return ''.join(dec)


def get_default_login():
"
1	"
@pytest.fixture
def csv_float(tmp_path):
    p = tmp_path / ""shape.txt""
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
"
2	"        return int(yaml.safe_load(output.getvalue())[LSFUtils._THREAD_KEY])

"
1	"import copy
from distutils.version import LooseVersion
from io import StringIO
from typing import Callable
"
1	"            tuple[torch.Tensor, List[Any]]: Tuple of
                batchfied scores for next token with shape of `(n_batch, n_vocab)`
                and next state list for ys.
"
4	"            splits = re.split(r'(\d.*)', op)
            if len(splits) >= 2:
                key, value = splits[:2]
"
4	"        return res

    def save_inference_model(self,
                             dirname,
"
1	"
        state_seq = -1 * np.ones((lpz.size(0), 1), dtype=np.int16)

"
0	"        self.assertEqual(y.to_tuple(1), Vector((0, 0, 1)).to_tuple(1))
        self.assertEqual(z.to_tuple(1), Vector((0, 1, 0)).to_tuple(1))

"
6	"    @property
    def raw_file_names(self):
        return 'WN18.gpickle'
"
2	"    queue: cpu
- label: ':tensorflow: Single Keras MNIST (test-cpu-openmpi-py3_6-tf1_14_0-keras2_2_4-torch1_2_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c "" python /horovod/examples/keras_mnist_advanced.py --epochs 3 --batch-size 64""
  plugins:
  - docker-compose#v2.6.0:
"
4	"        h_start = np.random.randint(0, height - size + 1)
    w_end = w_start + size
    h_end = h_start + size
    img = img.crop((w_start, h_start, w_end, h_end))
"
2	"  plugins:
  - docker-compose#v2.6.0:
"
3	"        self.login()
        with TimeContext(f'pulling {self.args.name}', self.logger):
"
4	"        name='b2',
        is_test=is_test,
"
4	"            low=-init_range, high=init_range))
    bias_attr = fluid.ParamAttr(
        name=name + '_offset',
        initializer=fluid.initializer.Constant(value=0.0))
    return param_attr, bias_attr
"
2	"# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"
4	"        blocks_args = []
        for block_string in string_list:
            blocks_args.append(BlockDecoder._decode_block_string(block_string))
"
4	"bassoon
bathing cap, swimming cap
"
2	"                tests.append(
                    tf.reduce_all(
                        tf.equal(tf.cast(rank_tensor, tf.int32), value)))

"
4	"loupe, jeweler's loupe
lumbermill, sawmill
magnetic compass
"
4	"menu
plate
guacamole
consomme
"
2	"    def test_broadcast_object(self):
        if LooseVersion(tf.__version__) < LooseVersion('1.15.0'):
            self.skipTest(""Broadcasting object requires TensorFlow 1.15 or above"")
"
1	"        return iter(self.data)

"
4	"pole
police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria
poncho
pool table, billiard table, snooker table
pop bottle, soda bottle
"
4	"running shoe
safe
safety pin
saltshaker, salt shaker
"
4	"    'EfficientNet', 'EfficientNetB0_small', 'EfficientNetB0', 'EfficientNetB1',
    'EfficientNetB2', 'EfficientNetB3', 'EfficientNetB4', 'EfficientNetB5',
    'EfficientNetB6', 'EfficientNetB7'
"
1	"                        self_attention_dropout_rate,
                        conv_kernel_length,
"
4	"viaduct
violin, fiddle
volleyball
waffle iron
"
4	"little blue heron, Egretta caerulea
American egret, great white heron, Egretta albus
bittern
crane
limpkin, Aramus pictus
"
2	"
  std::string substr;
  getline(ss, substr, ',');
"
4	"        Run as a service.
        """"""
        images_decode = [base64_to_cv2(image) for image in images]
"
2	"    (stdout_r, stdout_w) = ctx.Pipe()
    (stderr_r, stderr_w) = ctx.Pipe()

    # This Pipe is how we ensure that the executed process is properly terminated (not orphaned) if
"
4	"        self.add_module_config_arg()
        self.add_module_input_arg()
        args = self.parser.parse_args(argvs)
        results = self.classify(
            paths=[args.input_path],
"
1	"
@pytest.mark.skipif(
    LooseVersion(torch.__version__) < LooseVersion(""1.2""), reason=""require pytorch>=1.2""
)
def test_ESPnetDataset_rand_float(shape_file):
"
4	"    m = DeeplabV3pXception65HumanSeg()
    import cv2
    img = cv2.imread('./meditation.jpg')
    res = m.segmentation(images=[img])
"
4	"        self.assertEqual(mean.tolist(), self.true_mean)
        self.assertEqual(std.tolist(), self.true_std)
"
6	"    if torch.is_tensor(item) and item.dim() == 0:
        out = item.item()
    elif torch.is_tensor(item):
"
4	"                      bn_act='swish',
                      use_cudnn=True,
                      use_bn=True,
"
1	"                    n_layers=dprenet_layers,
                    n_units=dprenet_units,
                    dropout_rate=dprenet_dropout_rate,
"
4	"        return bn

    def mb_conv_block(self,
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
4	"运行启动命令：
```shell
"
4	"            model_filename = ""__model__"" if not model_filename else model_filename
            params_filename = ""__params__"" if not params_filename else params_filename
        place = fluid.CPUPlace()
"
4	"            width_coefficient=w, depth_coefficient=d, dropout_rate=p)
    else:
"
2	"  agents:
    queue: cpu
- label: ':muscle: Single MXNet MNIST (test-cpu-openmpi-py3_6-tf1_14_0-keras2_2_4-torch1_2_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c "" python /horovod/examples/mxnet_mnist.py --epochs 3""
"
2	"    A YARN container does not share memory with other containers on the same host,
    so it must be considered a `host` in the sense of the `host_hash`.
"
2	"

"
4	"                    dtype='float32',
                    param_attr=w_param_attrs)
"
4	"screen, CRT screen
screw
screwdriver
seat belt, seatbelt
"
4	"        self.module = hub.Module(name='efficientnetb6_imagenet')
        self.test_images = [
            ""../image_dataset/classification/animals/dog.jpeg"",
            ""../image_dataset/keypoint_detection/girl2.jpg""
        ]
"
4	"                        param_attr=w_param_attrs)
                    emb_3_name = emb_3.name
                    data_list.append(text_3)
"
2	"            momentum_correction=momentum_correction, steps_per_epoch=steps_per_epoch, initial_lr=initial_lr,
            *args)
"
2	"
        self.assertEqual(0, results[0]['start_rank'])
        self.assertEqual(1, results[0]['size'])
        self.assertEqual(1, results[0]['rendezvous'])

"
4	"# -*- coding:utf-8 -*-
# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.
"
4	"American black bear, black bear, Ursus americanus, Euarctos americanus
ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus
sloth bear, Melursus ursinus, Ursus ursinus
mongoose
"
1	"        conv_wshare=4,
        conv_kernel_length=11,
        conv_usebias=False,
"
2	"            if use_mpi and not mpi_built():
                continue

            if use_gloo and not gloo_built():
                continue
"
4	"timber wolf, grey wolf, gray wolf, Canis lupus
white wolf, Arctic wolf, Canis lupus tundrarum
red wolf, maned wolf, Canis rufus, Canis niger
"
2	"                    rank=rank,
                    host=host,
                    scpu=cpu_val,
                    ecpu=cpu_val + cpu_per_gpu - 1
"
3	"
class Encode1(BaseEncoder):
    def encode(self, data: Any, *args, **kwargs) -> Any:
        print('i only encode text/plain')
        return np.random.random([data.shape[0], 3])
"
4	"             num_data(int): It's number of data inputted to the model, selectted as following options:

"
2	"                client.register_task(0, task.addresses(), host_hash)
                settings = hvd_settings.Settings(verbose=2, key=key)
"
4	"            args.append('noskip')
        return '_'.join(args)

"
2	"                    break
                executor = self._executor_re.match(line)
"
2	"# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
"
1	"        A string formatted according to the pattern described above.
    """"""
    assert number >= 0
"
4	"tractor
trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi
tray
trench coat
"
1	"    return phones

"
3	"!MWUEncoder
with:
  greetings: im from internal yaml!
metas:
"
4	"ice lolly, lolly, lollipop, popsicle
French loaf
bagel, beigel
"
2	"import horovod
from horovod.torch.elastic import run
from horovod.common.util import gloo_built
import horovod.spark.torch as hvd_spark
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
"
4	"            input_filters=int(options['i']),
            output_filters=int(options['o']),
            expand_ratio=int(options['e']),
"
1	"                    local_scores = (
                        local_att_scores + recog_args.lm_weight * local_lm_scores
                    )
"
4	"acoustic guitar
aircraft carrier, carrier, flattop, attack aircraft carrier
"
4	"                global_vars = context_prog.global_block().vars
                inputs = {
                    key: global_vars[value]
                    for key, value in inputs.items()
"
4	"Great Dane
Saint Bernard, St Bernard
Eskimo dog, husky
malamute, malemute, Alaskan malamute
"
4	"            for image_id in range(batch_size):
                try:
                    batch_data.append(all_data[handle_id + image_id])
                except:
                    pass
"
4	"
        block_args_copy = copy.deepcopy(self._blocks_args)
        idx = 0
        block_size = 0
        for block_arg in block_args_copy:
"
2	"        self._host_slots = host_slots

"
6	"    Args:
        root (string): Root directory where the dataset should be saved.
        transform (callable, optional): A function/transform that takes in an
"
4	"            help=""whether use GPU or not."")
        self.arg_config_group.add_argument(
            '--batch_size',
            type=ast.literal_eval,
"
2	"                               .format(extra_mpi_args=settings.extra_mpi_args if settings.extra_mpi_args else '',
                                       rsh_agent=' '.join(rsh_agent)))
    command = (sys.executable,
"
2	"                        threshold = 5e-4
                    else:
"
0	"    del bpy.types.Object.facemap_materials

"
1	"    num_groups = int(np.ceil(num_digits / 3))
    num_groups = min(num_groups, len(labels))  # don't abbreviate beyond trillions
    shift = -3 * (num_groups - 1)
    number = number * (10 ** shift)
"
2	"# ==============================================================================

import contextlib
import io
"
4	"stupa, tope
submarine, pigboat, sub, U-boat
suit, suit of clothes
sundial
sunglass
"
4	"

def postprocess(data_out, label_list, top_k):
    """"""
"
2	"    queue: cpu
- label: ':factory: Elastic Tests (test-cpu-gloo-py3_7-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""cd /horovod/test/integration && HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format '[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s' --capture=no test_elastic_torch.py test_elastic_tensorflow2.py""
  plugins:
  - docker-compose#v2.6.0:
"
2	"def _get_discovery_lines(schedule_step, start, end):
    epoch, hosts = schedule_step
    hosts_str = os.linesep.join(['echo ""{}""'.format(host) for host in hosts])
    if start and end:
"
4	"                    def _if_exist(var):
                        b = os.path.exists(
                            os.path.join(self.default_pretrained_model_path,
"
4	"        """"""
        self.arg_input_group.add_argument(
            '--input_path', type=str, help=""path to image."")

"
1	"import numpy as np
import pytest

"
1	"import h5py
import kaldiio
"
4	"        exe = fluid.Executor(place)

"
2	"        state.commit()


"
1	"            dropout_rate,
            positionwise_conv_kernel_size,
        )
"
1	"        'HELLO WORLD, AND JUNIOR AND DOCTOR'

    """"""

"
2	"    if not gloo_built(verbose=(verbose >= 2)):
        raise ValueError('Gloo support is required to use elastic training, but has not been built.  Ensure CMake is '
                         'installed and reinstall Horovod with HOROVOD_WITH_GLOO=1 to debug the build error.')
"
1	"                # 2.b. Load data from non-iterable dataset
                _, from_non_iterable = self.non_iterable_dataset[uid]
                data.update(from_non_iterable)

"
4	"king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica
American lobster, Northern lobster, Maine lobster, Homarus americanus
spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish
"
1	"        # calculate guided attention loss
        if self.use_guided_attn_loss:
            # calculate for encoder
            if ""encoder"" in self.modules_applied_guided_attn:
                att_ws = []
"
1	"        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)

        Args:
"
4	"            padding_type=self.padding_type,
            name='',
            conv_name='_conv_head',
"
2	"        else:
            local_hosts = ['localhost']
            remote_hosts = ['127.0.0.1']
"
3	"        if validate:
            self._validate_manifest(tmp)

        return tmp
"
4	"        dropout_rate=dropout_rate,
        drop_connect_rate=drop_connect_rate,
        num_classes=1000,
        width_coefficient=width_coefficient,
        depth_coefficient=depth_coefficient,
"
4	"import math
import copy
"
4	"        context_prog = fluid.Program()
        startup_prog = fluid.Program()
        with fluid.program_guard(context_prog, startup_prog):
"
6	"        add_self_loops (bool, optional): If set to :obj:`False`, will not add
            self-loops to the input graph. (default: :obj:`True`)
"
2	"  agents:
    queue: 2x-gpu-g4
- label: ':spark: Spark Keras Rossmann Estimator (test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
"
4	"            prog='hub run {}'.format(self.name),
            usage='%(prog)s',
            add_help=True)
        self.arg_input_group = self.parser.add_argument_group(
"
2	"        loss_value = training_step(images, labels)

        if state.batch % 10 == 0 and hvd.local_rank() == 0:
            print('Step #%d\tLoss: %.6f' % (state.batch, loss_value))
"
3	"            elif v and isinstance(v, list):
                manifest[k] = ','.join(v)

        # show manifest key-values
        for k, v in manifest.items():
"
4	"def textcnn_net(emb,
                seq_len,
"
6	"    out1 = conv((x1, x2), edge_index)
    out2 = conv((x1, None), edge_index, (4, 2))
    assert out1.size() == (2, 32)
"
4	"            bn_eps=self._bn_eps,
            name=name,
            conv_name=name + '_project_conv',
            bn_name='_bn2')
        return conv
"
4	"lipstick, lip rouge
Loafer
lotion
"
2	"            .withColumn('CompetitionOpenSinceYear', F.coalesce(df.CompetitionOpenSinceYear, F.lit(1900))) \
            .withColumn('CompetitionOpenSinceMonth', F.coalesce(df.CompetitionOpenSinceMonth, F.lit(1))) \
            .withColumn('Promo2SinceYear', F.coalesce(df.Promo2SinceYear, F.lit(1900))) \
"
2	"      run: test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
"
2	"        }

        results = self._run(hosts=hosts, exit_schedule=exit_schedule, np=2, min_np=2, max_np=2,
                            extra_conf=[conf.SPARK_CONF_ALWAYS_RESTART_FAILED_TASK,
"
3	"      name='pod', full_name='jina.Status.Details.pod', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
"
6	"    def __init__(self, channels: Union[int, Tuple[int, int]], dim: int = 0,
                 aggr: str = 'add', batch_norm: bool = False,
                 bias: bool = True, **kwargs):
"
2	"                            % [(index, self._task_addresses_for_tasks[index])
                               for index in self._task_addresses_for_tasks])

        return nics

"
1	"        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
        self.linear1 = nn.Linear(n_feat, n_feat * 2)
"
2	"
    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
    @mock.patch('horovod.run.elastic.driver.ElasticDriver.host_assignments')
"
2	"    def test_horovod_join_allreduce(self):
        """"""Test that the hvd.join with allreduce works on GPUs.""""""
"
2	"    import horovod.spark
    import horovod.tensorflow.keras as hvd

"
0	"    roof_hang = map_new_faces(FaceMap.ROOF_HANGS)(extrude_and_outset)
    faces = roof_hang(bm, faces, prop.thickness, prop.outset)
"
1	"import numpy
import torch

from espnet.nets.asr_interface import ASRInterface
from espnet.nets.ctc_prefix_score import CTCPrefixScore
"
6	"    assert out22.size() == (2, 32)
    assert out23.size() == (2, 32)
    assert out24.size() == (2, 32)
    assert conv((x1, x2), edge_index, size=(4, 2)).tolist() == out21.tolist()
    assert conv((x1, x2), edge_index, value, (4, 2)).tolist() == out22.tolist()
"
2	"    of that host hash and invoke the command there.

    The method returns immediately after launching the command if background is True (default).
    When background is set to False, this method waits for command termination and returns
"
1	"import torch

"
2	"      run: test-mixed-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
"
2	"                optimizer.zero_grad()
                output = model(data)
                loss = F.cross_entropy(output, target)
"
2	">         return conf

"
4	"Brittany spaniel
clumber, clumber spaniel
English springer, English springer spaniel
Welsh springer spaniel
cocker spaniel, English cocker spaniel, cocker
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
"
2	"    @mock.patch('horovod.run.util.lsf.LSFUtils.get_num_gpus', MagicMock(return_value=4))
    @mock.patch('horovod.run.util.lsf.LSFUtils.get_num_cores', MagicMock(return_value=4))
    @mock.patch('horovod.run.util.lsf.LSFUtils.get_num_threads', MagicMock(return_value=4))
    def test_generate_jsrun_rankfile(self):
        settings = hvd_settings.Settings(
"
2	"                    help='number of batches per epoch')
parser.add_argument('--batches-per-commit', type=int, default=1,
                    help='number of batches per commit of the elastic state object')
parser.add_argument('--epochs', type=int, default=3,
                    help='number of epochs')
"
3	"    def _get_encoder(self):
        self.input_dim = 200
        self.target_output_dim = 7
        encoder = FastICAEncoder(
            output_dim=self.target_output_dim, whiten=True, num_features=self.input_dim)
"
2	"    def _wait(self, key, state, rendezvous_id):
        while True:
"
4	"cellular telephone, cellular phone, cellphone, cell, mobile phone
chain
chainlink fence
"
2	"    )

    num_steps = 50

    np.random.seed(1 + hvd.rank())
"
5	"                x += layout.XS
            x += 2*layout.XS
"
4	"espresso maker
face powder
feather boa, boa
file, file cabinet, filing cabinet
"
2	"  retry:
    automatic: true
  agents:
    queue: cpu
- label: ':tensorflow: Test TensorFlow MNIST (test-cpu-oneccl-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
"
2	"            hvd.callbacks.BroadcastGlobalVariablesCallback(root_rank=0),

            # Horovod: average metrics among workers at the end of every epoch.
"
2	"        """"""Returns and sets the static CSM allocation info.""""""
        if not LSFUtils._csm_allocation_info:
            lsf_allocation_id = os.environ[""CSM_ALLOCATION_ID""].strip()
"
3	"  is_extendable=False,
  syntax='proto3',
"
4	"sunscreen, sunblock, sun blocker
suspension bridge
swab, swob, mop
sweatshirt
"
4	"chest
chiffonier, commode
chime, bell, gong
"
2	"  agents:
    queue: 2x-gpu-g4
- label: ':tensorflow: Test TensorFlow 2.0 MNIST (test-gpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0)'
  command: bash -c ""\$(cat /mpirun_command) python /horovod/examples/tensorflow2_mnist.py""
  plugins:
"
2	"    def __init__(self, model, optimizer=None, **kwargs):
        super(KerasState, self).__init__(model, optimizer=optimizer, backend=keras.backend, **kwargs)


class CommitStateCallback(_impl.CommitStateCallbackImpl, keras.callbacks.Callback):
"
4	"miniskirt, mini
minivan
"
4	"
def init_fc_layer(fout, name='fc'):
"
4	"grille, radiator grille
grocery store, grocery, food market, market
guillotine
hair slide
hair spray
"
2	"# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
"
2	"      login: true
  timeout_in_minutes: 30
  retry:
    automatic: true
  agents:
"
4	"wool, woolen, woollen
worm fence, snake fence, snake-rail fence, Virginia fence
wreck
yawl
yurt
"
2	"  retry:
    automatic: true
  agents:
"
4	"            attr=offset_param, shape=input.shape[1:2], dtype=dtype)

        tmp = fluid.layers.elementwise_mul(x=(input - mean), y=scale, axis=1)
        tmp = tmp / fluid.layers.sqrt(var + epsilon)
        tmp = fluid.layers.elementwise_add(tmp, offset, axis=1)
"
2	"        driver_ip = network.get_driver_ip(nics)
        gloo_run(settings, nics, env, driver_ip, command)

    def mpi_run_fn():
"
4	"lawn mower, mower
lens cap, lens cover
letter opener, paper knife, paperknife
"
3	"        def validate(req):
            self.assertEqual(req.status.code, jina_pb2.Status.ERROR)
            self.assertEqual(len(req.status.details), 2)
            self.assertEqual(req.status.details[0].executor, 'DummyCrafter')
            self.assertEqual(req.status.details[1].executor, 'BaseEncoder')
"
4	"knot
lab coat, laboratory coat
ladle
"
4	"nematode, nematode worm, roundworm
conch
snail
slug
"
2	"from horovod.spark.driver.driver_service import SparkDriverService
from horovod.run.elastic.discovery import HostDiscovery


"
4	"```

"
1	"            outstates.append(outstate)
            scores.append(score)
"
1	"                     [1, 1, 1, 1, 1]],
                    [[1, 0, 0, 0, 0],
                     [1, 1, 0, 0, 0],
                     [1, 1, 1, 0, 0],
                     [1, 1, 1, 0, 0],
"
4	"muzzle
nail
neck brace
necklace
nipple
"
4	"magnetic compass
mailbag, postbag
mailbox, letter box
maillot
maillot, tank suit
"
4	"        global_params = global_params._replace(**override_params)
    return blocks_args, global_params
"
2	"
from tensorflow.python.framework import ops

"
4	"    model = EfficientNet(
        name='b4',
        is_test=is_test,
        padding_type=padding_type,
"
4	"wool, woolen, woollen
worm fence, snake fence, snake-rail fence, Virginia fence
wreck
"
2	"      run: test-gpu-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
"
2	"        local_rank = hvd.local_rank()
        size = hvd.size()

        dtypes = [tf.uint8, tf.int8, tf.uint16, tf.int16,
                  tf.int32, tf.int64, tf.float16, tf.float32,
"
4	"                drop_connect_rate = self._global_params.drop_connect_rate
                if drop_connect_rate:
                    drop_connect_rate *= float(idx) / block_size
                conv = self.mb_conv_block(conv, block_args, is_test,
                                          drop_connect_rate,
"
2	"        subprocess.check_output(['wget', data_url, '-O', libsvm_path])

    # Load dataset into a Spark DataFrame
    df = spark.read.format('libsvm') \
        .option('numFeatures', '784') \
"
2	"        self._save_model()
        super(TensorFlowState, self).save()
"
4	"knee pad
knot
"
2	"    queue: cpu
- label: ':muscle: Test Stall (test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""\$(cat /oneccl_env) && echo '/mpirun_command_ofi' > /mpirun_command && \$(cat /mpirun_command) python /horovod/test/test_stall.py""
"
4	"            usage='%(prog)s',
            add_help=True)
        self.arg_input_group = self.parser.add_argument_group(
            title=""Input options"", description=""Input data. Required"")
"
4	"            attr=scale_param, shape=input.shape[1:2], dtype=dtype)
        offset = helper.create_parameter(
            attr=offset_param, shape=input.shape[1:2], dtype=dtype)

"
2	"
    try:
        return mpi_lib.horovod_torch_join(device)
    except RuntimeError as e:
"
4	"                batch_image
            ]) if use_gpu else self.cpu_predictor.run([batch_image])
"
2	"                                              attempts=attempts)

    def sleep(self):
        self._send(SleepRequest())
"
2	"  timeout_in_minutes: 5
  retry:
"
4	"            input=input,
            num_filters=num_filters,
"
2	"        """"""Map of interface to list of (ip, port) pairs of other task service.""""""


class GetTaskToTaskAddressesResponse(object):
    def __init__(self, task_addresses_for_task):
"
4	"bull mastiff
Tibetan mastiff
French bulldog
Great Dane
"
4	"# -*- coding:utf-8 -*-
# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.
#
"
0	"
    def test_cube(self):
        btools.utils.cube(self.bm)
        self.assertEquals(len(self.bm.faces), 6)
        self.assertEquals(len(self.bm.verts), 8)
"
4	"        place = fluid.CPUPlace()
        exe = fluid.Executor(place)

        program, feeded_var_names, target_vars = fluid.io.load_inference_model(
"
2	"
    def forward(self, input):
        # currently only GPU input is supported by underlying kernel from PyTorch
        if not input.is_cuda:
            raise ValueError('SyncBatchNorm expected input tensor to be on GPU')
"
2	"

"
4	"# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
"
3	"
        def validate(req):
            self.assertEqual(req.docs[0].chunks[0].text, 'This ,  text is...')
            self.assertEqual(req.docs[0].chunks[1].text, 'Amazing')

"
2	"  command: horovodrun -np 2 -H localhost:2 python /horovod/examples/tensorflow_mnist.py
  plugins:
  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
"
2	"    def has_rank_assignment(self, host, slot):
        if self._host_manager.is_blacklisted(host):
            return False
        return host in self._host_assignments and len(self._host_assignments[host]) > slot

"
2	"            return tf.group(*[var.assign(broadcast(var, root_rank))
                              for var in variables])

        return broadcast_group

"
2	"      login: true
  timeout_in_minutes: 5
"
2	"- label: ':pytest: Run Cluster PyTests (test-gpu-openmpi-py3_6-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_6_0-pyspark2_4_0)'
  command: bash -c "" /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd test_static_run.py""
  plugins:
  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-py3_6-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_6_0-pyspark2_4_0
"
0	"        sys.modules[mod_name] = mod

        # Fianally, execute the module.
        exec(compile(f.read(), filepath, 'exec'), mod.__dict__)
    except IOError:
"
4	"            context_prog (fluid.Program): program for transfer learning.
        """"""
"
4	"pay-phone, pay-station
pedestal, plinth, footstall
pencil box, pencil case
"
6	"            there are any). (default :obj:`None`)
        towers (int, optional): Number of towers (default: :obj:`1`).
        pre_layers (int, optional): Number of transformation layers before
"
4	"        args = [
            'r%d' % block.num_repeat,
            'k%d' % block.kernel_size,
"
4	"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
"
6	"        elif isinstance(value, SparseTensor):
            return (0, 1)
        return 0

"
4	"schipperke
groenendael
"
4	"        return out, pool

"
2	"
    """"""
    Tests mpi_run without PYTHONPATH set.
"
4	"
    need_crop = False
    if padding_type == ""SAME"":
"
4	"badger
armadillo
"
4	"black and gold garden spider, Argiope aurantia
barn spider, Araneus cavaticus
garden spider, Aranea diademata
black widow, Latrodectus mactans
tarantula
"
1	"    def forced_align(self, h, y, blank_id=0):
        """"""forced alignment.

        :param torch.Tensor h: hidden state sequence, 2d tensor (T, D)
        :param torch.Tensor y: id sequence tensor 1d tensor (L)
"
4	"bolo tie, bolo, bola tie, bola
bonnet, poke bonnet
bookcase
"
4	"        idx = 0
        block_size = 0
"
6	"                self.num_relations, self.in_channels_l, self.out_channels)

        if self.num_blocks is not None:  # Block-diagonal-decomposition =====

            if x_l.dtype == torch.long and self.num_blocks is not None:
"
4	"# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
"
1	"            cer_ctc = None

        # 3. forward decoder
        if self.mtlalpha == 1.0:
            loss_att, self.acc, cer, wer = None, None, None, None
"
4	"hub run efficientnetb2_imagenet --input_path ""/PATH/TO/IMAGE""
```
"
4	"            attr=scale_param, shape=input.shape[1:2], dtype=dtype)
        offset = helper.create_parameter(
            attr=offset_param, shape=input.shape[1:2], dtype=dtype)

"
3	"            else:
                setattr(chunk, k, v)
        if new_chunk_id:
            chunk.chunk_id = random.randint(0, ctypes.c_uint(-1).value)

"
1	"        )
    )
    message += f""    Size: {num_bytes}\n""
    dtype = next(iter(model.parameters())).dtype
    message += f""    Type: {dtype}""
"
4	"

def EfficientNetB4(is_test=False,
"
4	"                        self.default_pretrained_model_path,
                        context_prog,
"
2	"target = tf.random_uniform([batch_size, 1], minval=0, maxval=2, dtype=tf.int64)

probs = tf.layers.dense(data, 2, activation=None)
loss = tf.losses.sparse_softmax_cross_entropy(target, probs)
"
4	"hand blower, blow dryer, blow drier, hair dryer, hair drier
hand-held computer, hand-held microcomputer
handkerchief, hankie, hanky, hankey
hard disc, hard disk, fixed disk
"
2	"        if settings.verbose >= 2:
            print('Launching horovod task function: {}'.format(command))
        args_list.append([command])
    # Each thread will use ssh command to launch the server on one task. If an
    # error occurs in one thread, entire process will be terminated. Otherwise,
"
3	"                self.logger.warning(f'{r} is missing in your manifest.yml, you may want to check it')

        # check name
"
4	"            num_filters=out_channels,
            filter_size=1,
"
1	"    if [[ $(get_yaml.py ${train_config} model-module) = *transformer* ]]; then
        recog_model=model.last${n_average}.avg.best
        average_checkpoints.py --backend ${backend} \
                               --snapshots ${expdir}/results/snapshot.ep.* \
                               --out ${expdir}/results/${recog_model} \
"
6	"
    t = '(Tensor, SparseTensor, OptTensor, Size) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert jit(x1, adj.t()).tolist() == out.tolist()

"
2	"        });

   OP_REQUIRES_OK_ASYNC(context, ConvertStatus(enqueue_result), done);
  }
};
"
4	"pizza, pizza pie
potpie
burrito
"
4	"television, television system
tennis ball
thatch, thatched roof
theater curtain, theatre curtain
"
2	"  retry:
    automatic: true
  agents:
    queue: 4x-gpu-g4
"
4	"whistle
wig
window screen
window shade
"
4	"**返回**

res (list\[dict\]): 分类结果，列表的每一个元素均为字典，其中 key 为识别动物的类别，value为置信度。

"
1	"        )
        if self.loss_type == ""L1"":
            loss = l1_loss + bce_loss
        elif self.loss_type == ""L2"":
"
2	"        self.max_np = max_np
        self.elastic_timeout = elastic_timeout
        self.reset_limit = reset_limit

"
4	"                label_list=self.label_list,
                top_k=top_k)
            res += out
        return res
"
4	"    return int(new_filters)


def round_repeats(repeats, global_params):
"
1	"            x = x.masked_fill(mask == 0, 0.0)

        # second linear layer
"
4	"os.environ['CUDA_VISIBLE_DEVICES'] = '1'

import cv2
"
1	"            new_cache = []
            for c, e in zip(cache[: len(self.encoders_sd)], self.encoders_sd[ns]):
                xs, masks = e(xs, masks, cache=c)
                new_cache.append(xs)
            for c, e in zip(cache[: len(self.encoders_sd) :], self.encoders_rec):
"
4	"
        return conv
"
4	"        return conv

"
2	"    :return: Comma separated string of <IP address> or <host name>:<Number of GPUs>
    """"""
"
4	"    elif act == 'leaky_relu':
        conv = fluid.layers.leaky_relu(
            conv, alpha=relufactor, name=name + '_leaky_relu')
    elif act == 'tanh':
        conv = fluid.layers.tanh(conv, name=name + '_tanh')
"
1	"            text_lengths (LongTensor): Batch of lengths of each input batch (B,).
            speech (Tensor): Batch of padded target features (B, Lmax, odim).
            speech_lengths (LongTensor): Batch of the lengths of each target (B,).
            spembs (Tensor, optional): Batch of speaker embeddings (B, spk_embed_dim).

"
1	"
        Args:
            y: previous char
            state: previous state
"
4	"                global_vars = context_prog.global_block().vars
                inputs = {
                    key: global_vars[value]
                    for key, value in inputs.items()
"
4	"ski mask
sleeping bag
"
2	"
import mock
"
2	"        self.skipTest('This test fails due to https://github.com/horovod/horovod/issues/1994')

"
4	"shower cap
shower curtain
ski
ski mask
"
4	"        left_padding, right_padding = cal_padding(input.shape[2], stride,
                                                  filter_size)
"
4	"        model_name = 'efficientnet-' + name
        self._blocks_args, self._global_params = get_model_params(
            model_name, override_params)
"
2	"    """"""
    def test_mpi_run_without_pythonpath(self):
        self.do_test_mpi_run_env_override({}, {}, 'PYTHONPATH', None)

"
1	"        # fill missing arguments for compatibility
        args = fill_missing_args(args, self.add_arguments)

"
3	"                   '--random-doc-id', '--timeout-ready', '--filter-by'],
        'export-api': ['--help', '--yaml-path', '--json-path']}}
"
2	"

class DiscoveredHosts(object):
    def __init__(self, host_slots, host_assignment_order):
        self._host_slots = host_slots
"
4	"rifle
rocking chair, rocker
rotisserie
rubber eraser, rubber, pencil eraser
"
3	"  fields=[
    _descriptor.FieldDescriptor(
"
2	"        if input.dim() < 2:
            raise ValueError('expected at least 2D input (got {}D input)'.format(input.dim()))

    def _run_bn(self, input):
"
3	"        a.save()
        a.close()

        b = BaseIndexer.load_config('annoy-wrap.yml')
"
4	"lemon
fig
"
4	"triceratops
thunder snake, worm snake, Carphophis amoenus
"
6	"    row, col = edge_index
    value = torch.rand(row.size(0), 3)
    adj = SparseTensor(row=row, col=col, value=value, sparse_sizes=(4, 4))

    conv = CGConv(8, dim=3)
"
4	"dishrag, dishcloth
dishwasher, dish washer, dishwashing machine
disk brake, disc brake
dock, dockage, docking facility
"
2	"            self._register_hooks()

    def load_state_dict(self, *args, **kwargs):
        self._handles = {}
        self._synchronized = False
"
2	"    })
    .Doc(R""doc(
"
4	"def EfficientNetB5(is_test=False,
                   padding_type='SAME',
                   override_params=None,
                   use_se=True):
    model = EfficientNet(
"
3	"
        g = Flow(timeout_ready=-1).add(yaml_path='yaml/test-joint-wrap.yml')
"
4	"spoonbill
flamingo
little blue heron, Egretta caerulea
American egret, great white heron, Egretta albus
"
2	"        self._default_slots = slots
        super(HostDiscoveryScript, self).__init__()

"
2	"        self.assertEqual(1, results[0]['rendezvous'])

        self.assertEqual(1, results[1]['start_rank'])
"
2	"
    The purpose of this decorator is to allow for uninterrupted execution of the wrapped function
    across multiple workers in parallel, as workers come and go from the system. When a new worker is added,
    its state needs to be brought to the same point as the other workers, which is done by synchronizing
"
2	"                                          stdout=stdout, stderr=stderr, verbose=verbose)

        self.assertFalse(str(e.value).startswith('Timed out waiting for Spark tasks to start.'),
                         'Spark timed out before mpi_run was called, test setup is broken.')
"
4	"        final_oup = block_args.output_filters
        conv = self.conv_bn_layer(
"
4	"        'efficientnet-b6': (1.8, 2.6, 528, 0.5),
        'efficientnet-b7': (2.0, 3.1, 600, 0.5),
    }
    return params_dict[model_name]
"
1	"        att_ws = torch.stack(att_ws, dim=0)

        return outs, probs, att_ws

    def _add_first_frame_and_remove_last_frame(self, ys):
"
4	"seashore, coast, seacoast, sea-coast
valley, vale
volcano
"
2	"from horovod.torch.functions import broadcast_object, broadcast_optimizer_state, broadcast_parameters


def run(func):
    """"""Decorator used to run the elastic training process.
"
2	"        if args.epoch_wait > 0:
            time.sleep(args.epoch_wait)

"
2	"    Args:
        model: Keras model.
"
4	"import os
from unittest import TestCase, main
os.environ['CUDA_VISIBLE_DEVICES'] = '1'

import cv2
"
4	"# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
"
6	"    assert out1.size() == (2, 32)
    assert out2.size() == (2, 32)
    assert conv((x1, x2), edge_index, value, (4, 2)).tolist() == out1.tolist()
"
4	"coyote, prairie wolf, brush wolf, Canis latrans
dingo, warrigal, warragal, Canis dingo
dhole, Cuon alpinus
"
4	"    main()

"
1	"        self.always_2d = always_2d
        self.normalize = normalize
        self.data = read_2column_text(fname)

"
4	"espresso
cup
eggnog
alp
bubble
"
2	"from horovod.run.util import lsf
from distutils.spawn import find_executable
from horovod.run.mpi_run import _get_mpi_implementation_flags, _MPI_NOT_FOUND_ERROR_MSG
"
0	"    BTOOLS_OT_create_facemap_material,
)


def register_material():
"
4	"        else:
            bias_attr = False
    return param_attr, bias_attr
"
4	"    data = cv2.imdecode(data, cv2.IMREAD_COLOR)
    return data
"
2	"
        # we inject the secret key here
"
4	"        name='b4',
        is_test=is_test,
"
4	"            if len(input.shape) > 2:
                fan_in = input.shape[1] * input.shape[2] * input.shape[3]
            else:
                fan_in = input.shape[1]
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
"
4	"bearskin, busby, shako
beer bottle
beer glass
bell cote, bell cot
bib
"
4	"crutch
cuirass
"
4	"saltshaker, salt shaker
sandal
"
4	"            return inputs, outputs, main_program

"
3	"            return msg

"
2	"- label: ':spark: Spark Torch MNIST (test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/pytorch_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""
  plugins:
  - docker-compose#v2.6.0:
      run: test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
"
4	"    return model


def EfficientNetB0(is_test=False,
"
4	"window shade
Windsor tie
wine bottle
"
4	"        Encodes a list of BlockArgs to a list of strings.
        :param blocks_args: a list of BlockArgs namedtuples of block args
        :return: a list of strings, each string is a notation of block
        """"""
        block_strings = []
"
2	"        for event in events:
            on_event(event, task_client.abort_command, stop=stop)

"
2	"    })

    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
    state = hvd.elastic.TorchState(model, optimizer, batch=0, epoch=0, commits=0, rendezvous=0)
    return train(state, dir)
"
1	"import os
import pytest

from math import isclose
"
2	"
def check_rank(epoch):
    if epoch == 2 and int(os.environ.get('HOROVOD_RANK')) == 0:
"
4	"Staffordshire bullterrier, Staffordshire bull terrier
American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier
Bedlington terrier
"
4	"                fan_in = input.shape[1] * input.shape[2] * input.shape[3]
            else:
                fan_in = input.shape[1]
"
4	"red wine
espresso
cup
eggnog
"
4	"                    input_filters=block_args.output_filters, stride=1)
            for _ in range(block_args.num_repeat - 1):
                drop_connect_rate = self._global_params.drop_connect_rate
                if drop_connect_rate:
                    drop_connect_rate *= float(idx) / block_size
"
4	"print(r.json()[""results""])
```

"
2	"        pending_slots = [slot_info
                         for host, slots in self._host_assignments.items()
"
2	"                                  'code on the first call. Subsequent calls will be retried until timeout.')

"
2	"from __future__ import print_function

import os
"
2	"    model.compile(opt, 'mae', metrics=[exp_rmspe])
    model_bytes = serialize_model(model)


    def train_fn(model_bytes):
"
2	"        # Days & weeks of promotion, cap to 25 weeks.
        df = df.withColumn('Promo2Since',
                           F.expr('date_add(format_string(""%s-01-01"", Promo2SinceYear), (cast(Promo2SinceWeek as int) - 1) * 7)'))
        df = df.withColumn('Promo2Days',
                           F.when(df.Promo2SinceYear > 1900,
"
1	"    dataset = IterableESPnetDataset(
        path_name_type_list=[(csv_float, ""data8"", ""csv_float"")], preprocess=preprocess,
    )

    for key, data in dataset:
"
4	"        name='b0',
        is_test=is_test,
"
1	"            model.parameters(),
            args.adim,
            args.transformer_warmup_steps,
            args.transformer_lr,
"
3	"    $(""#line"").removeClass(""three"");
    $(""#line"").removeClass(""four"");
})

$("".pay"").click(function () {
"
4	"television, television system
tennis ball
"
4	"kuvasz
schipperke
groenendael
"
2	"    event = register_shutdown_event()
    run_command = get_run_command(command, server_ip, nics, global_rendezv_port, elastic=True)
    create_worker = _create_elastic_worker_fn(exec_command, run_command, env, event)

    driver.start(settings.num_proc, create_worker)
"
1	"        nn.init.xavier_uniform(self.linear_weight.weight)
        self.linear_weight_f = nn.Linear(n_feat, self.kernel_size)
        nn.init.xavier_uniform(self.linear_weight_f.weight)
        self.act = nn.GLU()

"
2	"        logging.info('shutting down Spark cluster')
        self._shutdown.set()
        self.stop_workers()
        self.stop_master()
"
2	"    :param settings: the object that contains the setting for running horovod
    :type settings: Horovod.run.common.util.settings.Settings
"
1	"        super().__init__()

        # store hyperparameters
        self.idim = idim
"
2	"    """"""
    def test_spark_run_with_non_zero_exit_with_gloo(self):
"
4	"motor scooter, scooter
mountain bike, all-terrain bike, off-roader
mountain tent
mouse, computer mouse
mousetrap
"
6	"        if self.add_self_loops:
            if isinstance(edge_index, Tensor):
"
4	"indri, indris, Indri indri, Indri brevicaudatus
Indian elephant, Elephas maximus
"
2	"
    print('===================================')
    print('Data frame with transformed columns')
    print('===================================')
    train_df.show()
"
1	"        Returns:
            tuple[torch.Tensor, List[Any]]: Tuple of
                batchfied scores for next token with shape of `(n_batch, n_vocab)`
                and next state list for ys.
"
2	"    # Pass secret key through the environment variables.
    env[secret.HOROVOD_SECRET_KEY] = codec.dumps_base64(settings.key)
"
2	"    data, target = data.cuda(), target.cuda()

"
1	"                            logdelta[t - 1, s - 1],
                            logdelta[t - 1, s - 2],
"
4	"folding chair
football helmet
forklift
"
4	"    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%
        new_filters += divisor
    return int(new_filters)


"
2	"      run: test-cpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
"
4	"            width_coefficient=w, depth_coefficient=d, dropout_rate=p)
    else:
        raise NotImplementedError(
            'model name is not pre-defined: %s' % model_name)
    if override_params:
"
2	"from pyspark.sql.types import ArrayType, BooleanType, DoubleType, FloatType, IntegerType, \
    NullType, StructField, StructType
"
1	"    from pypinyin import pinyin
    from pypinyin import Style
    from pypinyin.style._utils import get_finals
    from pypinyin.style._utils import get_initials

"
1	"
    def __init__(
        self,
"
4	"Shetland sheepdog, Shetland sheep dog, Shetland
collie
Border collie
Bouvier des Flandres, Bouviers des Flandres
Rottweiler
"
2	"
    def get_host_event(self, host):
        return self._hosts_state[host].get_event()

"
3	"        self.to_device()
        import tensorflow_hub as hub
        self.model = hub.load(self.model_url)

    @batching
"
4	"                                         self._global_params))
            block_size += 1
"
1	"
    This implementation is based on
    https://github.com/pytorch/fairseq/tree/master/fairseq

    Args:
"
4	"wood rabbit, cottontail, cottontail rabbit
hare
Angora, Angora rabbit
"
1	"
\3-grams:
"
4	"assault rifle, assault gun
backpack, back pack, knapsack, packsack, rucksack, haversack
bakery, bakeshop, bakehouse
"
4	"                      use_bn=True,
                      bn_mom=0.9,
                      bn_eps=1e-05,
                      use_bias=False,
                      name=None,
"
4	"stove
strainer
streetcar, tram, tramcar, trolley, trolley car
stretcher
"
4	"broccoli
cauliflower
zucchini, courgette
spaghetti squash
acorn squash
"
4	"

def efficientnet(width_coefficient=None,
                 depth_coefficient=None,
                 dropout_rate=0.2,
"
6	"    t = '(OptPairTensor, Tensor, Size) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert jit((x1, x2), edge_index).tolist() == out1.tolist()
    assert jit((x1, x2), edge_index, size=(4, 2)).tolist() == out1.tolist()
    assert jit((x1, None), edge_index, size=(4, 2)).tolist() == out2.tolist()
"
1	"                    # will be (2 x beam) hyps at most
                    hyps_best_kept.append(new_hyp)

"
2	"            self._host_manager.blacklist(host)

"
6	"    def __init__(self, data, num_parts: int, recursive: bool = False,
                 save_dir: Optional[str] = None, log: bool = True):

"
2	"                                    '--allow-run-as-root --tag-output '
                                    '-np {expected_np} -H [^ ]+ '
                                    '{binding_args} '
                                    '{mpi_flags}  '
                                    '-mca btl_tcp_if_include [^ ]+ -x NCCL_SOCKET_IFNAME=[^ ]+  '
"
2	"                    help='use fp16 compression during allreduce')

"
6	"    jit = torch.jit.script(conv.jittable(t))
    assert jit(x1, adj.t()).tolist() == out.tolist()

    adj = adj.sparse_resize((4, 2))
    conv = CGConv((8, 16))
"
4	"Irish terrier
Norfolk terrier
Norwich terrier
Yorkshire terrier
wire-haired fox terrier
"
4	"    print(res)
    res = b1.classification(images=test_image)
    print(res)
    res = b1.classify(images=test_image)
    print(res)
"
0	"    arc_direction = edge_vector(edge).cross(xyz[2])
    orient = xyz[1] if edge_is_vertical(edge) else xyz[0]
"
4	"
    Args:
        data_out (numpy.ndarray): output data of network.
        label_list (list): list of label.
"
2	"                    [r * 3 + 1],
                    [r * 4 + 1]
                ])
                for r in range(hvd.size())
            ]),
"
6	"

"
2	"    :type local_host_names: set
    :param driver_addresses: map of interfaces and their address and port for
    the service. For example:
        {
            'lo': [('127.0.0.1', 34588)],
"
4	"    return int(new_filters)


"
6	"    y = 2.
    z = torch.tensor(0.)
    name = 'data'
"
4	"        return BlockArgs(
            kernel_size=int(options['k']),
"
6	"    # propagate_type: (x: Tensor)
    def forward(self, x: Tensor, edge_index: Adj) -> Tensor:
"
2	"- label: ':muscle: Single MXNet MNIST (test-cpu-oneccl-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""\$(cat /oneccl_env) && echo '/mpirun_command_mpi' > /mpirun_command && python /horovod/examples/mxnet_mnist.py --epochs 3""
  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-oneccl-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
"
6	"    out = conv(x, edge_index)
    assert out.size() == (4, 16)
    assert torch.allclose(conv(x, adj.t()), out, atol=1e-6)
"
4	"beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon
beacon, lighthouse, beacon light, pharos
"
4	"
import os
"
1	"
            if att_ws.ndim == 2:
                att_ws = att_ws[None][None]
            elif att_ws.ndim != 4:
"
4	"brassiere, bra, bandeau
breakwater, groin, groyne, mole, bulwark, seawall, jetty
breastplate, aegis, egis
"
2	"        self.assertEqual(3, len(results))

        self.assertEqual(0, results[0]['start_rank'])
        self.assertEqual(2, results[0]['size'])
        self.assertEqual(1, results[0]['rendezvous'])
"
6	"
def test_dna_conv():
    x = torch.randn((4, 3, 32))
"
2	"            self._driver.stop()

"
2	"
def mpi_available(env=None):
    return _get_mpi_implementation(env) not in {_UNKNOWN_IMPL, _MISSING_IMPL}

"
6	"    x2 = torch.randn(2, 16)
    edge_index = torch.tensor([[0, 1, 2, 3], [0, 0, 1, 1]])
    row, col = edge_index
    value = torch.randn(row.size(0))
    adj = SparseTensor(row=row, col=col, value=value, sparse_sizes=(4, 4))
"
1	"                    hs_pad[i], hs_mask[i], ys_pad[i], batch_size
                )

            # 4. compute attention loss
            # The following is just an approximation
"
4	"        use_se=use_se)
    return model


"
2	"        state = hvd.elastic.TorchState(model1, optimizer, batch=20 + hvd.rank(), epoch=10 + hvd.rank())
        state.sync()

"
1	"                    if lpz is not None:
                        new_hyp[""ctc_state_prev""] = ctc_states[joint_best_ids[0, j]]
                        new_hyp[""ctc_score_prev""] = ctc_scores[joint_best_ids[0, j]]
"
2	"data = tf.random.uniform([batch_size, 2])
indices = tf.random.uniform([batch_size], minval=0, maxval=2, dtype=tf.int64)
target = tf.one_hot(indices, 2)

"
2	"parser.add_argument('--no-cuda', action='store_true', default=False,
                    help='disables CUDA training')
parser.add_argument('--seed', type=int, default=42, metavar='S',
                    help='random seed (default: 42)')
"
4	"
    def conv_bn_layer(self,
                      input,
                      filter_size,
                      num_filters,
"
4	"                inputs,
                num_filters=oup,
                filter_size=1,
"
2	"            expected_pipeline = ''.join(lines)

        gen_pipeline_env = 'BUILDKITE_PIPELINE_SLUG=SLUG BUILDKITE_BRANCH=BRANCH'
        gen_pipeline_cmd = '{env} ../.buildkite/gen-pipeline.sh'.format(env=gen_pipeline_env)
"
4	"bloodhound, sleuthhound
bluetick
black-and-tan coonhound
"
4	"            input=inputs, pool_type='avg', global_pooling=True, use_cudnn=False)
        x_squeezed = conv2d(
            x_squeezed,
            num_filters=num_squeezed_channels,
"
4	"            out = postprocess(
                data_out=predictor_output[0].as_ndarray(),
"
2	"
    def _notify_workers_host_changes(self, current_hosts):
        next_host_assignments = {}
"
2	"  retry:
    automatic: true
  agents:
"
6	"
# Compute in-degree histogram over training data.
deg = torch.zeros(5, dtype=torch.long)
for data in train_dataset:
    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)
"
2	"            self.mode = 1
            self.param = torch.nn.Parameter(torch.FloatTensor([1., -1., 1.]))
"
4	"classifier = hub.Module(name=""efficientnetb4_imagenet"")

"
4	"tennis ball
thatch, thatched roof
theater curtain, theatre curtain
"
2	"
            expected_rankfile = (
"
2	"  plugins:
  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0
      config: docker-compose.test.yml
"
6	"            if item.size(0) == N:
                data[key] = item[node_idx]
            if item.size(0) == E:
                data[key] = item[edge_idx]
"
2	"    test_df = cast_columns(test_df, continuous_cols)
    test_df = lookup_columns(test_df, vocab)

"
0	"
class TestFloorplan(unittest.TestCase):

    @classmethod
"
1	"        elayers=1,
        eunits=4,
        adim=4,
        aheads=2,
        dlayers=1,
"
2	"            self.skipTest(""No GPUs available"")

"
4	"football helmet
forklift
"
4	"                   use_se=True):
    model = EfficientNet(
        name='b4',
"
2	"  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
"
4	"            conv, alpha=relufactor, name=name + '_leaky_relu')
    elif act == 'tanh':
"
4	"torch
totem pole
"
4	"
def EfficientNetB7(is_test=False,
                   padding_type='SAME',
                   override_params=None,
                   use_se=True):
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: cpu
"
4	"                    'feature_map': name_prefix + feature_map.name
                }
"
6	"    x1 = torch.randn(4, 16)
    pos1 = torch.randn(4, 3)
    pos2 = torch.randn(2, 3)
    edge_index = torch.tensor([[0, 1, 2, 3], [0, 0, 1, 1]])
"
1	"
    inputs = dict(
        text=torch.randint(0, 10, (2, 4)),
        text_lengths=torch.tensor([4, 1], dtype=torch.long),
"
4	"                 padding_type='SAME',
                 override_params=None,
                 is_test=False,
"
4	"pillow
ping-pong ball
pinwheel
pirate, pirate ship
"
4	"            'i%d' % block.input_filters,
            'o%d' % block.output_filters
        ]
        if 0 < block.se_ratio <= 1:
"
4	"loudspeaker, speaker, speaker unit, loudspeaker system, speaker system
loupe, jeweler's loupe
lumbermill, sawmill
magnetic compass
mailbag, postbag
"
2	"    def _get_host_assignments(self, current_hosts):
        # Adjust the host assignments to account for added / removed hosts
        host_list = [hosts.HostInfo(host, current_hosts.get_slots(host))
                     for host in current_hosts.host_assignment_order]
        host_assignments_list = hosts.get_host_assignments(host_list, self._min_np, self._max_np)
"
2	"            shape_tests_passed, value_tests_passed = \
                self.evaluate([tf.reduce_all(shape_tests), tf.reduce_all(tests)])

"
6	"
    conv = PNAConv(16, 32, aggregators, scalers,
                   deg=torch.tensor([0, 3, 0, 1]), edge_dim=3, towers=4)
"
4	"import re
import math
import copy

"
4	"# You may obtain a copy of the License at
#
"
4	"            low=-init_range, high=init_range))
    bias_attr = fluid.ParamAttr(
        name=name + '_offset',
"
2	"- label: ':muscle: Single MXNet MNIST (test-cpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0)'
  command: bash -c "" python /horovod/examples/mxnet_mnist.py --epochs 3""
  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0
"
4	"barbershop
barn
"
1	"        # GLU activation
        x = self.act(x)
"
4	"            name='_fc',
            param_attr=param_attr,
            bias_attr=bias_attr)
        return out, pool
"
2	"
@hvd.elastic.run
"
4	"hognose snake, puff adder, sand viper
green snake, grass snake
"
4	"        out_size = max(filter_size - stride, 0)
    else:
"
4	"# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
#
"
2	"    if len(sys.argv) != 5:
        print('Usage: {} <index> <num_hosts> <driver_addresses> <settings>'.format(sys.argv[0]))
"
4	"    return int(math.ceil(multiplier * repeats))

"
1	"        return len(self.data)

    def __iter__(self):
"
2	"
        model2 = torch.nn.Sequential(torch.nn.Linear(2, 2))
        model2.load_state_dict({
            '0.weight': torch.tensor([[1.0, 2.0], [3.0, 4.0]]),
            '0.bias': torch.tensor([0.0, 0.0])
"
4	"tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui
loggerhead, loggerhead turtle, Caretta caretta
"
4	"            class_dim,
            name='_fc',
"
4	"tiger shark, Galeocerdo cuvieri
hammerhead, hammerhead shark
electric ray, crampfish, numbfish, torpedo
stingray
"
4	"            param_attr, bias_attr = init_batch_norm_layer(bn_name)
            return fluid.layers.batch_norm(
                input=conv,
"
2	"    train_opt = optimizer.minimize(loss)
    train(state, lambda: session.run(train_opt))

"
4	"Brittany spaniel
clumber, clumber spaniel
English springer, English springer spaniel
Welsh springer spaniel
cocker spaniel, English cocker spaniel, cocker
"
4	"        conv = fluid.layers.tanh(conv, name=name + '_tanh')
    elif act == 'sigmoid':
        conv = fluid.layers.sigmoid(conv, name=name + '_sigmoid')
    elif act == 'swish':
        conv = fluid.layers.swish(conv, name=name + '_swish')
"
2	"def is_gloo_used(use_gloo=None, use_mpi=None, use_jsrun=None):
    # determines whether run_controller will run gloo
    # for the given (use_gloo, _, use_mpi, _, use_jsrun, _, _)
"
3	"             r'(?:\.[0-9a-zA-Z-]+)*))?$'
name_regex = r'^[a-zA-Z_$][a-zA-Z_\s\-$0-9]{2,20}$'
"
4	"    version=""1.1.0"")
class EfficientNetB6ImageNet(hub.Module):
"
2	"            with is_built(gloo_is_built=use_gloo, mpi_is_built=use_mpi):
                with pytest.raises(Exception, match='^Timed out waiting for Spark tasks to start.'):
                    horovod.spark.run(None, num_proc=4, start_timeout=5,
                                      use_mpi=use_mpi, use_gloo=use_gloo,
"
4	"Loafer
lotion
loudspeaker, speaker, speaker unit, loudspeaker system, speaker system
loupe, jeweler's loupe
lumbermill, sawmill
"
4	"Polaroid camera, Polaroid Land camera
pole
police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria
poncho
pool table, billiard table, snooker table
"
2	"            with open(ckpt_file, 'rb') as f:
                return history.history, f.read()


    # Create Spark session for training.
"
4	"tray
trench coat
tricycle, trike, velocipede
trimaran
"
4	"basset, basset hound
beagle
bloodhound, sleuthhound
bluetick
"
4	"    'EfficientNetB2', 'EfficientNetB3', 'EfficientNetB4', 'EfficientNetB5',
    'EfficientNetB6', 'EfficientNetB7'
"
2	"        tensor_compressed, ctx = self._compression.compress(tensor)

        handle = allreduce_async_(tensor_compressed, name=name, op=self.op)
"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
"
4	"
    def _drop_connect(self, inputs, prob, is_test):
        if is_test:
"
4	"

def norm_layer(input, norm_type='batch_norm', name=None):
"
2	"    for name, value in sorted(res.worker_results.items(), key=lambda item: item[1][1]):
        exit_code, timestamp = value
        if exit_code != 0:
            raise RuntimeError('Horovod detected that one or more processes exited with non-zero '
                               'status, thus causing the job to be terminated. The first process '
"
4	"tusker
echidna, spiny anteater, anteater
"
4	"Staffordshire bullterrier, Staffordshire bull terrier
American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier
Bedlington terrier
Border terrier
"
6	"# BGS and AM graphs are too big to process them in a full-batch fashion.
# Since our model does only make use of a rather small receptive field, we
# filter the graph to only contain the nodes that are at most 2-hop neighbors
"
2	"if not hasattr(torch.jit, 'unused'):
    torch.jit.unused = lambda x: x


_SYNC_BN_V2 = (
"
2	"- label: ':spark: Spark Keras Rossmann Run (test-cpu-openmpi-py3_6-tf1_14_0-keras2_2_4-torch1_2_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/keras_spark_rossmann_run.py --num-proc 2 --data-dir file:///data --epochs 3 --sample-rate 0.01""
  plugins:
  - docker-compose#v2.6.0:
"
3	"             .add(name='r2', yaml_path='!BaseEncoder')
             .add(name='r3', yaml_path='!BaseEncoder'))

        # always test two times, make sure the flow still works after it fails on the first
"
2	"        start = self._starting_models[p]

        stashed_params = []
        for group in self.param_groups:
            stashed_params.append(group['params'])
"
2	"
    if stop is None:
        if not daemon:
"
4	"from paddlehub.common.paddle_helper import add_vars_prefix

"
3	"            self.logger.debug(k)
        return f

"
2	"    point before executing `func` again. This ensures that workers do not diverge when such reset events occur.

    It's important to note that collective operations (e.g., broadcast, allreduce) cannot be the call to
"
2	"if __name__ == '__main__':
    args = parser.parse_args()
"
4	"#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
"
4	"            conv = self.se_block(conv, num_squeezed_channels, oup, name)

"
5	"        for group in self.groups:
            unbind_destroy(group)
"
4	"dingo, warrigal, warragal, Canis dingo
dhole, Cuon alpinus
African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus
hyena, hyaena
"
5	"                    m = re.match(
                        '(?P<ncards>a card|(?P<count>[0-9]+) cards) '
                        'from (?P<source_type>stack|freecell) '
"
2	"# Horovod: (optional) compression algorithm.
compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none

# Horovod: wrap optimizer with DistributedOptimizer.
optimizer = hvd.DistributedOptimizer(optimizer,
"
4	"                   padding_type='SAME',
                   override_params=None,
                   use_se=True):
    model = EfficientNet(
        name='b5',
"
2	"## Checklist before submitting

- [ ] Did you read the [contributor guide](https://github.com/horovod/horovod/blob/master/CONTRIBUTING.md)?
- [ ] Did you update the docs?
- [ ] Did you write any tests to validate this change?  
"
1	"def get_parser():
    """"""Get default arguments.""""""
"
2	"    automatic: true
  agents:
    queue: 2x-gpu-g4
- label: ':spark: Spark Keras MNIST (test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/keras_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""
"
4	"
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

"
2	"    queue: 2x-gpu-g4
- label: ':muscle: Test Stall (test-mixed-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c "" \$(cat /mpirun_command) python /horovod/test/test_stall.py""
  plugins:
  - docker-compose#v2.6.0:
"
4	"            pool = fluid.layers.dropout(
                pool,
                self._global_params.dropout_rate,
                dropout_implementation='upscale_in_train')
"
4	"wombat
jellyfish
sea anemone, anemone
"
2	"        self.size = size
        self.local_rank = local_rank
        self.local_size = local_size
        self.cross_rank = cross_rank
"
2	"  retry:
    automatic: true
  agents:
"
2	"    def test_auto_scale_down_by_exception(self):
        hosts = 'host-1:1,host-2:1,host-3:1,host-4:1'

        exit_schedule = {
            str((1, 0)): [0],
"
4	"echidna, spiny anteater, anteater
platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus
"
4	"skunk, polecat, wood pussy
badger
armadillo
"
3	"    yield d


class MyTestCase(JinaTestCase):
"
4	"            padding_type=self.padding_type,
            bn_eps=self._bn_eps,
            name='',
            conv_name='_conv_stem',
"
2	"# Horovod: save checkpoints only on worker 0 to prevent other workers from
# corrupting it.
if hvd.rank() == 0:
    checkpoint.save(checkpoint_dir)

"
4	"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk
damselfly
admiral
ringlet, ringlet butterfly
monarch, monarch butterfly, milkweed butterfly, Danaus plexippus
"
3	"        .. highlight:: python
        .. code-block:: python
            # generate a training file in `.tgz`
"
2	"
class HostDiscovery(object):
    def find_available_hosts_and_slots(self):
        """"""Returns a dict mapping <hostname> -> <number of slots>.""""""
        raise NotImplementedError()
"
2	"                                                      local_rank=alloc_info.local_rank,
                                                      local_size=alloc_info.local_size,
                                                      np=num_proc))

"
4	"
        # Skip connection and drop connect
"
4	"        raise NotImplementedError(""norm tyoe: [%s] is not support"" % norm_type)


"
4	"        x -= tmp.reshape((x.shape[0], 1))
        x = np.exp(x)
        tmp = np.sum(x, axis=1)
        x /= tmp.reshape((x.shape[0], 1))
"
2	"            with tempdir() as d:
                file = os.path.sep.join([d, 'command_executed'])
"
4	"dung beetle
rhinoceros beetle
weevil
fly
"
4	"            name=name + ""_weights"",
            initializer=fluid.initializer.NormalInitializer(
                loc=0.0, scale=math.sqrt(2.0 / n)))
        if use_bias == True:
            bias_attr = fluid.ParamAttr(
"
2	"            local_rank_zero_index = task_indices_on_this_host[0]
        else:
            local_rank_zero_index = None

"
2	"            .withColumn('StateHoliday', df.StateHoliday != '0') \
            .withColumn('SchoolHoliday', df.SchoolHoliday != '0')
"
4	"
* images (list\[numpy.ndarray\]): 图片数据，每一个图片数据的shape 均为 \[H, W, C\]，颜色空间为 BGR；
* paths (list\[str\]): 图片的路径；
* batch\_size (int): batch 的大小；
"
4	"# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
"
4	"        return inputs, outputs, context_prog

    def classify(self,
"
2	"# limitations under the License.
# ==============================================================================
"
4	"American lobster, Northern lobster, Maine lobster, Homarus americanus
spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish
"
4	"
        return conv

"
2	"    hvd.callbacks.LearningRateScheduleCallback(start_epoch=args.warmup_epochs, end_epoch=30, multiplier=1.,
                                               initial_lr=initial_lr),
    hvd.callbacks.LearningRateScheduleCallback(start_epoch=30, end_epoch=60, multiplier=1e-1, initial_lr=initial_lr),
    hvd.callbacks.LearningRateScheduleCallback(start_epoch=60, end_epoch=80, multiplier=1e-2, initial_lr=initial_lr),
"
1	"    for rtask in ${recog_set}; do
        feat_recog_dir=${dumpdir}/${rtask}/delta${do_delta}
        data2json.sh --feat ${feat_recog_dir}/feats.scp \
            --nlsyms ${nlsyms} data/${rtask} ${dict} > ${feat_recog_dir}/data.json
"
4	"        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
        use_se=use_se)
    return model
"
4	"alligator lizard
Gila monster, Heloderma suspectum
green lizard, Lacerta viridis
"
2	"        import horovod.tensorflow.keras as hvd
        from horovod.spark.task import get_available_devices
        import os
        from petastorm import make_batch_reader
"
4	"abacus
abaya
academic gown, academic robe, judge's robe
accordion, piano accordion, squeeze box
"
2	"    Name(""HorovodRank"").Device(DEVICE_GPU).HostMemory(""rank""),
    HorovodReturnScalarOp<int, common::horovod_rank>);
#endif

REGISTER_OP(""HorovodRank"")
"
4	"bicycle-built-for-two, tandem bicycle, tandem
bikini, two-piece
binder, ring-binder
"
4	"           use_bias=False,
           padding_type=None,
"
4	"import warnings

"
2	"            self._saved_optimizer_state = self.optimizer.get_weights()

    def _load_model(self):
"
2	"    queue: cpu
- label: ':tensorflow: Test TensorFlow 2.0 Keras MNIST (test-cpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0)'
"
4	"freight car
French horn, horn
frying pan, frypan, skillet
fur coat
garbage truck, dustcart
"
1	"            positional_dropout_rate=args.dropout_rate,
            attention_dropout_rate=args.transformer_attn_dropout_rate,
            num_spkrs=args.num_spkrs,
"
0	"        description=""Whether to add extruded border around flat roof""
    )

"
2	"def gloo_run_elastic(settings, driver, env):
    """"""
"
2	"
    def get_ranks_to_indices(self):
        self._lock.acquire()
        try:
"
6	"                edge_weight: OptTensor = None) -> Tensor:

"
3	"    $(""#line"").addClass(""three"");
    $(""#line"").removeClass(""two"");
    $(""#line"").removeClass(""one"");
    $(""#line"").removeClass(""four"");
})
"
4	"marimba, xylophone
mask
matchstick
"
4	"        return res

"
3	"        _index = AnnoyIndex(self.num_dim, self.metric)
        vecs = vecs.astype(np.float32)
        for idx, v in enumerate(vecs):
"
4	"    * feature\_map (paddle.fluid.framework.Variable): 特征匹配，全连接层前面的那个张量。
* context\_prog(fluid.Program): 计算图，用于迁移学习。

```python
def classify(images=None,
"
2	"parser.add_argument('--model', type=str, default='resnet50',
                    help='model to benchmark')
parser.add_argument('--batch-size', type=int, default=32,
                    help='input batch size')

"
2	"                self._lock.release()

"
4	"            bn_mom=self._bn_mom,
            bn_eps=self._bn_eps,
            name=name,
            use_cudnn=False,
            conv_name=name + '_depthwise_conv',
"
2	"        driver = ElasticDriver(mock.Mock(), discovery, min_np=8, max_np=12)
        driver.wait_for_available_slots(min_np=16)
        driver.stop()
"
2	"    _CSM_NODE_QUERY = ""/opt/ibm/csm/bin/csm_node_attributes_query""
    _LSCPU_CMD = ""LANG=en_US.utf8 lscpu""
    _THREAD_KEY= ""Thread(s) per core""
    _csm_allocation_info = {}

"
4	"        dropout_rate=dropout_rate,
        drop_connect_rate=drop_connect_rate,
        num_classes=1000,
        width_coefficient=width_coefficient,
        depth_coefficient=depth_coefficient,
"
4	"    global_params = GlobalParams(
        batch_norm_momentum=0.99,
"
4	"            inputs (dict): key is 'image', corresponding vaule is image tensor.
            outputs (dict): key is :
                'classification', corresponding value is the result of classification.
                'feature_map', corresponding value is the result of the layer before the fully connected layer.
"
2	"# limitations under the License.
# ==============================================================================

RESET_LIMIT_EXCEEDED_MESSAGE = 'Horovod was shutdown because number of resets during elastic job exceeded limit: {}'
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
2	"                add_host()
            driver.wait_for_available_slots(4)

            if slot_info.rank == 0:
                remove_host()
"
1	"            loss_att = None
            self.acc = None

"
2	"                                     '--max-np', str(max_np)]

                if reset_limit is not None:
"
4	"            # Add embedding layer.
            w_param_attrs = fluid.ParamAttr(
                name=""emb"",
                initializer=fluid.initializer.TruncatedNormal(scale=0.02),
"
4	"        std = self.module.get_pretrained_images_std()

"
4	"             num_data(int): It's number of data inputted to the model, selectted as following options:

                 - 1(default): There's only one data to be feeded in the model, e.g. the module is used for text classification task.
                 - 2: There are two data to be feeded in the model, e.g. the module is used for text matching task (point-wise).
                 - 3: There are three data to be feeded in the model, e.g. the module is used for text matching task (pair-wise).
"
4	"            text_1 = fluid.layers.data(
                name=""text_1"",
                shape=[-1, max_seq_len, 1],
                dtype=""int64"",
"
0	"

@contextmanager
def suppress_stdout_stderr():
    """"""A context manager that redirects stdout and stderr to devnull""""""
"
2	"            thread.join(0.1)
            self.assertFalse(thread.is_alive(), 'thread should have terminated by now')

    def test_shutdown_during_request_basic_task(self):
"
1	"            assert isinstance(iter_factory, AbsIterFactory), type(iter_factory)
            yield from iter_factory.build_iter(epoch, shuffle)

"
1	"        :return: ctc loass value
        :rtype: torch.Tensor
        :return: attention loss value
        :rtype: torch.Tensor
"
4	"
    def conv_bn_layer(self,
                      input,
"
2	"        fail = os.path.sep.join([dir, 'rank_{}_epoch_{}_batch_{}_fail'.format(rank, epoch, batch)])
        if os.path.exists(fail):
            logging.info('rank %s: failing epoch %s batch %s', rank, epoch, batch)
            os.unlink(fail)
            raise Exception('training failed, restart the task')
"
2	"                                'HOROVOD_RANK={rank} '
                                'HOROVOD_SIZE={size} '
                                'HOROVOD_LOCAL_RANK={local_rank} '
"
6	"            if isinstance(edge_index, Tensor):
                edge_index, _ = remove_self_loops(edge_index)
                edge_index, _ = add_self_loops(edge_index,
"
6	"@torch.jit._overload
def gcn_norm(edge_index, edge_weight=None, num_nodes=None, improved=False,
             add_self_loops=True, dtype=None):
    # type: (Tensor, OptTensor, Optional[int], bool, bool, Optional[int]) -> PairTensor  # noqa
    pass
"
2	"    @staticmethod
    def get_num_gpus():
"
4	"
from __future__ import absolute_import
"
4	"
    def _expand_conv_norm(self, inputs, block_args, is_test, name=None):
        # Expansion phase
        oup = block_args.input_filters * block_args.expand_ratio  # number of output channels

"
4	"loggerhead, loggerhead turtle, Caretta caretta
leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea
mud turtle
terrapin
box turtle, box tortoise
"
4	"                bias_attr=bias_attr)

    def _conv_stem_norm(self, inputs, is_test):
        out_channels = round_filters(32, self._global_params)
        bn = self.conv_bn_layer(
"
4	"        override_params=override_params,
        use_se=use_se)
    return model

"
2	"    Runs mpirun.

    :param settings: Settings for running MPI.
                     Note: settings.num_proc and settings.hosts must not be None.
    :param nics: Interfaces to include by MPI.
"
2	"    queue: 2x-gpu-g4
- label: ':tensorflow: Test Keras MNIST (test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
  command: horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/keras_mnist_advanced.py
"
4	"        initializer=fluid.initializer.UniformInitializer(
            low=-init_range, high=init_range))
"
4	"cauliflower
zucchini, courgette
"
2	"  command: horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/mxnet_mnist.py
  plugins:
  - docker-compose#v2.6.0:
      run: test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
"
4	"mantis, mantid
cicada, cicala
leafhopper
lacewing, lacewing fly
"
2	"            # the client that started the command needs some time to connect again
            # to wait for the result (see horovod.spark.driver.rsh).
            if self._minimum_command_lifetime is not None:
                time.sleep(self._minimum_command_lifetime.remaining())

"
4	"            is_test = False
        else:
"
6	"            x_l, x_r = x[0], x[1]
            assert x[0].dim() == 2, 'Static graphs not supported in `GATConv`.'
"
4	"            inputs,
            num_filters=out_channels,
            filter_size=3,
"
4	"ant, emmet, pismire
grasshopper, hopper
cricket
"
1	"            loss_att = sum(map(lambda x: x[0] * x[1], zip(loss_att, ys_out_len))) / sum(
                ys_out_len
"
6	"        self.node_emb = Embedding(21, 75)
        self.edge_emb = Embedding(4, 50)

        aggregators = ['mean', 'min', 'max', 'std']
"
1	"    exit 1;
fi

# Check model name or model file is set
"
4	"    img -= img_mean
    img /= img_std
"
4	"wig
window screen
window shade
Windsor tie
wine bottle
"
2	"
        driver = ElasticDriver(mock.Mock(), discovery, min_np=2, max_np=4)
        driver.wait_for_available_slots(min_np=2)

        rank_results = {}
"
4	"            self.gpu_predictor = create_paddle_predictor(gpu_config)

    def context(self,
                trainable=True,
                pretrained=True,
"
1	"        f.write(""a 100,80\n"")
        f.write(""b 150,80\n"")
    return str(p)

"
4	"            use_bias=True,
            padding_type=self.padding_type,
            name=name + '_se_expand')
        se_out = inputs * fluid.layers.sigmoid(x_squeezed)
"
0	"    FloatProperty,
    PointerProperty
)

"
4	"                # pretrained
                if pretrained:
"
2	"      run: test-cpu-oneccl-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
"
4	"        'r1_k3_s11_e1_i32_o16_se0.25',
        'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25',
        'r3_k3_s22_e6_i40_o80_se0.25',
"
4	"            filter_size=k,
            stride=s,
            num_groups=oup,
            bn_act=None,
            padding_type=self.padding_type,
"
4	"            if len(input.shape) > 2:
                fan_in = input.shape[1] * input.shape[2] * input.shape[3]
            else:
                fan_in = input.shape[1]
"
2	"                                            *nccl_op_context_.nccl_comm_, *gpu_op_context_.stream),
                              *nccl_op_context_.nccl_comm_);
"
4	"                                  not None) and (0 < block_args.se_ratio <= 1)
        id_skip = block_args.id_skip  # skip connection and drop connect
        conv = inputs
        if block_args.expand_ratio != 1:
            conv = fluid.layers.swish(
"
4	"                 padding_type='SAME',
                 override_params=None,
"
1	"        loss_data = float(self.loss)
        if loss_data < CTC_LOSS_THRESHOLD and not math.isnan(loss_data):
            self.reporter.report(
                loss_ctc_data, loss_att_data, self.acc, cer_ctc, cer, wer, loss_data
"
4	"

"
4	"swimming trunks, bathing trunks
swing
switch, electric switch, electrical switch
"
2	"            # only want to step on p
            if any([p is v for v in group['params']]):
                group['params'] = [p]
            else:
"
6	"            edge_index, _ = remove_self_loops(edge_index)
            data = Data(edge_index=edge_index, y=y, num_nodes=adj.shape[0])
            if self.pre_filter is not None and not self.pre_filter(data):
"
3	"
        with g:
            g.search(random_docs(10), output_fn=lambda x: validate(x, 'AnnoyIndexer'))
"
4	"        """"""context for transfer learning.

        Args:
"
6	"

"
4	"                 use_gpu=False,
                 top_k=1):
        """"""
        API for image classification.

"
2	"    queue: 2x-gpu-g4
- label: ':spark: Spark Torch MNIST (test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/pytorch_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""
  plugins:
  - docker-compose#v2.6.0:
"
4	"## 命令行预测

```
hub run efficientnetb7_imagenet --input_path ""/PATH/TO/IMAGE""
"
4	"European gallinule, Porphyrio porphyrio
American coot, marsh hen, mud hen, water hen, Fulica americana
bustard
ruddy turnstone, Arenaria interpres
"
2	"
# nodes with failed executors may have other executors that can still be used across the app
# NOTE: in dynamic allocation, only executors blacklisted for the entire app
#       can get reclaimed by the cluster manager
"
1	"        """"""
        # linear -> GLU -> lightconv -> linear
        x = query
        B, T, C = x.size()
"
6	"        pre_transform (callable, optional): A function/transform that takes in
            an :obj:`torch_geometric.data.Data` object and returns a
            transformed version. The data object will be transformed before
            being saved to disk. (default: :obj:`None`)
"
2	"    queue: 2x-gpu-g4
- label: ':spark: Spark Keras Rossmann Run (test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/keras_spark_rossmann_run.py --num-proc 2 --data-dir file:///data --epochs 3 --sample-rate 0.01""
  plugins:
"
1	"        ""Hello"",
        ""World!!"",
        ""Ummm"",
    ]

"
2	"if __name__ == '__main__':
    args = parser.parse_args()

"
2	"
    def test_rank_and_size(self):
        """"""Tests two hosts, two slots each in standard happy path.""""""
        slots = {'host-1': 2, 'host-2': 2}
"
1	"    ldconv_encoder_kernel_length=""5_7_11"",
    ldconv_decoder_kernel_length=""3_7"",
    ldconv_usebias=False,
)

"
2	"
    # Make sure Spark Job did not fail.
    driver.check_for_spark_job_failure()

"
4	"capuchin, ringtail, Cebus capucinus
howler monkey, howler
titi, titi monkey
"
2	"
import cloudpickle
import torch
"
3	"      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
"
4	"

def cal_padding(img_size, stride, filter_size, dilation=1):
    """"""Calculate padding size.""""""
    if img_size % stride == 0:
"
4	"cannon
canoe
"
2	"    :param network_interface: Network interfaces to use for communication separated by comma. If
                             not specified, Horovod will find the common NICs among all the
                             workers and use those; example, eth0,eth1.
"
1	"    elif args.mtlalpha == 0:
        assert model.ctc is None


"
1	"                for p, n, t in data_path_and_name_and_type
            ]
"
4	"        return filters
    divisor = global_params.depth_divisor
    min_depth = global_params.min_depth
    filters *= multiplier
    min_depth = min_depth or divisor
"
4	"    res = b0.classification(images=test_image)
    print(res)
"
1	"    Examples:
        >>> dataset = IterableESPnetDataset([('wav.scp', 'input', 'sound'),
"
2	"                          name_func=test_name_func)
    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
    def test_fault_tolerance_spark_blacklist(self, setting, _):
        """"""
        Same as test_fault_tolerance_no_spark_blacklist except Spark blacklists the executor
"
1	"        dropout_rate (float): dropout_rate
        kernel_size_str (str): kernel size (length)
        lnum (inst): index of layer
        use_kernel_mask (bool): Use causal mask or not for convolution kernel
"
1	"if [ ! $# -eq 2 ]; then
    echo ""${help_message}""
    exit 1;
fi

"
4	"#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
"
0	"            bl_idname = ""btools_test.dummy_op_fail""
            bl_label = ""Dummy Test Fail""
            bl_options = {""REGISTER"", ""UNDO""}

            def execute(self, context):
"
2	"    # prepend HOROVOD_SPARK_PYTHONPATH to PYTHONPATH
    if 'HOROVOD_SPARK_PYTHONPATH' in os.environ:
        ppath = os.environ['HOROVOD_SPARK_PYTHONPATH']
"
4	"        each (collections.OrderedDict): info of original image, preprocessed image.
    """"""
    component = list()
"
1	"
    steps/make_fbank_pitch.sh --cmd ""$train_cmd"" --nj 1 --write_utt2num_frames true \
        ${align_dir}/data ${align_dir}/log ${align_dir}/fbank || exit 1;
"
4	"bottlecap
bow
bow tie, bow-tie, bowtie
"
2	"    automatic: true
  agents:
"
3	"        if not ret:
            ret = [(text, 0, len(text))]
"
1	"        threshold: float = 0.5,
        minlenratio: float = 0.0,
        maxlenratio: float = 10.0,
"
4	"lakeside, lakeshore
promontory, headland, head, foreland
sandbar, sand bar
"
2	"        self.assertFalse(thread.is_alive())
        fn.assert_called_once_with(1)

        fn = mock.Mock()
"
4	"from paddlehub.module.module import moduleinfo, runnable, serving
from paddlehub.common.paddle_helper import add_vars_prefix
"
3	"        f = (Flow().add(name='encode1', yaml_path='yaml/test-if-encode1.yml')
             .add(name='encode2', yaml_path='yaml/test-if-encode2.yml', needs='gateway')
"
4	"            batch_image = np.array([data['image'] for data in batch_data])
            batch_image = PaddleTensor(batch_image.copy())
            predictor_output = self.gpu_predictor.run([
                batch_image
            ]) if use_gpu else self.cpu_predictor.run([batch_image])
"
2	"                                    '--erf_input /tmp/rankfile '
                                    '--stdio_stderr >output filename goes here< '
                                    '--stdio_stdout >output filename goes here< '
                                    '--smpiargs \'{mpi_args} >mpi-extra args go here<\' '
"
2	"                                     env=None, stdout=None, stderr=None, verbose=2,
                                     cores=2, expected_np=1):
        if env is None:
"
4	"        """"""
        cpu_config = AnalysisConfig(self.default_pretrained_model_path)
        cpu_config.disable_glog_info()
        cpu_config.disable_gpu()
        self.cpu_predictor = create_paddle_predictor(cpu_config)
"
2	"        session: Session for TensorFlow v1 compatibility.
        name: Optional name to use during broadcast, will default to the class
              type.
"
2	"            for var, saved_var in zip(self.optimizer.variables(), self._saved_optimizer_state):
                var.assign(saved_var)
"
2	"    def add_result(self, key, value):
        if key in self._worker_results:
            return
        self._worker_results[key] = value

"
2	"from horovod.spark.task import task_service
from horovod.spark.gloo_run import gloo_run, gloo_run_elastic
from horovod.spark.mpi_run import mpi_run
"
4	"            stride=[int(options['s'][0])])

    @staticmethod
"
4	"forklift
fountain
fountain pen
"
4	"                    name=""image"", shape=[3, 224, 224], dtype=""float32"")
                efficientnet_b2 = EfficientNetB2(
                    override_params=override_params)
                output, feature_map = efficientnet_b2.net(
                    input=image,
"
6	"        return node_idx, edge_idx, adj_t

"
2	"REGISTER_KERNEL_BUILDER(
    Name(""HorovodLocalSize"").Device(DEVICE_GPU).HostMemory(""local_size""),
    HorovodReturnScalarOp<int, common::horovod_local_size>);
"
4	"            act='swish',
            name=name + '_se_reduce')
"
4	"url = ""http://127.0.0.1:8866/predict/efficientnetb4_imagenet""
r = requests.post(url=url, headers=headers, data=json.dumps(data))

# 打印预测结果
"
2	"            total_core_count = (int(node_output[""Record_1""][""discovered_cores""]) -
                               int(node_output[""Record_1""][""discovered_sockets""]) * LSFUtils._csm_allocation_info[""isolated_cores""])
            LSFUtils._csm_allocation_info[""compute_node_cores""]= total_core_count
            LSFUtils._csm_allocation_info[""compute_node_gpus""] = int(node_output[""Record_1""][""discovered_gpus""])
"
4	"zebra
hog, pig, grunter, squealer, Sus scrofa
wild boar, boar, Sus scrofa
warthog
"
4	"            emb_1_name = emb_1.name
            data_list = [text_1]
            emb_name_list = [emb_1_name]

"
2	"    automatic: true
  agents:
    queue: cpu
- label: ':tensorflow: Test Keras MNIST (test-cpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
"
2	"    @mock.patch('horovod.run.util.lsf.LSFUtils.get_compute_hosts', MagicMock(return_value=['host1', 'host2']))
    @mock.patch('horovod.run.util.lsf.LSFUtils.get_num_gpus', MagicMock(return_value=2))
    @mock.patch('horovod.run.util.network.filter_local_addresses', MagicMock(return_value=['host1', 'host2']))
    @mock.patch('horovod.run.runner._check_all_hosts_ssh_successful', MagicMock())
    @mock.patch('horovod.run.runner.run_controller')
"
1	"        x = self.act(x)

        # convolution along frequency axis
"
6	"        num_blocks (int, optional): If set to not :obj:`None`, this layer will
            use the block-diagonal-decomposition regularization scheme where
"
4	"cockroach, roach
mantis, mantid
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
"
2	"            expected_obj = {
                'hello': 123,
                0: [1, 2]
            }
            obj = expected_obj if hvd.rank() == 0 else {}
"
1	"                multiple_iterator=False,
                iterator_type=iterator_type,
                batch_type=batch_type,
                batch_bins=batch_bins,
"
4	"## 命令行预测

```
hub run efficientnetb5_imagenet --input_path ""/PATH/TO/IMAGE""
"
4	"redshank, Tringa totanus
dowitcher
oystercatcher, oyster catcher
"
1	"            checkpoint,
            map_location=f""cuda:{torch.cuda.current_device()}"" if ngpu > 0 else ""cpu"",
        )
        model.load_state_dict(states[""model""])
        reporter.load_state_dict(states[""reporter""])
"
2	"        self.assertEqual(2, results[1]['size'])
        self.assertEqual(1, results[1]['rendezvous'])
        self.assertEqual(results[0]['hostname'], results[1]['hostname'])

        self.assertEqual(0, results[2]['start_rank'])
"
2	"    def test_spark_run_with_gloo(self):
        self.do_test_spark_run(use_mpi=False, use_gloo=True)

"
3	"        """"""
        return gzip.open(self.index_abspath, 'ab', compresslevel=self.compress_level)

"
2	"        self._should_synchronize = False
        try:
            yield
"
2	"
        # Adjust the host assignments to account for added / removed hosts
        host_assignments, host_assignments_list = self._get_host_assignments(current_hosts)

        if len(self._host_assignments) > 0:
"
4	"import cv2
import numpy as np
"
4	"book jacket, dust cover, dust jacket, dust wrapper
menu
plate
"
4	"def get_pretrained_images_mean()
```

"
4	"ballplayer, baseball player
groom, bridegroom
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
"
2	"    Set SPARK_DRIVER_MEM=512m env when run forked to reduce driver memory usage.
    """"""
"
6	"
        if args.use_normalization:
"
2	"                   ]))
# Horovod: use DistributedSampler to partition the training data.
"
4	"    n = fout  # fan-out
    init_range = 1.0 / math.sqrt(n)

"
4	"        self.arg_config_group = self.parser.add_argument_group(
            title=""Config options"",
            description=
"
3	"    margin-left: -2px;
    margin-top: 197px;
    -webkit-transition: all .4s ease-in-out;
"
4	"crane
limpkin, Aramus pictus
"
4	"result = classifier.classify(images=[cv2.imread('/PATH/TO/IMAGE')])
# or
# result = classifier.classify(paths=['/PATH/TO/IMAGE'])
```

"
1	"import collections.abc
from pathlib import Path
from typing import Union

import numpy as np
"
4	"earthstar
hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa
bolete
ear, spike, capitulum
toilet tissue, toilet paper, bathroom tissue
"
1	"        :rtype: list
        """"""

        def interpolate_blank(label, blank_id=0):
            """"""Insert blank token between every two label token.""""""
"
4	"    def serving_method(self, images, **kwargs):
        """"""
"
2	"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"
2	"pyspark_require_list = ['pyspark>=2.3.2;python_version<""3.8""',
                        # TODO: change to 'pyspark>=3.0.0' once spark3 is released
                        'pyspark>=3.0.0.dev;python_version>=""3.8""']
spark_require_list = ['h5py>=2.9', 'numpy', 'petastorm>=0.9.0', 'pyarrow>=0.15.0'] + \
                     pyspark_require_list
"
4	"            self.assertTrue(res.keys(), results_2[index].keys())
            diff = list(res.values())[0] - list(results_2[index].values())[0]
            self.assertTrue((diff < 1e-5))

        test_images = [cv2.imread(img) for img in self.test_images]
"
2	"    # start Spark driver service and launch settings.num_proc Spark tasks
    spark_job_group = 'horovod.spark.run.%d' % job_id.next_job_id()
    driver = driver_service.SparkDriverService(settings.num_proc, settings.num_proc,
                                               fn, args, kwargs,
"
6	"    t = '(Tensor, SparseTensor, OptTensor, Size) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
"
4	"tiger beetle
ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle
ground beetle, carabid beetle
long-horned beetle, longicorn, longicorn beetle
leaf beetle, chrysomelid
"
1	"            data = {}
            # 2.a. Load data streamingly
            for value, (path, name, _type) in zip(values, self.path_name_type_list):
                func = DATA_TYPES[_type]
"
4	"            args.append('se%s' % block.se_ratio)
        if block.id_skip is False:
"
4	"bookshop, bookstore, bookstall
bottlecap
"
2	"        self.assertEqual(list(range(0, num_proc)), actual_ranks)

"
2	"    while state.epoch < args.epochs:
        print('epoch {} batch {}'.format(state.epoch, state.batch))

        while state.batch < args.batches_per_epoch:
"
4	"chiffonier, commode
chime, bell, gong
"
3	"      name='key', full_name='jina.Document.TagsEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
"
4	"knot
lab coat, laboratory coat
ladle
lampshade, lamp shade
laptop, laptop computer
"
4	"shovel
shower cap
shower curtain
ski
"
2	"        raise RuntimeError('failing as expected')
    return rank


class StaticRunTests(unittest.TestCase):
"
2	"                with open(logfile, 'r') as f:
                    lines = f.readlines()

"
1	"        self.data[key] = str(wav)

    def get_path(self, key):
"
4	"        'efficientnet-b2': (1.1, 1.2, 260, 0.3),
        'efficientnet-b3': (1.2, 1.4, 300, 0.3),
        'efficientnet-b4': (1.4, 1.8, 380, 0.4),
"
4	"loupe, jeweler's loupe
lumbermill, sawmill
magnetic compass
mailbag, postbag
mailbox, letter box
"
2	"      ""horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow_mnist.py""

    run_test ""${test}"" ""${queue}"" \
      "":tensorflow: Test Keras MNIST (${test})"" \
"
2	"        remaining_slots -= needed_slots
        if remaining_slots == 0:
"
2	"      run: test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2
      config: docker-compose.test.yml
      pull-retries: 3
"
2	"        elif run == 'cmd':
            command = 'false'
            run_func = None
"
1	"        for optimizer, state in zip(optimizers, states[""optimizers""]):
            optimizer.load_state_dict(state)
        for scheduler, state in zip(schedulers, states[""schedulers""]):
            if scheduler is not None:
"
4	"cornet, horn, trumpet, trump
cowboy boot
cowboy hat, ten-gallon hat
cradle
crane
"
4	"dial telephone, dial phone
diaper, nappy, napkin
digital clock
digital watch
dining table, board
"
2	"        exit_schedule = {
            str((1, 0)): [0],
"
1	"        aheads (int, optional): Number of heads for multi head attention.
        dlayers (int, optional): Number of decoder layers.
        dunits (int, optional): Number of decoder hidden units.
        postnet_layers (int, optional): Number of postnet layers.
        postnet_chans (int, optional): Number of postnet channels.
"
4	"def init_batch_norm_layer(name=""batch_norm""):
    param_attr = fluid.ParamAttr(
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
6	"@torch.jit._overload
def gcn_norm(edge_index, edge_weight=None, num_nodes=None, improved=False,
             add_self_loops=True, dtype=None):
    # type: (SparseTensor, OptTensor, Optional[int], bool, bool, Optional[int]) -> SparseTensor  # noqa
"
2	"  - ecr#v1.2.0:
      login: true
"
1	"            linear_units=dunits,
            num_blocks=dlayers,
            dropout_rate=transformer_dec_dropout_rate,
"
4	"echidna, spiny anteater, anteater
platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus
wallaby, brush kangaroo
koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus
"
1	"    def keys(self):
        return self.data.keys()

"
4	"stage
steam locomotive
"
0	"    # If we just have one (non __init__) file then return just that file.
    return paths, filepaths or [filepath]
"
1	"                            f""Keys are mismatched. Text files is not sorted or ""
                            f""not having same keys at L{linenum}""
                        )
"
3	"    $("".choose"").removeClass(""active"");
    $("".pay > .icon"").removeClass(""active"");
    $("".wrap > .icon"").removeClass(""active"");
    $("".choose > .icon"").removeClass(""active"");
    $(""#line"").addClass(""four"");
"
1	"    parser.add_argument(""--seed"", type=int, default=1, help=""Random seed"")
    parser.add_argument(""--verbose"", ""-V"", type=int, default=1, help=""Verbose option"")
    parser.add_argument(
        ""--batchsize"",
        type=int,
"
1	"        )


"
1	"    # check CUDA_VISIBLE_DEVICES
    if args.ngpu > 0:
"
4	"        random_tensor = keep_prob + fluid.layers.uniform_random_batch_size_like(
            inputs, [-1, 1, 1, 1], min=0., max=1.)
        binary_tensor = fluid.layers.floor(random_tensor)
        output = inputs / keep_prob * binary_tensor
"
2	"    ssh_port_arg = '-p {ssh_port}'.format(ssh_port=settings.ssh_port) if settings.ssh_port else ''

    def _exec_command(command, slot_info, events):
        index = slot_info.rank
"
2	"        self.worker_results = worker_results


"
2	"        def broadcast_group(variables, root_rank):
            for var in variables:
                var.assign(broadcast(var, root_rank))
"
4	"    elif act == 'sigmoid':
        conv = fluid.layers.sigmoid(conv, name=name + '_sigmoid')
    elif act == 'swish':
        conv = fluid.layers.swish(conv, name=name + '_swish')
"
2	"
        def broadcast_object_with_session(obj):
            return bcast_obj(obj)

        super(TensorFlowState, self).__init__(bcast_object=broadcast_object_with_session,
"
2	"  command: bash -c "" cd /horovod/test && (echo test_*.py | sed 's/[a-z_]*tensorflow2[a-z_.]*//g' | sed 's/test_interactiverun.py//g' | sed 's/test_spark_keras.py//g' | sed 's/test_spark_torch.py//g' | sed 's/test_spark.py//g' | sed 's/test_run.py//g' | xargs -n 1 \$(cat /mpirun_command) pytest -v --capture=no) && pytest --forked -v --capture=fd test_spark.py test_run.py""
  plugins:
  - docker-compose#v2.6.0:
"
4	"cougar, puma, catamount, mountain lion, painter, panther, Felis concolor
lynx, catamount
leopard, Panthera pardus
snow leopard, ounce, Panthera uncia
"
1	"    return str(p)


@pytest.mark.skipif(
"
4	"        """"""
        self.parser = argparse.ArgumentParser(
"
6	"    assert jit((x1, x2), edge_index, edge_type).tolist() == out1.tolist()
    if num_blocks is None:
        assert torch.allclose(jit((None, idx2), edge_index, edge_type), out2)
        assert torch.allclose(jit((idx1, idx2), edge_index, edge_type), out2)

"
6	"    adj = adj.sparse_resize((4, 2))
    assert torch.allclose(conv1((x, x[:2]), edge_index, edge_index), out1[:2],
                          atol=1e-6)
"
0	"        if self.type == ""CIRCULAR"":
            col.prop(self, ""resolution"", text=""Resolution"")
"
4	"                initializer=fluid.initializer.Uniform(low=-bound, high=bound))
        else:
            bias_attr = False
    elif init == 'google':
        n = filter_size * filter_size * fan_out
"
2	"import json
import os
import sys

"
2	"            # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during
            # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.
            hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, initial_lr=scaled_lr, verbose=verbose),

            # Reduce LR if the metric is not improved for 10 epochs, and stop training
"
4	"coucal
bee eater
hornbill
"
4	"    def run_cmd(self, argvs):
        """"""
        Run as a command.
"
4	"spider web, spider's web
spindle
"
1	"        delimiter = "" ""
        dtype = float
    elif loader_type == ""csv_int"":
        delimiter = "",""
        dtype = int
"
4	"            add_vars_prefix(
                program=main_program, prefix=prefix_name, vars=variable_names)
"
6	"    out11 = conv(x1, edge_index)
    assert out11.size() == (4, 32)
    assert conv(x1, edge_index, size=(4, 4)).tolist() == out11.tolist()
"
2	"
def broadcast_variables(variables, root_rank):
    """"""Broadcasts variables from root rank to all other processes.

    Arguments:
"
0	"        dummy.normal = Y
        gb = btools.utils.local_to_global(dummy, X)
        self.assertEqual(gb.to_tuple(1), Vector((-1, 0, 0)).to_tuple(1))

        dummy.normal = X
"
4	"            name=name + '_se_reduce')
        x_squeezed = conv2d(
            x_squeezed,
            num_filters=oup,
            filter_size=1,
"
2	"        def _exec_command(command, alloc_info, event):
            return 1, alloc_info.rank

        exec_command = mock.MagicMock(side_effect=_exec_command)
        gloo_exec_command_fn = mock.MagicMock(return_value=exec_command)
"
1	"                # Load entry
                array = func(value)
                data[name] = array
            if self.non_iterable_dataset is not None:
"
1	"                self.num_layers_applied_guided_attn = elayers
            else:
                self.num_layers_applied_guided_attn = num_layers_applied_guided_attn
            if num_heads_applied_guided_attn == -1:
                self.num_heads_applied_guided_attn = aheads
"
4	"water snake
vine snake
night snake, Hypsiglena torquata
"
1	"    frontend = DefaultFrontend(
        fs=160, n_fft=128, win_length=32, hop_length=32, frontend_conf=None
    )
"
6	"        x_r = x[1]
        if x_r is not None and self.root is not None:
            out += torch.matmul(x_r, self.root)
"
2	"opt = tf.optimizers.SGD(lr * hvd.size())

"
1	"        if args.mtlalpha > 0.0:
            self.ctc = CTC(
"
1	"        ] = None,
        float_dtype: str = ""float32"",
        int_dtype: str = ""long"",
        key_file: str = None,
"
4	"isopod
white stork, Ciconia ciconia
black stork, Ciconia nigra
spoonbill
flamingo
"
6	"    pred = model(data.edge_index, data.edge_type).argmax(dim=-1)
    train_acc = pred[data.train_idx].eq(data.train_y).to(torch.float).mean()
    test_acc = pred[data.test_idx].eq(data.test_y).to(torch.float).mean()
    return train_acc.item(), test_acc.item()
"
2	"        self._synchronized = False
        self._should_synchronize = True
        if size() > 1 or os.environ.get('HOROVOD_ELASTIC') == '1':
"
4	"
    def conv_bn_layer(self,
                      input,
                      filter_size,
                      num_filters,
"
3	"        self.assertEqual(idx.shape, dist.shape)
        self.assertEqual(idx.shape, (10, 4))
"
1	"            cls.resume(
                checkpoint=output_dir / ""checkpoint.pth"",
"
4	"            paths (list[str]): The paths of images.
            batch_size (int): batch size.
            use_gpu (bool): Whether to use gpu.
"
3	"
def _decode(enc, key=__binary_delimiter__.decode()):
    dec = []
    enc = base64.urlsafe_b64decode(enc).decode()
"
4	"
        if not is_test and self._global_params.dropout_rate:
            pool = fluid.layers.dropout(
                pool,
"
2	"
        self.assertEqual(1, results[1]['start_rank'])
        self.assertEqual(2, results[1]['size'])
        self.assertEqual(2, results[1]['rendezvous'])

"
2	"# limitations under the License.
# ==============================================================================
"
2	"            self.assertEqual(num_proc, alloc_info.size)
            self.assertEqual(num_proc, alloc_info.local_size)
"
4	"    }
    return params_dict[model_name]
"
4	"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
"
4	"                 use_bias=False,
                 filter_size=0,
"
2	"  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
"
2	"# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
"
4	"import base64


def cv2_to_base64(image):
"
5	"        s.waste.CARD_XOFFSET = layout.XOFFSET
        layout.createText(s.waste, ""n"")
"
1	"        This function takes query, key and value but uses only query.
        This is just for compatibility with self-attention layer (attention.py)

        Args:
"
3	"        except (ImportError, ModuleNotFoundError):
            self.logger.critical('requires ""docker"" dependency, please install it via ""pip install jina[docker]""')
            raise

    def push(self, name: str = None, readme_path: str = None):
"
2	"    def test_mpi_exec_fn_provides_driver_with_local_rank(self):
        self.do_test_exec_fn_provides_driver_with_local_rank(
            mpirun_exec_fn, 'OMPI_COMM_WORLD_RANK', 'OMPI_COMM_WORLD_LOCAL_RANK'
"
4	"mantis, mantid
cicada, cicala
"
2	"    .Doc(R""doc(
Returns the number of Horovod processes.

Output
"
4	"        conv = conv2d(
            input=input,
            num_filters=num_filters,
            filter_size=filter_size,
"
2	"    queue: 2x-gpu-g4
- label: ':muscle: Test MXNet MNIST (test-gpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0)'
  command: bash -c "" OMP_NUM_THREADS=1 \$(cat /mpirun_command) python /horovod/examples/mxnet_mnist.py""
  plugins:
  - docker-compose#v2.6.0:
"
4	"great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias
tiger shark, Galeocerdo cuvieri
hammerhead, hammerhead shark
"
4	"        )
    ]
    res = b4.classification(images=test_image)
    print(res)
"
4	"car wheel
cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM
cassette
cassette player
"
4	"            except:
                raise RuntimeError(
                    ""Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES as cuda_device_id.""
"
4	"        raise NotImplementedError(
            'model name is not pre-defined: %s' % model_name)
    if override_params:
"
4	"    multiplier = global_params.depth_coefficient
    if not multiplier:
        return repeats
    return int(math.ceil(multiplier * repeats))

"
1	"            Whether to use batch normalization in encoder prenet.
        encoder_normalize_before (bool, optional):
            Whether to perform layer normalization before encoder block.
        decoder_normalize_before (bool, optional):
            Whether to perform layer normalization before decoder block.
"
3	"
    def test_calling_train_sets_is_trained(self):
        data = np.random.rand(1, 2)
        i = DummyIndexerTrain(index_filename='test2.bin')
"
2	"        # start new workers
        threads = []
        for host in hosts:
"
4	"washbasin, handbasin, washbowl, lavabo, wash-hand basin
washer, automatic washer, washing machine
water bottle
water jug
water tower
"
2	"
class SetLocalRankToRankResponse(object):
    def __init__(self, index):
        self.index = index
        """"""Index for rank given in request.""""""
"
2	"            if slot_info.hostname == 'host-1':
                if slot_info.rank == 0:
                    add_host()
                driver.wait_for_available_slots(4)
"
4	"
paddlehub >= 1.6.0
"
2	"            optimizer = tf.keras.optimizers.Adam(0.001 * hvd.size())

            state = hvd.elastic.KerasState(model1, optimizer, batch=20 + hvd.rank(), epoch=10 + hvd.rank())
"
4	"
        tmp = fluid.layers.elementwise_mul(x=(input - mean), y=scale, axis=1)
        tmp = tmp / fluid.layers.sqrt(var + epsilon)
"
4	"bearskin, busby, shako
beer bottle
beer glass
"
4	"        self._bn_eps = self._global_params.batch_norm_epsilon
        self.padding_type = padding_type
        self.use_se = use_se

"
6	"        return osp.join(self.root, self.name, 'processed')

    @property
    def raw_file_names(self):
"
1	"
        files = [open(lis[0], encoding=""utf-8"") for lis in self.path_name_type_list]

"
3	"

"
1	"            src_attention_dropout_rate=transformer_enc_dec_attn_dropout_rate,
            input_layer=decoder_input_layer,
            use_output_layer=False,
"
4	"warplane, military plane
washbasin, handbasin, washbowl, lavabo, wash-hand basin
"
2	"            model1.set_weights(
                [np.array([[v,  v], [v, v]], dtype=np.float32),
                 np.array([v, v], dtype=np.float32)])

            model2 = tf.keras.Sequential([
"
4	"        name='b0',
        is_test=is_test,
"
4	"        with open(label_file, 'r', encoding='utf-8') as file:
            self.label_list = file.read().split(""\n"")[:-1]
        self.classification = self.classify
        self._set_config()
"
1	"
# general configuration
"
2	"- label: ':spark: Spark Torch MNIST (test-cpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0)'
  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/pytorch_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""
  plugins:
"
4	"        height = self.module.get_expected_image_height()
        mean = self.module.get_pretrained_images_mean()
"
4	"church, church building
cinema, movie theater, movie theatre, movie house, picture palace
cleaver, meat cleaver, chopper
cliff dwelling
cloak
"
2	"def host_hash(salt=None):
    """"""
    Computes this host's host hash by invoking horovod.run.common.util.host_hash.host_hash.

    Consider environment variable CONTAINER_ID which is present when running Spark via YARN.
"
4	"    param_attr = fluid.ParamAttr(
        name=name + '_weights',
        initializer=fluid.initializer.UniformInitializer(
"
6	"idx = torch.tensor([0, 1, 2, 3, 4, 5, 6, 12, 13, 14, 15, 11])
dataset.data.y = dataset.data.y[:, idx]
"
4	"def get_model_params(model_name, override_params):
    """""" Get the block args and global params for a given model """"""
    if model_name.startswith('efficientnet'):
        w, d, _, p = efficientnet_params(model_name)
        blocks_args, global_params = efficientnet(
"
4	"tape player
teapot
teddy, teddy bear
"
2	"    def add_elapsed(df, cols):
        def add_elapsed_column(col, asc):
"
4	"damselfly
admiral
ringlet, ringlet butterfly
monarch, monarch butterfly, milkweed butterfly, Danaus plexippus
cabbage butterfly
"
2	"            self._handle_worker_exit(slot_info, exit_code, timestamp)

        thread = threading.Thread(target=run_worker)
"
2	"                    '{host} {cmd}'.format(
            host=LSFUtils.get_compute_hosts()[0],
"
6	"                edge_index, _ = remove_self_loops(edge_index)
                edge_index, _ = add_self_loops(edge_index,
                                               num_nodes=x.size(self.node_dim))
            elif isinstance(edge_index, SparseTensor):
                edge_index = set_diag(edge_index)
"
6	"                    'inside the `MessagePassing` module.')
            prop_types = split_types_repr(match.group(1))
            prop_types = dict([re.split(r'\s*:\s*', t) for t in prop_types])

"
2	"# limitations under the License.
# ==============================================================================

import mock
import os
"
3	"    def convert(self, d):
        if d.mime_type.startswith('text/'):
            super().convert(d)
"
4	"            input=inputs, pool_type='avg', global_pooling=True, use_cudnn=False)
        x_squeezed = conv2d(
            x_squeezed,
            num_filters=num_squeezed_channels,
"
1	"            ""t"",
            ""a"",
        ]
"
2	"      run: test-gpu-openmpi-py3_6-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_6_0-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
4	"electric locomotive
entertainment center
envelope
"
2	"# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
"
4	"        x = np.exp(x)
        tmp = np.sum(x)
        x /= tmp
    return x

"
2	"        else:
            sess = ops.get_default_session()
            if sess is None:
                with self.test_session(config=config) as sess:
"
2	"
            # Horovod: average metrics among workers at the end of every epoch.
"
3	"    display: flex;
    -webkit-box-align: center;
    align-items: center;
    -webkit-box-pack: center;
    justify-content: center;
"
4	"comic book
crossword puzzle, crossword
street sign
traffic light, traffic signal, stoplight
book jacket, dust cover, dust jacket, dust wrapper
"
4	"common iguana, iguana, Iguana iguana
American chameleon, anole, Anolis carolinensis
"
4	"bull mastiff
Tibetan mastiff
French bulldog
Great Dane
Saint Bernard, St Bernard
"
6	"        if isinstance(x, tuple):
            x_r = x[1]
"
4	"        is_test=is_test,
        padding_type=padding_type,
"
2	"      delete[] entry_component_sizes[ec];
      delete[] entry_component_offsets[ec];
    }   
    delete[] entry_component_sizes;
    delete[] entry_component_offsets;
"
2	"- label: ':factory: Elastic Spark TensorFlow Tests (test-cpu-gloo-py3_7-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""cd /horovod/test/integration && SPARK_HOME=/spark SPARK_DRIVER_MEM=512m HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format '[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s' --capture=no test_elastic_spark_tensorflow2.py""
  plugins:
  - docker-compose#v2.6.0:
"
4	"keeshond
Brabancon griffon
Pembroke, Pembroke Welsh corgi
Cardigan, Cardigan Welsh corgi
toy poodle
"
4	"        self._bn_mom = self._global_params.batch_norm_momentum
        self._bn_eps = self._global_params.batch_norm_epsilon
        self.padding_type = padding_type
        self.use_se = use_se
"
3	"        id.save_config('tmp.yml')
        id = BaseExecutor.load_config('tmp.yml')
        self.assertEqual(id._drivers['IndexRequest'][0].if_expression, '2 > 1')
        self.add_tmpfile('tmp.yml')

"
1	"
    params.extend([""pypinyin_g2p"", ""pypinyin_g2p_phone""])
    del pypinyin
"
4	"    need_crop = False
    if padding_type == ""SAME"":
"
4	"
        block_args_copy = copy.deepcopy(self._blocks_args)
        idx = 0
        block_size = 0
        for block_arg in block_args_copy:
"
4	"                      is_test=False,
                      drop_connect_rate=None,
"
3	"    width: 5px;
    height: 54px;
    background-color: #009999;
"
4	"bagel, beigel
pretzel
cheeseburger
hotdog, hot dog, red hot
"
2	"      run: test-gpu-openmpi-py3_6-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_6_0-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
"
3	"bsl-1.0: Boost Software License 1.0
bsd-2-clause: BSD 2-clause ""Simplified"" license
"
4	"                   override_params=None,
                   use_se=True):
    model = EfficientNet(
        name='b1',
        is_test=is_test,
"
2	"        discovery = FixedHosts(slots)

        def add_host():
            slots = {'host-1': 2, 'host-2': 2}
            discovery.set(slots)
"
1	"    layer = LogSpectrogram(n_fft=16, hop_length=4)
    x = torch.randn(1, 100)
"
2	"  command: horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow_mnist.py
  plugins:
  - docker-compose#v2.6.0:
"
1	"    dataset = IterableESPnetDataset(
        path_name_type_list=[(h5file_1, ""data4"", ""hdf5"")], preprocess=preprocess,
    )

"
2	"                if p.requires_grad:
                    p.grad = p.data.new(p.size()).zero_()
                    self._requires_update.add(p)
                    p_tmp = p.expand_as(p)
                    grad_acc = p_tmp.grad_fn.next_functions[0][0]
"
4	"wolf spider, hunting spider
tick
centipede
black grouse
"
4	"
            if num_data > 2:
                text_3 = fluid.data(
                    name='text_3',
"
6	"    assert jit(x, edge_index, edge_weight,
               lambda_max=torch.tensor(3.0)).tolist() == out3.tolist()

"
4	"kite
bald eagle, American eagle, Haliaeetus leucocephalus
vulture
"
1	"        ctc_weight = recog_args.ctc_weight

"
2	"        # Job should succeed with reset_limit=2
        results = self._run(discovery_schedule, np=2, min_np=2, max_np=4, reset_limit=2)
        assert len(results) == 3
"
3	"afl-3.0: Academic Free License v3.0
apache-2.0: Apache license 2.0
artistic-2.0: Artistic license 2.0
"
6	"        for _ in range(4):
            conv = PNAConv(in_channels=75, out_channels=75,
                           aggregators=aggregators, scalers=scalers, deg=deg,
                           edge_dim=50, towers=5, pre_layers=1, post_layers=1,
"
4	"armadillo
three-toed sloth, ai, Bradypus tridactylus
"
2	"    def _get_types(x):
        if isinstance(x, Iterable):
"
4	"                text_1 = fluid.data(
                    name='text_1',
                    shape=[-1, max_seq_len],
                    dtype='int64',
                    lod_level=0)
"
1	"        :param Namespace args: argument Namespace containing options
        """"""
        super(E2E, self).__init__(idim, odim, args, ignore_id=-1)
        if args.transformer_attn_dropout_rate is None:
            args.transformer_attn_dropout_rate = args.dropout_rate
"
4	"__all__ = [
    'EfficientNet', 'EfficientNetB0_small', 'EfficientNetB0', 'EfficientNetB1',
    'EfficientNetB2', 'EfficientNetB3', 'EfficientNetB4', 'EfficientNetB5',
    'EfficientNetB6', 'EfficientNetB7'
]
"
6	"                # Treat 0-dimensional tensors as 1-dimensional.
                if isinstance(item, Tensor) and item.dim() == 0:
                    item = item.unsqueeze(0)


"
2	"  - docker-compose#v2.6.0:
      run: test-cpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0
"
2	"    queue: 2x-gpu-g4
- label: ':factory: Elastic Spark Torch Tests (test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c ""cd /horovod/test/integration && SPARK_HOME=/spark SPARK_DRIVER_MEM=512m HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format '[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s' --capture=no test_elastic_spark_torch.py""
"
2	"    def __init__(self, state):
        """"""
        Constructs a new UpdateBatchStateCallback.

"
1	"
    p = tmp_path / ""dummy.scp""
    with p.open(""w"") as f:
"
2	"        assert exit_code_sum == 3

    def test_host_shutdown_on_worker_failure(self):
"
4	"computer keyboard, keypad
confectionery, confectionary, candy store
container ship, containership, container vessel
"
6	"    t = '(Tensor, SparseTensor, OptTensor) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert torch.allclose(conv(x, adj1.t()), out1, atol=1e-6)
    assert torch.allclose(conv(x, adj2.t()), out2, atol=1e-6)
"
2	"    # allocation is done.
    for alloc_item in alloc_list:
        alloc_item.local_size = local_sizes[alloc_item.cross_rank]
        alloc_item.cross_size = cross_sizes[alloc_item.local_rank]
"
2	"    @mock.patch('horovod.run.gloo_run._get_min_start_hosts', return_value=1)
    def test_min_hosts_timeout(self, mock_get_min_start_hosts):
        self.skipTest('This test fails due to https://github.com/horovod/horovod/issues/2030')

"
1	"        ys_mask = target_mask(ys_in_pad, self.ignore_id)
        pred_pad, pred_mask = self.decoder(ys_in_pad, ys_mask, hs_pad, hs_mask)

        # compute attention loss
        loss_att = self.criterion(pred_pad, ys_out_pad)
"
1	"        :param Namespace recog_args: argment Namespace contraining options
        :param list char_list: list of characters
"
4	"binoculars, field glasses, opera glasses
birdhouse
boathouse
bobsled, bobsleigh, bob
"
4	"
    def _conv_stem_norm(self, inputs, is_test):
        out_channels = round_filters(32, self._global_params)
        bn = self.conv_bn_layer(
            inputs,
"
4	"red-backed sandpiper, dunlin, Erolia alpina
redshank, Tringa totanus
dowitcher
oystercatcher, oyster catcher
pelican
"
4	"fly
bee
"
2	"        if self._hosts:
            self.provide_hosts(self._hosts)

    def start_master(self):
"
2	"# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
"
2	"    hvd.init()

"
4	"chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour
chain saw, chainsaw
chest
"
2	"
    def do_test_exec_fn_provides_driver_with_local_rank(self, exec_fn, rank_env, local_rank_env):
        with mock.patch(""horovod.spark.task.task_service.SparkTaskService._get_resources"", return_value={}):
            with spark_driver_service(num_proc=3) as (driver, client, key), \
"
2	"                                         MINIMUM_COMMAND_LIFETIME_S if is_elastic or use_gloo else None,
                                         settings.verbose)
    try:
        driver_client = driver_service.SparkDriverClient(driver_addresses, settings.key, settings.verbose)
        driver_client.register_task(index, task.addresses(), hosthash)
"
6	"    def message(self, x_i, x_j, edge_attr: OptTensor) -> Tensor:
        if edge_attr is None:
"
4	"
def base64_to_cv2(b64str):
    data = base64.b64decode(b64str.encode('utf8'))
    data = np.fromstring(data, np.uint8)
"
2	"        logging.info('starting Spark master %s', self._master_instance)
        res = self._execute('env SPARK_MASTER_OPTS=-Dspark.deploy.maxExecutorRetries=-1 '
                            '{spark_home}/sbin/spark-daemon.sh '
                            'start org.apache.spark.deploy.master.Master {instance} '
"
4	"
DATA_DIM = 224
img_mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))
img_std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))

"
4	"            executor=exe,
            feeded_var_names=feeded_var_names,
"
4	"def get_expected_image_height()
```
"
4	"
import cv2
import numpy as np
import paddlehub as hub
"
4	"        # Skip connection and drop connect
        input_filters, output_filters = block_args.input_filters, block_args.output_filters
"
2	"

state = hvd.elastic.TensorFlowKerasState(model, opt, img_secs=[], iter=0, batch=0, warm=False)
state.register_reset_callbacks([on_state_reset])
run_benchmark(state)
"
1	"            self.attn_criterion = GuidedMultiHeadAttentionLoss(
                sigma=guided_attn_loss_sigma, alpha=guided_attn_loss_lambda,
            )

"
4	"        is_test=is_test,
        padding_type=padding_type,
"
1	"@pytest.mark.skipif(
    LooseVersion(torch.__version__) < LooseVersion(""1.2""), reason=""require pytorch>=1.2""
"
4	"tape player
teapot
teddy, teddy bear
television, television system
"
4	"    'num_classes',
    'width_coefficient',
    'depth_coefficient',
    'depth_divisor',
    'min_depth',
"
4	"        width_padding = 0
        padding = [height_padding, width_padding]
    elif padding_type == ""DYNAMIC"":
        padding = get_padding(filter_size, stride)
"
4	"            inputs,
            num_filters=out_channels,
"
1	"    # ngram related
    parser.add_argument(
"
1	"        ""--model-conf"", type=str, default=None, help=""Model config file""
    )
    parser.add_argument(
"
4	"cleaver, meat cleaver, chopper
cliff dwelling
cloak
clog, geta, patten, sabot
cocktail shaker
"
2	"    # self.httpd.init needs to be called after server start
    def start(self, handler_cls=RendezvousHandler):
        self._httpd, port = find_port(
"
1	"            Dropout rate in decoder except attention & positional encoding.
        transformer_dec_positional_dropout_rate (float, optional):
            Dropout rate after decoder positional encoding.
"
1	"            decoder lstm outputs.
        reduction_factor (int, optional): Reduction factor.
        spk_embed_dim (int, optional): Number of speaker embedding dimenstions.
        spk_embed_integration_type (str, optional): How to integrate speaker embedding.
"
4	"seat belt, seatbelt
sewing machine
shield, buckler
shoe shop, shoe-shop, shoe store
shoji
"
1	"        if self.use_bias:
            x = x + self.bias.view(1, -1, 1)
        x = x.transpose(1, 2)  # B x T x C
"
4	"

"
1	"class SoundScpReader(collections.abc.Mapping):
    """"""Reader class for 'wav.scp'.

    Examples:
        key1 /some/path/a.wav
"
4	"            trainable (bool): Set parameters in program to be trainable.
            pretrained (bool) : Whether to load pretrained model.
"
4	"Arabian camel, dromedary, Camelus dromedarius
llama
weasel
"
2	"  retry:
    automatic: true
"
2	"    parser.add_argument('--x_max', type=float, default=1., dest='x_max')
    args = parser.parse_args()

    train(args)
"
2	"optimizer = tf.optimizers.SGD(lr * hvd.size())

hostname = os.environ.get('HOROVOD_HOSTNAME')
start_rank = int(os.environ.get('HOROVOD_RANK', 0))
"
1	"    ):
        """"""Construct Lightweight 2-Dimentional Convolution layer.""""""
"
3	"  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
"
1	"        report_cer=False,
        report_wer=False,
        sortagrad=0,
"
1	"        l1_loss, l2_loss, bce_loss = self.criterion(
            after_outs, before_outs, logits, ys, labels, olens
"
2	"                func(*args)

    return in_thread(fn, daemon=daemon, silent=silent)
"
4	"
    param_attr = fluid.ParamAttr(
        name=name + '_weights',
        initializer=fluid.initializer.UniformInitializer(
            low=-init_range, high=init_range))
"
4	"hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa
bolete
ear, spike, capitulum
toilet tissue, toilet paper, bathroom tissue

"
0	"    resolution, height, function = arch_prop.resolution, arch_prop.height, arch_prop.function
    upper_arc = filter_geom(
        arc_edge(bm, arc_edges[0], resolution, height, xyz, function)[""geom_split""], BMEdge)
"
4	"配置好服务端，以下数行代码即可实现发送预测请求，获取预测结果

```python
import requests
"
4	"                }
                outputs = {
                    key: global_vars[value]
                    for key, value in outputs.items()
                }
"
4	"    @staticmethod
    def _encode_block_string(block):
        """"""Encodes a block to a string.""""""
"
2	"
    def record_success(self, host, slot):
        return self._record_state(host, slot, SUCCESS)

    def record_failure(self, host, slot):
"
4	"Saint Bernard, St Bernard
Eskimo dog, husky
malamute, malemute, Alaskan malamute
Siberian husky
"
4	"langur
colobus, colobus monkey
proboscis monkey, Nasalis larvatus
"
4	"            default=1,
            help=""Return top k results."")
"
2	"
class ElasticSparkTorchTests(BaseElasticSparkTests):
    __test__ = True
"
2	"            str((1, 0)): [0],
        }
"
1	"            logging.info(f""Building {i}th iter-factory..."")
            iter_factory = build_func()
"
2	"        # don't wait for discovery of new hosts but have epochs be long enough to see hosts changes
        results = self._run(discovery_schedule=discovery_schedule, discovery_wait=0, epoch_wait=10,
"
2	"    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
    def test_fault_tolerance_all_hosts_lost(self):
"
1	"#!/bin/bash

# Copyright 2017 Johns Hopkins University (author: Shinji Watanabe)
#           2020 The University of Tokyo (author:Zixiong Su)
"
4	"loudspeaker, speaker, speaker unit, loudspeaker system, speaker system
loupe, jeweler's loupe
"
1	"            ctc_prefix_score = CTCPrefixScore(lpz.detach().numpy(), 0, self.eos, numpy)
            hyp[""ctc_state_prev""] = ctc_prefix_score.initial_state()
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
"
1	"        positionwise_layer_type=positionwise_layer_type,
        positionwise_conv_kernel_size=1,
        use_scaled_pos_enc=True,
        use_batch_norm=True,
        reduction_factor=reduction_factor,
"
3	"ofl-1.1: SIL Open Font License 1.1
ncsa: University of Illinois/NCSA Open Source License
unlicense: The Unlicense
"
4	"balloon
ballpoint, ballpoint pen, ballpen, Biro
Band Aid
banjo
"
4	"    """""" Get block arguments according to parameter and coefficients. """"""
    blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25',
        'r2_k3_s22_e6_i16_o24_se0.25',
"
4	"American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier
Bedlington terrier
Border terrier
Kerry blue terrier
"
4	"Kerry blue terrier
Irish terrier
Norfolk terrier
Norwich terrier
"
2	"  command: bash -c ""\$(cat /oneccl_env) && echo '/mpirun_command_ofi' > /mpirun_command && OMP_NUM_THREADS=1 \$(cat /mpirun_command) python /horovod/examples/mxnet_mnist.py""
  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
"
4	"            inputs,
            num_filters=final_oup,
"
1	"    parser.add_argument(
        ""--log_level"",
        type=lambda x: x.upper(),
        default=""INFO"",
        choices=(""INFO"", ""ERROR"", ""WARNING"", ""INFO"", ""DEBUG"", ""NOTSET""),
"
4	"shield, buckler
shoe shop, shoe-shop, shoe store
"
2	"                        threshold = 0 
                    elif size < 10:
"
4	"### 依赖

paddlepaddle >= 1.6.2
"
4	"                      inputs,
                      block_args,
                      is_test=False,
                      drop_connect_rate=None,
                      name=None):
"
4	"import base64


def cv2_to_base64(image):
    data = cv2.imencode('.jpg', image)[1]
"
3	"    display: block;
    fill: rgba(51, 51, 51, 0.5);
    margin-right: 20px;
    -webkit-transition: all .2s ease-out;
    transition: all .2s ease-out;
"
2	"

if __name__ == '__main__':
"
2	"                test_sys_path = copy.copy(sys.path)

                def reset():
"
0	"import os
import sys
import unittest

"
2	"}


"
4	"china cabinet, china closet
Christmas stocking
church, church building
cinema, movie theater, movie theatre, movie house, picture palace
cleaver, meat cleaver, chopper
"
4	"broom
bucket, pail
buckle
"
2	"      run: test-cpu-openmpi-py3_6-tf1_14_0-keras2_2_4-torch1_2_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
2	"            discovery = SparkDriverHostDiscovery(driver)

            slots = discovery.find_available_hosts_and_slots()
            self.assertEqual({}, slots)

"
4	"canoe
can opener, tin opener
"
4	"        input=input,
        op_type='conv',
        fan_out=num_filters,
        init=initial,
        use_bias=use_bias,
"
4	"
if __name__ == '__main__':
"
4	"* combined: 是否将参数保存到统一的一个文件中

## 代码示例

```python
"
4	"great grey owl, great gray owl, Strix nebulosa
European fire salamander, Salamandra salamandra
"
4	"        self.arg_input_group.add_argument(
            '--input_path', type=str, help=""path to image."")

"
2	"                        verbose=2)

"
5	"            for label, min, max, avr, tot, top in ll:
                tkinter.Label(frame, text=label).grid(row=row, column=0)
"
4	"                                          drop_connect_rate,
                                          '_blocks.' + str(idx) + '.')
                idx += 1

"
4	"            if num_data > 1:
                text_2 = fluid.data(
                    name='text_2',
                    shape=[-1, max_seq_len],
"
4	"# limitations under the License.


"
4	"from __future__ import absolute_import
from __future__ import division
"
2	"from horovod.tensorflow.mpi_ops import _executing_eagerly, init, rank, shutdown


_IS_TF2 = LooseVersion(tf.__version__) >= LooseVersion('2.0.0')
"
2	"        for instance in self._workers.copy():
            in_thread(self.stop_worker(instance), daemon=False)
"
4	"    if not multiplier:
        return filters
"
5	"                    'Total number of states checked is ([0-9]+)\\.', s)
                if m:
                    self._setText(iter=int(m.group(1)))
"
2	"    @staticmethod
    def find_duplicates(lst):
        seen = set()
        dups = set()
"
1	"@pytest.mark.skipif(
    LooseVersion(torch.__version__) < LooseVersion(""1.2""), reason=""require pytorch>=1.2""
)
"
4	"eft
spotted salamander, Ambystoma maculatum
"
1	"
        :param ndnarray x: input acoustic feature (B, T, D) or (T, D)
        :param Namespace recog_args: argment Namespace contraining options
        :param list char_list: list of characters
        :param torch.nn.Module rnnlm: language model module
"
4	"hook, claw
hoopskirt, crinoline
"
2	"class FixedHosts(HostDiscovery):
    def __init__(self, host_slots):
        super(FixedHosts, self).__init__()
"
4	"
    for element in component:
        element['image'] = process_image(element['org_im'])
        yield element
"
1	"            att_ws_ = []
            for name, m in self.named_modules():
"
2	"        return gpu_operation

    gpu_allreduce = get_gpu_op_variable('HOROVOD_GPU_ALLREDUCE', ['NCCL', 'MPI', 'DDL'])
"
4	"
        fluid.io.save_inference_model(
"
1	"                seed=args.seed,
                allow_variable_data_keys=args.allow_variable_data_keys,
                ngpu=args.ngpu,
                fold_length=args.fold_length,
"
4	"entertainment center
envelope
espresso maker
"
4	"]

GlobalParams = collections.namedtuple('GlobalParams', [
"
6	"    conv = MyConv(8, 32)
    out = conv(x1, edge_index, value)
    assert out.size() == (3, 4, 32)
    assert conv(x1, edge_index, value, (4, 4)).tolist() == out.tolist()
"
1	"            y: previous char
            next_token: next token need to be score
            state: previous state
            x: encoded feature
"
4	"            self.assertTrue((diff < 1e-5))

        test_images = [cv2.imread(img) for img in self.test_images]
        results_3 = self.module.classify(images=test_images, use_gpu=False)
        for index, res in enumerate(results_1):
"
1	"        if init_type != ""pytorch"":
            initialize(self, init_type)

        # initialize alpha in scaled positional encoding
        if self.use_scaled_pos_enc:
"
1	"                            hyp[""score""] += recog_args.lm_weight * rnnlm.final(
                                hyp[""rnnlm_prev""]
                            )
                        ended_hyps.append(hyp)
                else:
"
1	"        dropout_rate=0.1,
        positional_dropout_rate=0.1,
        attention_dropout_rate=0.0,
        input_layer=""conv2d"",
"
1	"            backward_window (int, optional): Backward window in attention constraint.
            forward_window (int, optional): Forward window in attention constraint.

"
2	"        """"""
        super(CommitStateCallback, self).__init__(keras.backend, state, batches_per_commit)
"
4	"Kerry blue terrier
Irish terrier
Norfolk terrier
"
4	"mailbox, letter box
maillot
maillot, tank suit
"
4	"butternut squash
cucumber, cuke
artichoke, globe artichoke
"
4	"                memory_pool_init_size_mb=1000, device_id=0)
            self.gpu_predictor = create_paddle_predictor(gpu_config)

    def context(self,
                trainable=True,
"
1	"            layer's input and output in decoder.
        positionwise_layer_type (str, optional):
            Position-wise operation type.
"
4	"long-horned beetle, longicorn, longicorn beetle
leaf beetle, chrysomelid
dung beetle
rhinoceros beetle
"
4	"
        conv = fluid.layers.swish(self._conv_stem_norm(inputs, is_test=is_test))
"
4	"cock
hen
ostrich, Struthio camelus
brambling, Fringilla montifringilla
goldfinch, Carduelis carduelis
"
4	"        cpu_config.disable_gpu()
        self.cpu_predictor = create_paddle_predictor(cpu_config)

        try:
"
4	"            self.directory, ""efficientnetb1_imagenet_infer_model"")
        label_file = os.path.join(self.directory, ""label_list.txt"")
"
4	"castle
catamaran
CD player
"
4	"hair spray
half track
"
4	"                'feature_map', corresponding value is the result of the layer before the fully connected layer.
            context_prog (fluid.Program): program for transfer learning.
        """"""
        if phase in [""dev"", ""test"", ""predict"", ""eval""]:
"
2	"                stdout=output, stderr=output)
            if exit_code != 0:
                raise RuntimeError(
"
4	"jackfruit, jak, jack
custard apple
pomegranate
hay
carbonara
"
1	"
        # Create a directory, but set mismatched ids
        f[""aa2""][""ccccc""] = ""aaa""
"
2	"                self.assertIn('env', execute.call_args.kwargs)
                if 'PYTHONPATH' in execute.call_args.kwargs['env']:
                    execute.call_args.kwargs['env'].pop('PYTHONPATH')

                expected_env = {'PATH': os.environ.get('PATH')}
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
"
4	"        conv = fluid.layers.swish(
            self._depthwise_conv_norm(conv, block_args, is_test, name))

        # Squeeze and Excitation
        if has_se:
"
4	"        else:
            if len(input.shape) > 2:
                fan_in = input.shape[1] * input.shape[2] * input.shape[3]
            else:
"
3	"  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
"
6	"        glorot(self.weight)
        glorot(self.comp)
        glorot(self.root)
        zeros(self.bias)
"
4	"            class_dim,
            name='_fc',
            param_attr=param_attr,
            bias_attr=bias_attr)
        return out, pool
"
2	"rank: 2: { hostname: host1; cpu: {8-11} ; gpu: * ; mem: * }
rank: 3: { hostname: host1; cpu: {12-15} ; gpu: * ; mem: * }
"
4	"        return block_strings


def EfficientNetB0_small(is_test=False,
                         padding_type='SAME',
"
4	"    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%
        new_filters += divisor
"
4	"
        return conv

    def _project_conv_norm(self, inputs, block_args, is_test, name=None):
"
4	"window screen
window shade
"
4	"lampshade, lamp shade
laptop, laptop computer
lawn mower, mower
lens cap, lens cover
"
1	"        warnings.warn(
            ""{} batch score is implemented through for loop not parallelized"".format(
                self.__class__.__name__
            )
        )
"
2	"
        # calculate local stats as well as grad_weight / grad_bias
"
4	"    def _project_conv_norm(self, inputs, block_args, is_test, name=None):
        final_oup = block_args.output_filters
        conv = self.conv_bn_layer(
"
4	"bathing cap, swimming cap
bath towel
bathtub, bathing tub, bath, tub
beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon
"
2	"
        :param delay: delay in seconds
        :type delay: float
        """"""
"
2	"    /* Cleanup */
    for (size_t ec = 0; ec < entries.size(); ++ec) {
      delete[] entry_component_sizes[ec];
"
2	"optimizer = hvd.DistributedOptimizer(optimizer,
                                     named_parameters=model.named_parameters(),
"
1	"        elif isinstance(non_linguistic_symbols, (Path, str)):
            non_linguistic_symbols = Path(non_linguistic_symbols)
            with non_linguistic_symbols.open(""r"", encoding=""utf-8"") as f:
"
2	"    queue: 4x-gpu-g4
- label: ':pytest: Run Cluster PyTests (test-gpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0)'
  command: bash -c "" /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd test_static_run.py""
"
4	"oscilloscope, scope, cathode-ray oscilloscope, CRO
overskirt
oxcart
"
2	"  retry:
    automatic: true
  agents:
    queue: 4x-gpu-g4
"
4	"ruddy turnstone, Arenaria interpres
red-backed sandpiper, dunlin, Erolia alpina
redshank, Tringa totanus
dowitcher
"
1	"        use_batch_norm: bool = True,
        encoder_normalize_before: bool = False,
        decoder_normalize_before: bool = False,
"
4	"bakery, bakeshop, bakehouse
balance beam, beam
"
4	"German short-haired pointer
vizsla, Hungarian pointer
English setter
Irish setter, red setter
Gordon setter
"
5	"                m = re.match(
                    'Total number of states checked is ([0-9]+)\\.', s)
                if m:
"
2	"

"
2	"try:
    # Spark 3.0 moved to a pandas submodule
    from pyspark.sql.pandas.types import from_arrow_type
except ImportError:
"
4	"chocolate sauce, chocolate syrup
dough
meat loaf, meatloaf
pizza, pizza pie
"
2	"                        with is_built(gloo_is_built=use_gloo, mpi_is_built=use_mpi):
                            with pytest.raises(Exception, match=expected):
                                horovod.spark.run(fn, start_timeout=10, use_mpi=use_mpi, use_gloo=use_gloo, verbose=2)
"
1	"            ""t"",
            ""i1"",
            ""。"",
        ]
    else:
"
4	"jaguar, panther, Panthera onca, Felis onca
lion, king of beasts, Panthera leo
tiger, Panthera tigris
"
2	"    automatic: true
  agents:
    queue: 2x-gpu-g4
"
4	"crane
crash helmet
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
6	"    assert conv((x1, x2), adj.t()).tolist() == out1.tolist()
    assert conv((x1, None), adj.t()).tolist() == out2.tolist()

    t = '(OptPairTensor, Tensor, OptTensor, Size) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
"
0	"        name=""UV Mapping Method"",
        items=mapping_methods,
        default=""CUBE_PROJECTION"",
        description=""How to perform UV Mapping""
"
2	"  retry:
    automatic: true
  agents:
    queue: cpu
- label: ':tensorflow: Test TensorFlow 2.0 MNIST (test-cpu-gloo-py3_8-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark3_0_0)'
"
4	"    elif act == None:
        conv = conv
    else:
        raise NotImplementedError(""activation: [%s] is not support"" % act)

"
4	"        exe = fluid.Executor(place)

        program, feeded_var_names, target_vars = fluid.io.load_inference_model(
"
4	"toucan
drake
red-breasted merganser, Mergus serrator
goose
"
3	"            self.assertTrue(req.status.code < jina_pb2.Status.ERROR)
            self.assertEqual(req.search.docs[0].chunks[0].topk_results[0].score.op_name, indexer_name)
"
4	"        self.arg_input_group = self.parser.add_argument_group(
            title=""Input options"", description=""Input data. Required"")
        self.arg_config_group = self.parser.add_argument_group(
            title=""Config options"",
            description=
"
4	"    author=""paddlepaddle"",
    author_email=""paddle-dev@baidu.com"",
    summary=
    ""EfficientNetB4 is a image classfication model, this module is trained with imagenet datasets."",
    version=""1.1.0"")
"
2	"def _epoch_time_s():
    return int(time.time())

"
2	"        env = {'env1': 'val1', 'env2': 'val2', 'PATH': 'path'}
        expected_env = '-x PATH -x env1 -x env2'
"
4	"# limitations under the License.

"
2	"def execute(command, env=None, stdout=None, stderr=None, index=None, events=None):
    ctx = multiprocessing.get_context('spawn')

    # When this event is set, signal to middleman to terminate its children and exit.
    exit_event = _create_event(ctx)
"
4	"mink
polecat, fitch, foulmart, foumart, Mustela putorius
black-footed ferret, ferret, Mustela nigripes
"
4	"            conv = self.mb_conv_block(conv, block_args, is_test,
                                      drop_connect_rate,
"
4	"                    padding_idx=dict_dim - 1,
                    dtype='float32',
                    param_attr=w_param_attrs)
                emb_3_name = emb_3.name
"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the 'License');
"
4	"        self.assertEqual(height, 224)
        self.assertEqual(mean.tolist(), self.true_mean)
        self.assertEqual(std.tolist(), self.true_std)

"
4	"import paddle.fluid as fluid
import paddlehub as hub
"
4	"head cabbage
broccoli
cauliflower
zucchini, courgette
"
2	"
import io
import os

"
4	"            moving_variance_name=name + '_variance')

    elif norm_type == 'instance_norm':
"
1	"    echo ""stage 2: Json Data Preparation""

    nlsyms_opts=""""
    if [ ! -z ${nlsyms} ]; then
"
1	"    assert phoneme_tokenizer.tokens2text([""a"", ""b"", ""c""]) == ""abc""

"
2	"                                side_effect=lambda host: host), \
                     mock.patch('horovod.run.mpi_run.os.execve') as exec:
                    yield hargs, exec
        finally:
            stdout = stdout.readlines()
"
4	"    def setUp(self):
        self.module = hub.Module(name='efficientnetb5_imagenet')
        self.test_images = [
"
1	"        f.write(string.ascii_letters + ""\n"")

    spm.SentencePieceTrainer.Train(
        f""--input={input_text} ""
"
4	"```

将模型保存到指定路径。

**参数**
"
6	"        if self.divide_input:
            x = x.view(-1, self.towers, self.F_in)
"
3	"                    revised_dockerfile.append(
                        ' \\      \n'.join(f'{_label_prefix}{k}=""{v}""' for k, v in manifest.items()))

"
4	"

def EfficientNetB5(is_test=False,
"
4	"pill bottle
pillow
"
6	"    value = torch.rand(row.size(0), 3)
    adj = SparseTensor(row=row, col=col, value=value, sparse_sizes=(4, 4))

"
4	"        bound = 1 / math.sqrt(fan_in)
        param_attr = fluid.ParamAttr(
            name=name + ""_weights"",
            initializer=fluid.initializer.Uniform(low=-bound, high=bound))
"
4	"puffer, pufferfish, blowfish, globefish
abacus
"
2	"  command: bash -c "" cd /horovod/test && (echo test_*.py | sed 's/test_keras.py//g' | sed 's/test_tensorflow_keras.py//g' | sed 's/test_interactiverun.py//g' | sed 's/test_spark_keras.py//g' | sed 's/test_spark_torch.py//g' | sed 's/test_spark.py//g' | sed 's/test_run.py//g' | xargs -n 1 \$(cat /mpirun_command) pytest -v --capture=no) && pytest --forked -v --capture=fd test_spark.py test_run.py""
  plugins:
  - docker-compose#v2.6.0:
"
2	"    Args:
        bcast_object: Horovod broadcast object function used to sync state dictionary.
        get_rank: Horovod rank function used to identify is this process is the coordinator.
        kwargs: Properties to sync, will be exposed as attributes of the object.
"
4	"        'efficientnet-b3': (1.2, 1.4, 300, 0.3),
        'efficientnet-b4': (1.4, 1.8, 380, 0.4),
        'efficientnet-b5': (1.6, 2.2, 456, 0.4),
        'efficientnet-b6': (1.8, 2.6, 528, 0.5),
"
4	"vending machine
vestment
"
4	"            for _ in range(block_arg.num_repeat - 1):
                block_size += 1

        for block_args in self._blocks_args:

"
4	"                    lod_level=0)
                emb_2 = fluid.embedding(
                    input=text_2,
"
4	"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
4	"        name='b3',
        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
"
2	"                res = horovod.spark.run_elastic(fn, args=(2, 5, 5, dir),
                                                env={'HOROVOD_LOG_LEVEL': 'DEBUG'},
                                                num_proc=2, min_np=2, max_np=2,
                                                start_timeout=5, verbose=2)
                self.assertListEqual([([0, 4, 0, 4, 1, 4, 0, 4], 0),
"
1	"        # linear -> GLU -- -> lightconv -> linear
        #               \        /
        #                 Linear
"
4	"soap dispenser
soccer ball
sock
solar dish, solar collector, solar furnace
"
2	"
        v = 1.0 if hvd.rank() == 0 else 2.0
        model1 = torch.nn.Sequential(torch.nn.Linear(2, 2))
        model1.load_state_dict({
"
4	"                   use_se=True):
    model = EfficientNet(
        name='b7',
"
4	"        name=name + '_offset',
        initializer=fluid.initializer.Constant(value=0.0))
    return param_attr, bias_attr


"
4	"
    Args:
"
2	"        with mock.patch(""horovod.run.mpi_run._get_mpi_implementation_flags"", side_effect=mpi_impl_flags):
            with mock.patch(""horovod.run.mpi_run.safe_shell_exec.execute"", return_value=1):
                with mock.patch(""horovod.spark.gloo_run._exec_command_fn"", side_effect=gloo_exec_command_fn):
                    with spark_session('test_spark_run'):
"
4	"    data = cv2.imencode('.jpg', image)[1]
    return base64.b64encode(data.tostring()).decode('utf8')

"
6	"    jit = torch.jit.script(conv.jittable(t))
    assert jit((x1, x2), adj.t()).tolist() == out1.tolist()
"
2	"
    spark_context = pyspark.SparkContext._active_spark_context
    if spark_context is None:
        raise Exception('Could not find an active SparkContext, are you '
"
4	"backpack, back pack, knapsack, packsack, rucksack, haversack
bakery, bakeshop, bakehouse
balance beam, beam
balloon
ballpoint, ballpoint pen, ballpen, Biro
"
4	"#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
4	"        if id_skip and block_args.stride == 1 and input_filters == output_filters:
            if drop_connect_rate:
                conv = self._drop_connect(conv, drop_connect_rate, is_test)
            conv = fluid.layers.elementwise_add(conv, inputs)

"
4	"    return img


def crop_image(img, target_size, center):
"
1	"        weight_new = weight_new.narrow(-1, int((k - 1) / 2), T)  # B x H x T x T(k)
        if self.use_kernel_mask:
            kernel_mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0)
            weight_new = weight_new.masked_fill(kernel_mask == 0.0, float(""-inf""))
"
4	"lens cap, lens cover
letter opener, paper knife, paperknife
"
2	"        self.assertEqual(results[0]['hostname'], results[1]['hostname'])

        self.assertEqual(0, results[2]['start_rank'])
        self.assertEqual(3, results[2]['size'])
        self.assertEqual(3, results[2]['rendezvous'])
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
1	"        --tensorboard-dir tensorboard/${expname} \
        --debugmode ${debugmode} \
        --dict ${dict} \
        --debugdir ${expdir} \
"
4	"rubber eraser, rubber, pencil eraser
rugby ball
rule, ruler
running shoe
safe
"
2	"def parse_host_files(filename):
    """"""
    Transform the hostfile into a format of
    <IP address> or <host name>:<Number of GPUs>
    :param filename: Should be in <IP address> or <host name> slots=<number of GPUs>
"
3	"  full_name='jina.Status.Details',
  filename=None,
  file=DESCRIPTOR,
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: cpu
"
4	"        for index in indexs:
            label = label_list[index].split(',')[0]
            output_i[label] = float(result_i[index])
"
4	"                    batch_data.append(all_data[handle_id + image_id])
                except:
                    pass
"
2	"            self._allreduce_delay[p] = self.backward_passes_per_step
            p.grad.set_(self._compression.decompress(output, ctx))
        self._handles.clear()

"
2	"    """"""
    cpu_per_gpu = (lsf.LSFUtils.get_num_cores() * lsf.LSFUtils.get_num_threads()) // lsf.LSFUtils.get_num_gpus()
    host_list = (x.split(':') for x in settings.hosts.split(','))

    # Verify and truncate host list if necessary
"
1	"def test_ESPnetDataset_csv_int(csv_int):
    dataset = IterableESPnetDataset(
        path_name_type_list=[(csv_int, ""data8"", ""csv_int"")], preprocess=preprocess,
    )

"
2	"

def check_exit(epoch, batch):
    key = str((epoch, batch))
"
2	"    the end of each epoch.
    """"""
"
1	"
        # make labels for stop prediction
        labels = make_pad_mask(olens - 1).to(ys.device, ys.dtype)
        labels = F.pad(labels, [0, 1], ""constant"", 1.0)
"
4	"            splits = re.split(r'(\d.*)', op)
            if len(splits) >= 2:
"
4	"                name=bn_name,
                moving_mean_name=bn_name + '_mean',
"
2	"from horovod.run.runner import gloo_built, parse_args, run_controller, HorovodArgs, _run
from horovod.run.util.threads import in_thread, on_event

"
2	"                    state.commits += 1
                    state.commit()

            logging.info('rank %s: allgather', hvd.rank())
            hvd.allgather(torch.tensor([hvd.rank(), state.epoch, state.batch, state.rendezvous]), 'state').tolist()
"
2	"        verbose: Debug output verbosity (0-2). Defaults to 1.
        nics: List of NICs for tcp network communication.
"
4	"        scale = helper.create_parameter(
            attr=scale_param, shape=input.shape[1:2], dtype=dtype)
"
4	"        for im in images:
            each = OrderedDict()
            each['org_im'] = Image.fromarray(im[:, :, ::-1])
"
1	"
def test_text2tokens(spm_tokenizer: SentencepiecesTokenizer):
    assert spm_tokenizer.tokens2text(spm_tokenizer.text2tokens(""Hello"")) == ""Hello""

"
4	"fig
pineapple, ananas
banana
jackfruit, jak, jack
custard apple
"
2	"        elif args.op == ""Adasum"":
            net.param.grad.data = hvd.allreduce(net.param.grad.data, op=hvd.Adasum)

"
2	"class TensorFlowState(ObjectState):
    """"""State representation of a list of TensorFlow variables.

    Supports both TensorFlow v1 and v2. For TensorFlow v2, can only be used when eager execution is enabled.

"
1	"    with p.open(""w"") as f:
        f.write(""abc 12.3.3.,4.44\n"")
"
6	"    @property
    def processed_file_names(self):
"
2	"                            '--host {host} '
                            '--port 0'.format(spark_home=self._spark_home,
                                              instance=self._master_instance,
                                              host=self._master_host))
        if not res or res[1]:
"
4	"    def classify(self,
                 images=None,
                 paths=None,
                 batch_size=1,
"
4	"zebra
hog, pig, grunter, squealer, Sus scrofa
"
6	"        if isinstance(x, Tensor):
            x: PairTensor = (x, x)
"
4	"pizza, pizza pie
potpie
burrito
"
2	"""""""
The purpose of this file is to execute a safe_shell_exec command in an isolated Python interpreter that doesn't
share resources with the main unit testing interpreter.

Python runs a global process called Semaphore Tracker that is used across processes spawned with the multiprocessing
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: 2x-gpu-g4
"
2	"                                'spark.task.maxFailures': '4',
                                'spark.blacklist.enabled': 'false',
                                'spark.blacklist.stage.maxFailedTasksPerExecutor': '2',
                                'spark.blacklist.stage.maxFailedExecutorsPerNode': '2',
"
2	"class _DistributedAdasumOptimizer(torch.optim.Optimizer):
    def __init__(self, params, named_parameters, compression,
                 backward_passes_per_step=1):
"
2	"  - ecr#v1.2.0:
      login: true
"
4	"scale, weighing machine
school bus
schooner
"
2	"        os.chdir(work_dir)

"
1	"        if key == ""a"":
            assert data[""data6""].shape == (100, 80,)
        if key == ""b"":
            assert data[""data6""].shape == (150, 80,)
"
4	"Pekinese, Pekingese, Peke
Shih-Tzu
Blenheim spaniel
"
4	"partridge
African grey, African gray, Psittacus erithacus
"
4	"banana
jackfruit, jak, jack
custard apple
"
4	"cornet, horn, trumpet, trump
cowboy boot
cowboy hat, ten-gallon hat
cradle
crane
"
2	"                    help='input batch size for testing (default: 1000)')
parser.add_argument('--epochs', type=int, default=10, metavar='N',
                    help='number of epochs to train (default: 10)')
parser.add_argument('--lr', type=float, default=0.01, metavar='LR',
"
2	"      login: true
  timeout_in_minutes: 5
"
4	"            batch_size=args.batch_size,
            use_gpu=args.use_gpu)
        return results

    def add_module_config_arg(self):
"
1	"        super(LightweightConvolution2D, self).__init__()

        assert n_feat % wshare == 0
        self.wshare = wshare
"
4	"            name='_fc',
            param_attr=param_attr,
            bias_attr=bias_attr)
"
2	"    if settings.ssh_port:
        ssh_port_arg = '-p {ssh_port}'.format(ssh_port=settings.ssh_port)
    else:
"
2	"        # Days & months competition was open, cap to 2 years.
        df = df.withColumn('CompetitionOpenSince',
                           F.to_date(F.format_string('%s-%s-15', df.CompetitionOpenSinceYear,
"
1	"        ""--dtype"",
        choices=(""float16"", ""float32"", ""float64""),
        default=""float32"",
        help=""Float precision (only available in --api v2)"",
"
2	"    queue: cpu
- label: ':spark: Spark Keras MNIST (test-cpu-openmpi-py3_6-tf1_14_0-keras2_2_4-torch1_2_0-mxnet1_4_1-pyspark2_4_0)'
"
1	"            ys = list(y_feats_dict.values())
            assert len(xs[0]) == len(ys[0]), (len(xs[0]), len(ys[0]))
"
3	"        :return: a gzip file stream
        """"""
"
2	"            self.assertNotEqual(results[1]['hostname'], results[2]['hostname'])
            self.assertEqual(3, results[2]['rendezvous'])

    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
"
4	"                name=""embedding_0.w_0"",
                initializer=fluid.initializer.TruncatedNormal(scale=0.02),
                trainable=trainable)
            dict_dim = 1256607
"
4	"trench coat
tricycle, trike, velocipede
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
"
2	"
# executors where tasks fail due to exceptions can potentially be reused for the same task
# executors lost e.g. due to node failure can't be reused in any way
# requires SPARK_CONF_BLACKLIST_ENABLED
SPARK_CONF_REUSE_EXECUTOR_ALWAYS_FOR_SAME_TASK = ('spark.blacklist.task.maxTaskAttemptsPerExecutor', SPARK_CONF_MAX_INT)
"
6	"    """"""

    url = 'https://www.dropbox.com/s/qdwi3wh18kcumqd/WN18.gpickle?dl=1'
"
4	"cheeseburger
hotdog, hot dog, red hot
mashed potato
head cabbage
"
4	"            for _ in range(block_arg.num_repeat - 1):
                block_size += 1
"
0	"        x, y, z = btools.utils.local_xyz(dummy)
        self.assertEqual(x.to_tuple(1), Vector((0, 1, 0)).to_tuple(1))
        self.assertEqual(y.to_tuple(1), Vector((0, 0, 1)).to_tuple(1))
        self.assertEqual(z.to_tuple(1), Vector((1, 0, 0)).to_tuple(1))

"
1	"

@pytest.mark.parametrize(""test_sens"", [test_sens])
def test_ngram_build(test_sens):
    lm = kenlm.LanguageModel(os.path.join(root, ""test.arpa""))
"
2	"
        message = 'Horovod detected that one or more processes exited with non-zero status'
"
6	"        G = nx.read_gpickle(self.raw_paths[0])
        data = [[v, w, d['e_label'].item()] for v, w, d in G.edges(data=True)]

"
2	"
    // Write integer to output tensor
    Tensor* output;
    OP_REQUIRES_OK(context,
"
4	"little blue heron, Egretta caerulea
American egret, great white heron, Egretta albus
bittern
crane
"
2	"                               'to do so was:\nProcess name: {name}\nExit code: {code}\n'
                               .format(name=name, code=exit_code))

"
2	"        with mock.patch(""horovod.run.mpi_run._get_mpi_implementation_flags"", side_effect=mpi_impl_flags),\
             mock.patch(""horovod.run.mpi_run.safe_shell_exec.execute"", return_value=0) as execute,\
             override_env(sysenv):
            mpi_run(settings, None, argenv, cmd)
"
4	"        if isinstance(s, list) or isinstance(s, tuple):
            s = s[0]
"
2	"                       If not set, falls back to `HOROVOD_SPARK_START_TIMEOUT` environment variable value.
                       If it is not set as well, defaults to 600 seconds.
"
1	"    for key, data in dataset:
        if key == ""a"":
            assert tuple(data[""data7""]) == (0,)
        if key == ""b"":
"
4	"    global_params = GlobalParams(
        batch_norm_momentum=0.99,
"
4	"        self.arg_config_group = self.parser.add_argument_group(
            title=""Config options"",
            description=
"
2	"        for attr in self._saved_state.keys():
            new_state[attr] = getattr(self, attr)
        self._saved_state = new_state

    def restore(self):
"
2	"    with fs.open(path, 'rb') as train_meta_file:
        meta = train_meta_file.read()
"
4	"    @staticmethod
    def _decode_block_string(block_string):
        """""" Gets a block through a string notation of arguments. """"""
        assert isinstance(block_string, str)
"
1	"                _num_splits = int(f.read())
                if num_splits is not None and num_splits != _num_splits:
                    raise RuntimeError(
"
2	"        return _run_static(args)



"
4	"                    def _if_exist(var):
                        b = os.path.exists(
"
4	"#
#     http://www.apache.org/licenses/LICENSE-2.0
"
2	"
import mock
"
4	"    Preprocess to yield image.

"
4	"breakwater, groin, groyne, mole, bulwark, seawall, jetty
breastplate, aegis, egis
broom
bucket, pail
buckle
"
1	"    parser.add(
        ""--config3"",
        is_config_file=True,
        help=""Third config file path that overwrites the settings ""
        ""in `--config` and `--config2`"",
"
4	"fireboat
fire engine, fire truck
"
4	"            bn_act=None,
            padding_type=self.padding_type,
"
2	"
args = parser.parse_args()
args.cuda = not args.no_cuda and torch.cuda.is_available()

"
2	"            # Skip if compiled with CUDA but without HOROVOD_GPU_OPERATIONS.
            self.skipTest(""Not compiled with HOROVOD_GPU_OPERATIONS"")
"
2	"                with open(os.path.sep.join([dir, 'rank_1_epoch_2_batch_4_fail']), 'w'), \
                     open(os.path.sep.join([dir, 'rank_1_epoch_3_batch_1_fail']), 'w'):
"
2	"        .load(libsvm_path)

"
4	"vulture
great grey owl, great gray owl, Strix nebulosa
European fire salamander, Salamandra salamandra
common newt, Triturus vulgaris
"
4	"                    exe.run(startup_prog)
                # trainable
"
2	"    return run_fn(func, _reset)


def _reset():
    shutdown()
"
4	"        indexs = np.argsort(result_i)[::-1][0:top_k]
        for index in indexs:
            label = label_list[index].split(',')[0]
            output_i[label] = float(result_i[index])
"
4	"Brabancon griffon
Pembroke, Pembroke Welsh corgi
Cardigan, Cardigan Welsh corgi
toy poodle
miniature poodle
"
4	"            return conv
        else:
            bn_name = name + bn_name
            param_attr, bias_attr = init_batch_norm_layer(bn_name)
            return fluid.layers.batch_norm(
"
6	"    def __logstd__(self):
        return self.VGAE.__logstd__
"
6	"    if return_type is inspect.Parameter.empty:
        return 'torch.Tensor'
"
3	"        self.assertEqual(idx.shape, dist.shape)
        self.assertEqual(idx.shape, (10, 4))
        self.add_tmpfile(a.index_abspath, a.save_abspath)

"
2	"  retry:
    automatic: true
  agents:
"
4	"Greater Swiss Mountain dog
Bernese mountain dog
Appenzeller
"
2	"    conf = SparkConf().setAppName('training')
    if args.training_master:
"
3	"        locs = [i['location'] for i in sentencizer.craft(text, 0)]
        self.assertListEqual(chunks, [""This ,  text is..."", ""Amazing""])
        self.assertEqual(text[locs[0][0]:locs[0][1]], '  This ,  text is...')
        self.assertEqual(text[locs[1][0]:locs[1][1]], ' Amazing')
"
2	"  agents:
    queue: cpu
- label: ':factory: Elastic Spark TensorFlow Tests (test-cpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""cd /horovod/test/integration && SPARK_HOME=/spark SPARK_DRIVER_MEM=512m HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format '[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s' --capture=no test_elastic_spark_tensorflow.py""
"
4	"        test_images = [cv2.imread(img) for img in self.test_images]
        results_3 = self.module.classify(images=test_images, use_gpu=False)
        for index, res in enumerate(results_1):
            self.assertTrue(res.keys(), results_3[index].keys())
"
2	"  agents:
    queue: cpu
- label: ':muscle: Test MXNet MNIST (test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
"
3	"                # self.executor(self.request_type)
                # envelope will be dropped when returning to the client
"
4	"notebook, notebook computer
obelisk
oboe, hautboy, hautbois
ocarina, sweet potato
"
4	"coyote, prairie wolf, brush wolf, Canis latrans
dingo, warrigal, warragal, Canis dingo
dhole, Cuon alpinus
African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus
"
4	"
    return blocks_args, global_params
"
2	"    # Post-broadcast cleanup for non-tensor parameters
    for key, p in params:
        if key in callbacks:
            callbacks[key]()
"
1	"                    econv_chans=eprenet_conv_chans,
                    econv_filts=eprenet_conv_filts,
                    use_batch_norm=use_batch_norm,
                    dropout_rate=eprenet_dropout_rate,
                    padding_idx=self.padding_idx,
"
2	"

def fn(batches_per_commit, batches_per_epoch, epochs, dir=None):
    @run
"
2	"        self.check_host_updates()

    def check_host_updates(self):
        """"""Checks that a notification has been sent indicating that hosts can be added or will be removed.

"
2	"        logging.info('all {} workers recorded'.format(self.size()))

        # Check for success state, if any process succeeded, shutdown all other processes
        if self.count(SUCCESS) > 0:
            logging.info('success count == {} -> stop running'.format(self.count(SUCCESS)))
"
4	"German shepherd, German shepherd dog, German police dog, alsatian
Doberman, Doberman pinscher
miniature pinscher
"
2	"from horovod.run.elastic.worker import WorkerNotificationManager
from horovod.run.http.http_server import RendezvousServer
"
4	"        name=name,
        input=input,
"
1	"

def test_repr(spm_tokenizer: SentencepiecesTokenizer):
    print(spm_tokenizer)

"
2	"        if named_parameters is not None:
            named_parameters = list(named_parameters)
        else:
            named_parameters = [('allreduce.noname.%s' % i, v)
"
4	"stretcher
studio couch, day bed
"
1	"        # initialize
        idx = 0
        ys = hs.new_zeros(1, 1, self.odim)
        outs, probs = [], []
"
4	"        use_se=use_se)
    return model


"
2	"        state.commit()

"
6	"            final dataset. (default: :obj:`None`)
    """"""

    names = ['PATTERN', 'CLUSTER', 'MNIST', 'CIFAR10', 'TSP', 'CSL']
"
4	"
```python
def get_pretrained_images_mean()
```

"
4	"horizontal bar, high bar
horse cart, horse-cart
"
1	"    assert ""def"" in target
    assert tuple(target.keys()) == tuple(desired)
"
1	"                        normalize_before,
                        concat_after,
                    ),
"
4	"                top_k=top_k)
            res += out
"
4	"        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
        use_se=use_se)
"
4	"

if __name__ == '__main__':
"
2	"
    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
"
4	"Walker hound, Walker foxhound
English foxhound
redbone
borzoi, Russian wolfhound
"
4	"triceratops
thunder snake, worm snake, Carphophis amoenus
ringneck snake, ring-necked snake, ring snake
hognose snake, puff adder, sand viper
"
4	"fountain
fountain pen
"
4	"snowmobile
snowplow, snowplough
soap dispenser
soccer ball
sock
"
4	"tennis ball
thatch, thatched roof
theater curtain, theatre curtain
thimble
"
5	"from pysol_cards.random import LCRandom31  # noqa: E402,I100
from pysol_cards.random import match_ms_deal_prefix  # noqa: E402,I100
"
1	"    parser.add_argument(""--ngram-weight"", type=float, default=0.1, help=""ngram weight"")
    parser.add_argument(
"
4	"chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour
chain saw, chainsaw
chest
"
0	"
def clear_empty_facemaps(context):
"
3	"    @as_ndarray
    def encode(self, data: 'np.ndarray', *args, **kwargs) -> 'np.ndarray':
        """"""
"
1	"        if self.decoder is not None:
            ys_in_pad, ys_out_pad = add_sos_eos(
"
4	"                 is_test=False,
                 use_se=True):
        valid_names = ['b' + str(i) for i in range(8)]
        assert name in valid_names, 'efficient name should be in b0~b7'
        model_name = 'efficientnet-' + name
"
2	"
        if hvd.rank() == 0:
            log_state(state)

"
4	"bolo tie, bolo, bola tie, bola
bonnet, poke bonnet
"
1	"import collections.abc
from pathlib import Path
from typing import Union

import numpy as np
"
2	"    if key in exit_schedule:
        ranks_to_exit = exit_schedule[key]
        if start_rank in ranks_to_exit:
            if args.exit_mode == 'exception':
                raise RuntimeError('check_rank and exit epoch={} batch={} start_rank={} rank={}'
"
4	"            dirname=dirname,
            main_program=program,
"
4	"from efficientnetb7_imagenet.data_feed import reader
from efficientnetb7_imagenet.efficientnet import EfficientNetB7
"
3	"}

#border {
    height: 288px;
    width: 1px;
"
4	"
        if not is_test and self._global_params.dropout_rate:
"
5	"            self._setText(
                states=bh_solve_lib_obj.get_num_states_in_collection())
            if self.solver_state == 'solved':
                m = bh_solve_lib_obj.get_next_move()
"
4	"            drop_connect_rate = self._global_params.drop_connect_rate
            if drop_connect_rate:
"
4	"#
# Licensed under the Apache License, Version 2.0 (the ""License"");
"
2	"                                      verbose=0)

    """"""
    Test that horovod.spark.run fails with meaningful exception when mpirun cannot be found.
    This test does not require MPI to be installed.
"
2	"    queue: cpu
- label: ':tensorflow: Test TensorFlow MNIST (test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2)'
  command: bash -c "" \$(cat /mpirun_command) python /horovod/examples/tensorflow_mnist.py""
  plugins:
"
2	"            assert driver.finished()
        finally:
            ElasticDriver._discover_hosts = discover_hosts

"
2	"
def on_state_reset():
    tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())
"
4	"bluetick
black-and-tan coonhound
"
4	"koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus
wombat
"
1	"    if cleaner.cleaner_types[0] == ""tacotron"":
        assert (
"
4	"        out_channels = round_filters(32, self._global_params)
        bn = self.conv_bn_layer(
            inputs,
"
1	"                    preprocess_fn=cls.build_preprocess_fn(args, train=False),
                    collate_fn=cls.build_collate_fn(args),
                    num_batches=args.num_att_plot,
"
0	"
    # return None, None
    # -- add window depth
    win, frames = add_window_depth(bm, mid, prop.window_depth, xyz[2])
    add_faces_to_map(bm, [win], FaceMap.WINDOW)
"
0	"        bm = bmesh.from_edit_mesh(me)
    elif context.mode == ""OBJECT"":
"
4	"                conv = self._drop_connect(conv, drop_connect_rate, is_test)
            conv = fluid.layers.elementwise_add(conv, inputs)
"
1	"            / self.num_spkrs
        )
"
2	"# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"
2	"        # test None stop and non-daemon
        event = threading.Event()
"
4	"binder, ring-binder
binoculars, field glasses, opera glasses
"
0	"        res = bpy.ops.btools.add_floorplan()
        self.assertEqual(res, {""FINISHED""})

        obj = bpy.context.object
"
6	"            :obj:`""var""` and :obj:`""std""`.
        scalers: (list of str): Set of scaling function identifiers, namely
"
4	"earthstar
hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa
"
4	"        warnings.warn(
            'padding value and padding type are set in the same time, and the final padding width and padding height are computed by padding_type'
"
4	"```

**参数**

* images (list\[numpy.ndarray\]): 图片数据，每一个图片数据的shape 均为 \[H, W, C\]，颜色空间为 BGR；
"
2	"        new number of workers.

        Args:
"
4	"        return conv

"
4	"catamaran
CD player
"
3	"    def test_chunk_joint_idx(self):
        f = Flow().add(yaml_path='yaml/test-joint.yml')

        def random_docs(num_docs, chunks_per_doc=5, embed_dim=10):
"
2	"            callbacks: list of functions to execute.
        """"""
"
4	"vacuum, vacuum cleaner
vase
vault
velvet
"
4	"        name='b3',
        is_test=is_test,
        padding_type=padding_type,
"
4	"            self.label_list = file.read().split(""\n"")[:-1]
        self.classification = self.classify
        self._set_config()

    def get_expected_image_width(self):
"
4	"            'k%d' % block.kernel_size,
            's%d%d' % (block.strides[0], block.strides[1]),
"
4	"
                place = fluid.CPUPlace()
                exe = fluid.Executor(place)
                # pretrained
                if pretrained:
"
4	"                image = fluid.layers.data(
                    name=""image"", shape=[3, 224, 224], dtype=""float32"")
"
4	"
返回预处理的图片高度，也就是224。

```python
"
5	"use_fc_solve_lib = False

try:
    import freecell_solver
    fc_solve_lib_obj = freecell_solver.FreecellSolver()
"
1	"
        Returns:
            Tensor: Output sequence of features (L, odim).
            Tensor: Output sequence of stop probabilities (L,).
"
4	"# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
"
4	"

def crop_image(img, target_size, center):
"
2	"        df = df.withColumn('Promo2Weeks', (df.Promo2Days / 7).cast(T.IntegerType()))

        # Check that we did not lose any rows through inner joins.
        assert num_rows == df.count(), 'lost rows in joins'
        return df
"
4	"barbershop
barn
barometer
barrel, cask
"
1	"        self,
        text: torch.Tensor,
"
4	"                             dirname,
                             model_filename=None,
                             params_filename=None,
                             combined=True):
"
4	"def init_batch_norm_layer(name=""batch_norm""):
    param_attr = fluid.ParamAttr(
        name=name + '_scale', initializer=fluid.initializer.Constant(1.0))
    bias_attr = fluid.ParamAttr(
        name=name + '_offset',
"
2	"    Performs an actual horovod.spark.run test using MPI.
    """"""
"
4	"# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
1	"            self.encoder.embed[-1].alpha.data = torch.tensor(init_enc_alpha)
            self.decoder.embed[-1].alpha.data = torch.tensor(init_dec_alpha)

    def forward(
"
4	"                   batch_size=1,
                   use_gpu=False,
                   top_k=1):
```
"
4	"maillot, tank suit
manhole cover
maraca
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
2	"
import collections
import io

from collections.abc import Iterable
"
5	"            )
            if status == 0:
                m = fc_solve_lib_obj.get_next_move()
                while m:
"
4	"wallet, billfold, notecase, pocketbook
wardrobe, closet, press
"
2	"    encoded = base64.b64encode(serialized)
    return encoded.decode('ascii') if to_ascii else encoded
"
4	"    author=""paddlepaddle"",
    author_email=""paddle-dev@baidu.com"",
    summary=
    ""EfficientNetB6 is a image classfication model, this module is trained with imagenet datasets."",
"
6	"                # propagate_type: (x: Tensor, edge_weight: OptTensor)
                x = self.propagate(edge_index, x=x, edge_weight=edge_weight,
                                   size=None)
                if self.cached:
"
4	"        if phase in [""dev"", ""test"", ""predict"", ""eval""]:
            is_test = True
        elif phase in [""train""]:
"
4	"
返回预处理的图片标准差，也就是 \[0.229, 0.224, 0.225\]。


```python
"
4	"tench, Tinca tinca
goldfish, Carassius auratus
great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias
"
4	"        self._blocks_args, self._global_params = get_model_params(
            model_name, override_params)
        self._bn_mom = self._global_params.batch_norm_momentum
        self._bn_eps = self._global_params.batch_norm_epsilon
"
4	"minivan
missile
"
2	"
    def zero_grad(self):
"
3	"      name: wrap-npidx
    with:
"
2	"        first_update = True
        while not self._shutdown.is_set():
            self._wait_hosts_cond.acquire()
"
1	"class EncoderMix(Encoder, torch.nn.Module):
    """"""Transformer encoder module.

"
4	"        warnings.warn(
            'padding value and padding type are set in the same time, and the final padding width and padding height are computed by padding_type'
        )

    param_attr, bias_attr = initial_type(
"
2	"  command: bash -c "" /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd test_static_run.py""
  plugins:
  - docker-compose#v2.6.0:
"
4	"    ]
    res = b0.classification(images=test_image)
    print(res)
    res = b0.classification(paths=[
"
4	"        padding_type=padding_type,
        override_params=override_params,
"
4	"Tibetan terrier, chrysanthemum dog
silky terrier, Sydney silky
soft-coated wheaten terrier
West Highland white terrier
"
6	"
    t = '(Tensor, SparseTensor, Size) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert jit(x1, adj.t()).tolist() == out.tolist()
"
4	"        final_oup = block_args.output_filters
        conv = self.conv_bn_layer(
"
1	"                )
                for i in range(self.num_spkrs ** 2)
            ],
            dim=1,
        )  # (B, num_spkrs^2)
"
2	"    :param command: command and arguments to invoke
    :param env: environment to use
    :param local_rank: local rank on the host of task to run the command in
    :param verbose: verbosity level
    :param background: run command in background if True, returns command result otherwise
"
4	"    for element in component:
        element['image'] = process_image(element['org_im'])
"
4	"```python
def get_expected_image_height()
```
"
4	"            args.append('se%s' % block.se_ratio)
        if block.id_skip is False:
"
2	"            self.skipTest(""Gloo is not available"")

        cmd = ['whoami']
        settings = self.minimal_settings
        gloo_run(settings, ['lo'], {}, '127.0.0.1', cmd)
"
4	"            if len(splits) >= 2:
                key, value = splits[:2]
                options[key] = value
"
0	"
    def tearDown(self):
        self.bm.free()

"
4	"dishrag, dishcloth
dishwasher, dish washer, dishwashing machine
disk brake, disc brake
dock, dockage, docking facility
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
4	"speedboat
spider web, spider's web
"
4	"    main()

"
4	"echidna, spiny anteater, anteater
platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus
wallaby, brush kangaroo
koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus
"
4	"            seq_len = fluid.layers.data(
                name=""seq_len"", shape=[1], dtype='int64', lod_level=0)
"
2	"        self._hosts_state = defaultdict(HostState)
        self._discovery = discovery
"
4	"            bias_attr = fluid.ParamAttr(
                name=name + ""_offset"",
                initializer=fluid.initializer.Constant(0.0))
"
1	"
            # Plot attention weight
            att_ws = att_ws.cpu().numpy()
"
4	"window screen
window shade
Windsor tie
wine bottle
wing
"
2	"        # we are not asserting on the actual PYTHONPATH in actual_env, this is done in test_run.py
        self.assertEqual(expected_command, actual_command)
        if 'PYTHONPATH' in actual_env:
            actual_env.pop('PYTHONPATH')
        # we compare this secret below, not by comparing actual_env with env
"
2	"        }

        # it can take 5 seconds for a task to be restarted by Spark, so we make each epoch take 10s
        results = self._run(hosts=hosts, exit_schedule=exit_schedule,
                            epoch_wait=10, epochs=2, np=4, min_np=1,
"
1	"            assert data[""data4""].shape == (150, 80,)


"
2	"  - docker-compose#v2.6.0:
      run: test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
"
2	"                executable, args, env = args
                self.assertEqual('/bin/sh', executable)
                self.assertEqual(3, len(args))
                self.assertEqual('/bin/sh', args[0])
"
2	"  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/keras_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""
  plugins:
  - docker-compose#v2.6.0:
"
1	"
    def __init__(
        self,
"
2	"                         for v in param_group['params']}
        named_param_ids = {id(v) for k, v in named_parameters}
        unnamed_param_ids = all_param_ids - named_param_ids
"
4	"cliff, drop, drop-off
coral reef
geyser
lakeside, lakeshore
promontory, headland, head, foreland
"
4	"mask
matchstick
maypole
maze, labyrinth
"
4	"                         combined=True)
```

"
4	"        block_strings = []
        for block in blocks_args:
"
4	"television, television system
tennis ball
thatch, thatched roof
"
1	"    for key, data in dataset:
        if key == ""a"":
            assert tuple(data[""data8""]) == (0, 1, 2)
        if key == ""b"":
"
4	"waffle iron
wall clock
wallet, billfold, notecase, pocketbook
wardrobe, closet, press
"
4	"            out = postprocess(
                data_out=predictor_output[0].as_ndarray(),
"
0	"    # convenience routine for simplifying git command calls.
    return subprocess.check_output((""git"",) + args)


opts, args = getopt.getopt(sys.argv[1:], """", [])
"
4	"toaster
tobacco shop, tobacconist shop, tobacconist
toilet seat
torch
"
2	"    model.cuda()
    # If using GPU Adasum allreduce, scale learning rate by local_size.
    if args.use_adasum and hvd.nccl_built():
        lr_scaler = hvd.local_size()

"
2	"      login: true
  timeout_in_minutes: 5
  retry:
"
6	"                  return_type_repr: str) -> List[Tuple[List[str], str]]:
    out = []
    for type_repr in arg_types.values():
        if type_repr[:5] == 'Union':
            out.append(split_types_repr(type_repr[6:-1]))
"
6	"    jit = torch.jit.script(conv.jittable(t))
    assert jit(x1, edge_index).tolist() == out11.tolist()
"
4	"miniature schnauzer
giant schnauzer
standard schnauzer
Scotch terrier, Scottish terrier, Scottie
Tibetan terrier, chrysanthemum dog
"
0	"        btools.utils.circle(self.bm, segs=12, cap_tris=True)
        self.assertEquals(len(self.bm.faces), 12)
        self.assertEquals(len(self.bm.verts), 13)

    def test_cone(self):
"
0	"def profile():
    s = io.StringIO()
    pr = cProfile.Profile()

    pr.enable()
"
4	"washer, automatic washer, washing machine
water bottle
"
2	"            torch_model = hvd_spark.TorchModel(history=None,
                                               model=model,
                                               input_shapes=[[2]],
                                               feature_columns=['features'],
                                               label_columns=['y'],
"
2	"  - docker-compose#v2.6.0:
      run: test-mixed-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
"
2	"    else:
        if verbose >= 1:
            logging.info('Running %d processes...', num_proc)

    if min_np is None:
"
2	"        ]

        for ts in ts_list:
"
4	"chickadee
water ouzel, dipper
kite
bald eagle, American eagle, Haliaeetus leucocephalus
vulture
"
4	"        """"""
        Add the command input options.
        """"""
        self.arg_input_group.add_argument(
            '--input_path', type=str, help=""path to image."")
"
2	"        """"""Tests training starts with one host two slots, then a second host is added.""""""
        slots = {'host-1': 2}
"
4	"pole
police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria
poncho
pool table, billiard table, snooker table
pop bottle, soda bottle
"
0	"        faces = context.object.data.polygons
        self.assertEquals(len(faces), 1) # cap_tris False

        # -- check default size
        verts = context.object.data.vertices
"
5	"        if use_fc_solve_lib:
            fc_solve_lib_obj.input_cmd_line(args)
            status = fc_solve_lib_obj.solve_board(board)
        else:
            command = FCS_COMMAND+' '+' '.join(args)
"
2	"      run: test-cpu-openmpi-py3_6-tf1_14_0-keras2_2_4-torch1_2_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
"
1	"
class IterableESPnetDataset(IterableDataset):
    """"""Pytorch Dataset class for ESPNet.

"
4	"

def conv2d(input,
           num_filters=64,
           filter_size=7,
"
1	"
    def _target_mask(self, olens):
"
4	"African chameleon, Chamaeleo chamaeleon
Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis
African crocodile, Nile crocodile, Crocodylus niloticus
American alligator, Alligator mississipiensis
triceratops
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
"
4	"                initializer=fluid.initializer.TruncatedNormal(scale=0.02),
                trainable=trainable)
            dict_dim = 1256607
"
4	"import copy

import paddle.fluid as fluid
"
2	"      login: true
  timeout_in_minutes: 5
"
2	"    automatic: true
  agents:
    queue: 2x-gpu-g4
- label: ':tensorflow: Test TensorFlow MNIST (test-gpu-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
"
4	"shower curtain
ski
ski mask
sleeping bag
"
2	"  retry:
    automatic: true
  agents:
    queue: 4x-gpu-g4
- label: ':pytest: Run PyTests (test-gpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0)'
"
4	"        'efficientnet-b2': (1.1, 1.2, 260, 0.3),
        'efficientnet-b3': (1.2, 1.4, 300, 0.3),
        'efficientnet-b4': (1.4, 1.8, 380, 0.4),
        'efficientnet-b5': (1.6, 2.2, 456, 0.4),
        'efficientnet-b6': (1.8, 2.6, 528, 0.5),
"
2	"    Runs Horovod in Spark.  Runs `num_proc` processes executing `fn` using the same amount of Spark tasks.

"
4	"                    name='text_3',
                    shape=[-1, max_seq_len],
                    dtype='int64',
                    lod_level=0)
"
1	"@pytest.fixture
def shape_file(tmp_path):
"
4	"barometer
barrel, cask
barrow, garden cart, lawn cart, wheelbarrow
baseball
"
4	"                add_vars_prefix(startup_prog, name_prefix)
                global_vars = context_prog.global_block().vars
                inputs = {
                    key: global_vars[value]
                    for key, value in inputs.items()
"
2	"        self.assertIsNone(actual_rsh_settings.key)

"
4	"            use_gpu = True
        except:
            use_gpu = False
        if use_gpu:
            gpu_config = AnalysisConfig(self.default_pretrained_model_path)
"
1	"
@pytest.fixture
def char_tokenizer():
    return CharTokenizer(non_linguistic_symbols=[""[foo]""])
"
4	"from __future__ import absolute_import
from __future__ import division

import ast
"
4	"            input,
            param_attr=param_attr,
            bias_attr=bias_attr,
"
4	"        k = block_args.kernel_size
        s = block_args.stride
        if isinstance(s, list) or isinstance(s, tuple):
            s = s[0]
"
2	"        # Register task addresses of initial num_proc tasks
        _register_task_addresses(driver, settings)

        # Run the job
"
4	"        scale_param = fluid.ParamAttr(
            name=scale_name,
            initializer=fluid.initializer.Constant(1.0),
            trainable=True)
"
6	"    def __call__(self, data):
        assert data.edge_index is not None
"
1	"                ys_pad, self.sos, self.eos, self.ignore_id
            )
            ys_mask = target_mask(ys_in_pad, self.ignore_id)
"
4	"#
#    http://www.apache.org/licenses/LICENSE-2.0
#
"
4	"racket, racquet
radiator
radio, wireless
"
4	"
运行启动命令：
```shell
"
1	"                non_iterable_list.append((path, name, _type))
            else:
                self.path_name_type_list.append((path, name, _type))

"
2	"                             'model.named_parameters().')

        dups = _DistributedOptimizer.find_duplicates([k for k, _ in named_parameters])
        if len(dups) > 0:
"
4	"def gru_net(emb,
            seq_len,
"
1	"    def __init__(
        self,
        wshare,
        n_feat,
        dropout_rate,
"
6	"                sparse_sizes=(N, N)).t()

"
4	"siamang, Hylobates syndactylus, Symphalangus syndactylus
guenon, guenon monkey
"
2	"
@contextlib.contextmanager
def undo(fn):
"
1	"            logging.error(f'Error happened with path=""{path}"", id=""{k}"", value=""{v}""')
            raise
    return retval

"
6	"        loader = torch.utils.data.DataLoader(self, batch_size=200,
                                             collate_fn=lambda x: x,
                                             num_workers=self.num_workers)

"
4	"cup
eggnog
alp
bubble
"
6	"        outs = []
        for scaler in self.scalers:
            if scaler == 'identity':
                pass
            elif scaler == 'amplification':
"
2	"        # Merge in Google Trend for whole Germany.
        google_trend_de = google_trend_all[google_trend_all.file == 'Rossmann_DE']
        google_trend_de = google_trend_de.withColumnRenamed('trend', 'trend_de')
        df = df.join(google_trend_de, ['Year', 'Week']).select(df['*'], google_trend_de.trend_de)

"
2	"            # Gradients
            sync_bn_out.sum().backward()
            bn_out.mean(dim=0).sum().backward()
            assert (hvd.allreduce(sync_bn.weight.grad, name='sync_bn.weight.grad') - bn.weight.grad).abs().sum() < 1e-6
            assert (hvd.allreduce(sync_bn.bias.grad, name='sync_bn.bias.grad') - bn.bias.grad).abs().sum() < 1e-6
"
3	"});

$("".choose"").click(function () {
"
2	"    def _start_worker_process(self, slot_info):
        create_worker_fn = self._create_worker_fn
        shutdown_event = self._shutdown
"
2	"
        driver.start(np=2, create_worker_fn=exec_command)
        res = driver.get_results().worker_results
        driver.stop()
"
4	"                output_filters=round_filters(block_arg.output_filters,
                                             self._global_params),
"
4	"    if need_crop:
        conv = conv[:, :, 1:, 1:]

    if norm is not None:
"
1	"    model_class,
    args,
"
2	"@hvd.elastic.run
def run_benchmark(state):
    with tf.device(device):
        # Warm-up
        if not state.warm:
"
2	"
def on_event(event, func, args=(),
             stop=None, check_stop_interval_s=1.0,
             daemon=True, silent=False):
"
6	"    assert out2.size() == (2, 32)
    assert conv((x1, x2), edge_index, value, (4, 2)).tolist() == out1.tolist()
    assert conv((x1, x2), adj.t()).tolist() == out1.tolist()
    assert conv((x1, None), adj.t()).tolist() == out2.tolist()
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
"
4	"import os
import time
from collections import OrderedDict

import cv2
"
3	"    EXECUTOR = 1
    DRIVER = 2
    HANDLE = 3
    CALLBACK = 4

"
2	"        fn.assert_called_once_with(1, 2)

        fn = mock.Mock()
        thread = in_thread(fn, args=(1, 2), silent=True)
"
3	"        :param train_filepath: the training data file path, e.g ``faiss.tgz`` or `faiss.npy`. The data file is expected
            to be either `.npy` file from `numpy.save()` or a `.tgz` file from `NumpyIndexer`.


"
4	"        'efficientnet-b1': (1.0, 1.1, 240, 0.2),
        'efficientnet-b2': (1.1, 1.2, 260, 0.3),
        'efficientnet-b3': (1.2, 1.4, 300, 0.3),
"
2	"  agents:
    queue: 2x-gpu-g4
- label: ':factory: Elastic Spark TensorFlow Tests (test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c ""cd /horovod/test/integration && SPARK_HOME=/spark SPARK_DRIVER_MEM=512m HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format '[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s' --capture=no test_elastic_spark_tensorflow.py""
"
4	"birdhouse
boathouse
bobsled, bobsleigh, bob
"
4	"howler monkey, howler
titi, titi monkey
spider monkey, Ateles geoffroyi
"
1	"        raise ValueError(f""--dtype {args.dtype} does not support the CPU backend."")

"
3	"    position: relative;
    width: 32px;
    height: 32px;
"
4	"            gpu_config.enable_use_gpu(
                memory_pool_init_size_mb=1000, device_id=0)
            self.gpu_predictor = create_paddle_predictor(gpu_config)

    def context(self,
"
2	"        Raises a `HostsUpdatedInterrupt` if such a notification has been received.
        """"""
        # Iterate through the update messages sent from the server. If the update timestamp
"
2	"            log_state(state)

            current_hosts = epoch_to_hosts.get(state.epoch, default_hosts)
"
4	"            input_filters=int(options['i']),
            output_filters=int(options['o']),
            expand_ratio=int(options['e']),
            id_skip=('noskip' not in block_string),
"
3	"
#border #line.three {
    width: 5px;
"
4	"        ]
        if 0 < block.se_ratio <= 1:
            args.append('se%s' % block.se_ratio)
        if block.id_skip is False:
"
2	"with tf.Session(config=config) as session:
    session.run(tf.global_variables_initializer())
"
1	"    )
    parser.add_argument(""--debugmode"", type=int, default=1, help=""Debugmode"")
"
1	"            ""a"",
            ""o"",
"
4	"beacon, lighthouse, beacon light, pharos
beaker
"
4	"                    name='text_2',
                    shape=[-1, max_seq_len],
                    dtype='int64',
"
1	"ldconv_dconv_args = dict(
    transformer_decoder_selfattn_layer_type=""dynamicconv"",
    transformer_encoder_selfattn_layer_type=""dynamicconv"",
    wshare=4,
"
1	"        dtype = float
    else:
        raise ValueError(f""Not supported loader_type={loader_type}"")

    # path looks like:
"
2	"      run: test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
1	"                distributed=distributed_option.distributed,
                name=""valid"",
"
6	"    adj = adj.sparse_resize((4, 2))
    out1 = conv((x1, x2), edge_index, value)
    out2 = conv((x1, None), edge_index, value, (4, 2))
    assert out1.size() == (2, 32)
"
4	"                outputs = {
                    'classification': name_prefix + output.name,
"
2	"                print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                    state.epoch, state.batch * len(data), len(train_sampler),
"
4	"# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"
2	"    Keras Callback that will commit the `state` object every `batches_per_commit`
    batches at the end of each batch.
    """"""
"
2	"- label: ':jupyter: Run PyTests test_interactiverun (test-mixed-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""cd /horovod/test && pytest -v --capture=no test_interactiverun.py""
  plugins:
  - docker-compose#v2.6.0:
"
1	"    positionwise_layer_type,
    reduction_factor,
    spk_embed_dim,
    spk_embed_integration_type,
    loss_type,
"
4	"planetarium
plastic bag
plate rack
plow, plough
"
4	"airliner
airship, dirigible
altar
ambulance
amphibian, amphibious vehicle
"
3	"    return approved[s]


def check_image_type(s):
    allowed = {'pod', 'flow', 'app'}
"
2	"        warnings.simplefilter('module')

"
4	"Indian cobra, Naja naja
green mamba
sea snake
horned viper, cerastes, sand viper, horned asp, Cerastes cornutus
"
2	"        exit_schedule = {
            str((1, 0)): [0],
        }

"
4	"# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
"
2	"        self.assertIsNone(actual_settings.key)

        # the settings for the rsh agent should not contain the key
"
1	"                ),
                torch.nn.Linear(eprenet_conv_chans, adim),
            )
        else:
"
2	"  plugins:
  - docker-compose#6b0df8a98ff97f42f4944dbb745b5b8cbf04b78c:
"
4	"                pretrained=True,
                override_params=None,
                phase='train'):
"
4	"        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
        use_se=use_se)
    return model
"
4	"    return int(math.ceil(multiplier * repeats))


"
1	"    echo """"
    echo ""Finished""
fi

"
4	"gorilla, Gorilla gorilla
chimpanzee, chimp, Pan troglodytes
gibbon, Hylobates lar
siamang, Hylobates syndactylus, Symphalangus syndactylus
"
1	"        else:
            xs = self.embed(xs)

        new_cache_sd = []
"
3	"            return self._raw_ndarray

"
2	"
# Benchmark settings
"
2	"
    @mock.patch('horovod.run.elastic.driver.ELASTIC_TIMEOUT_SECS', 1)
    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
"
2	"    delete[] entry_component_offsets;
    delete[] recvcounts;
    delete[] displcmnts;

"
4	"        return inputs, outputs, context_prog

    def classify(self,
                 images=None,
"
2	"
    # Create Spark session for data preparation.
    conf = SparkConf().setAppName('data_prep').set('spark.sql.shuffle.partitions', '16')
    if args.processing_master:
"
4	"starfish, sea star
sea urchin
sea cucumber, holothurian
"
4	"trolleybus, trolley coach, trackless trolley
trombone
tub, vat
turnstile
typewriter keyboard
"
6	"
    def message(self, x_i: Tensor, x_j: Tensor,
                edge_attr: OptTensor) -> Tensor:

"
2	"        if instance in self._workers:
            self._workers.remove(instance)

    def stop_master(self):
"
4	"cock
hen
ostrich, Struthio camelus
brambling, Fringilla montifringilla
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
"
4	"def efficientnet_params(model_name):
    """""" Map EfficientNet model name to parameter coefficients. """"""
    params_dict = {
        # Coefficients:   width,depth,resolution,dropout
        'efficientnet-b0': (1.0, 1.0, 224, 0.2),
"
2	"
    for step in range(1, num_steps + 1):
        features = torch.Tensor(np.random.rand(1) * 2 * args.x_max - args.x_max)
"
0	"
class FaceMapMaterial(bpy.types.PropertyGroup):
    """""" Tracks materials for each facemap created for an object
    """"""
"
4	"
def init_batch_norm_layer(name=""batch_norm""):
    param_attr = fluid.ParamAttr(
        name=name + '_scale', initializer=fluid.initializer.Constant(1.0))
"
4	"            ""../image_dataset/keypoint_detection/girl2.jpg""
        ]
        self.true_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3).tolist()
"
0	"

@contextmanager
def bmesh_from_active_object(context=None):
"
2	"            state.batch = 21
            state.epoch = 11

            state.restore()

"
1	"            else:
                self.num_heads_applied_guided_attn = num_heads_applied_guided_attn
            self.modules_applied_guided_attn = modules_applied_guided_attn
        if self.spk_embed_dim is not None:
            self.spk_embed_integration_type = spk_embed_integration_type
"
6	"            else:
                raise ValueError(f'Unknown scaler ""{scaler}"".')
            outs.append(out)
        return torch.cat(outs, dim=-1)
"
6	"
            if hasattr(data, '__num_nodes__'):
                num_nodes_list.append(data.__num_nodes__)
            else:
"
2	"
        callbacks = [
            # Horovod: broadcast initial variable states from rank 0 to all other processes.
            # This is necessary to ensure consistent initialization of all workers when
            # training is started with random weights or restored from a checkpoint.
"
2	"                    if earlier_host_hash != req.host_hash:
                        self._task_host_hash_indices[earlier_host_hash].remove(req.index)

"
2	"SPARK_CONF_DONT_REUSE_NODE_FOR_SAME_TASK = ('spark.blacklist.task.maxTaskAttemptsPerNode', '1')

"
4	"tree frog, tree-frog
tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui
loggerhead, loggerhead turtle, Caretta caretta
leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea
mud turtle
"
4	"        tmp = fluid.layers.elementwise_add(tmp, offset, axis=1)
        return tmp
    else:
        raise NotImplementedError(""norm tyoe: [%s] is not support"" % norm_type)

"
4	"altar
ambulance
amphibian, amphibious vehicle
analog clock
"
6	"    t = '(PairOptTensor, PairTensor, SparseTensor) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert torch.allclose(jit((x1, None), (pos1, pos2), adj.t()), out,
                          atol=1e-6)
"
3	"    """"""
    :class:`UniversalSentenceEncoder` is a encoder based on the Universal Sentence
"
1	"                    if idx + 1 == self.num_layers_applied_guided_attn:
                        break
                att_ws = torch.cat(att_ws, dim=1)  # (B, H*L, T_in, T_in)
"
4	"bulbul
jay
magpie
chickadee
water ouzel, dipper
"
2	"# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"
6	"                    self._cached_x = x
        else:
"
1	"                for hyp in hyps:
                    logging.debug(
"
4	"                   padding_type='SAME',
                   override_params=None,
                   use_se=True):
    model = EfficientNet(
"
3	"    if s not in allowed:
        raise ValueError(f'type {s} is not allowed, should be one of {allowed}')


"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
"
4	"                padding_idx=dict_dim - 1,
                dtype='float32',
"
4	"            name=name + '_weights', initializer=fluid.initializer.Constant(1.0))
        bias_attr = fluid.ParamAttr(
            name=name + '_offset',
"
4	"        dtype = helper.input_dtype()
        epsilon = 1e-5
        mean = fluid.layers.reduce_mean(input, dim=[2, 3], keep_dim=True)
        var = fluid.layers.reduce_mean(
"
4	"hay
carbonara
"
4	"```

返回预处理的图片标准差，也就是 \[0.229, 0.224, 0.225\]。


"
1	"            self.reporter.report(
                loss_ctc_data, loss_att_data, self.acc, cer_ctc, cer, wer, loss_data
            )
"
0	"        tag_remove_indices = all_indices - used_indices

        # -- remove face maps
"
1	"
        # fill missing arguments for compatibility
        args = fill_missing_args(args, self.add_arguments)

"
2	"    def test_run_with_jsrun(self, mocked_run_controller):
        hargs = HorovodArgs()
        _run(hargs)
"
6	"                          atol=1e-6)

"
4	"        use_se=use_se)
    return model

"
2	"
        # Merge in store information.
        store = store_csv.join(store_states_csv, 'Store')
"
6	"    assert conv.__repr__() == 'NNConv(8, 32)'
    out = conv(x1, edge_index, value)
"
2	"# executors where any task fails due to exceptions can potentially be reused for any other task,
# including the task itself, unless SPARK_CONF_DONT_REUSE_EXECUTOR_FOR_SAME_TASK is set
# executors lost e.g. due to node failure can't be reused in any way
# NOTE: in dynamic allocation, only executors blacklisted for the entire app
"
4	"                      padding_type=""SAME"",
                      conv_act=None,
                      bn_act='swish',
"
1	"\data\
ngram 1=9
"
1	"    w = NpyScpWriter(tmp_path / ""data"", p)
    w[""a""] = np.random.randn(100, 80)
    w[""b""] = np.random.randn(150, 80)
"
2	"  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/keras_spark_rossmann_run.py --num-proc 2 --data-dir file:///data --epochs 3 --sample-rate 0.01""
  plugins:
  - docker-compose#v2.6.0:
"
1	"from espnet.nets.e2e_asr_common import end_detect
from espnet.nets.pytorch_backend.ctc import CTC
from espnet.nets.pytorch_backend.e2e_asr import CTC_LOSS_THRESHOLD
"
4	"Norwegian elkhound, elkhound
otterhound, otter hound
Saluki, gazelle hound
Scottish deerhound, deerhound
"
2	"    """"""Creates `settings.num_proc` Spark tasks in a parallel thread.""""""
    def run_spark():
        """"""Creates `settings.num_proc` Spark tasks, each executing `_task_fn` and waits for them to terminate.""""""
"
2	"
        results = self._run(discovery_schedule=discovery_schedule, np=3, min_np=1, max_np=4,
                            # TODO: remove these waits when discovery publishes failure right-away
                            #       currently, spark discovery does not know about failing nodes
"
1	"        output_activation (str, optional): The name of activation function for outputs.
        adim (int, optional): The number of dimension of mlp in attention.
"
1	"    for x in train test; do
        utils/fix_data_dir.sh data/${x}
    done

    # compute global CMVN
"
1	"    for dataset in [""data/train/"", ""data/test/""]:
        with open(dataset + ""text"", ""r"") as t:
            txts = t.readlines()
        with open(dataset + ""feats.scp"", ""r"") as s:
"
4	"    bias_attr = fluid.ParamAttr(
        name=name + '_offset',
        initializer=fluid.initializer.Constant(value=0.0))
"
4	"mailbox, letter box
maillot
maillot, tank suit
manhole cover
maraca
"
2	"  plugins:
  - docker-compose#v2.6.0:
"
4	"abaya
academic gown, academic robe, judge's robe
accordion, piano accordion, squeeze box
"
2	"        message = 'Horovod detected that one or more processes exited with non-zero status'
        with pytest.raises(RuntimeError, match=message):
            self._run(discovery_schedule, exit_schedule=exit_schedule)
"
2	"    finally:
        cluster.shutdown()


class SparkClusterController(object):
"
3	"
        f = (Flow().add(name='r1', yaml_path='!BaseDocCrafter')
"
1	"            loss_att_data = None
            loss_ctc_data = float(loss_ctc)
"
4	"def EfficientNetB6(is_test=False,
                   padding_type='SAME',
                   override_params=None,
"
2	"    Every object is specified as a keyword argument, and will be assigned as an attribute.

"
1	"                    distributed=False,
                    name=""plot_att"",
                    **common_iter_kwargs,
"
2	"            #
            # Note: This callback must be in the list before the ReduceLROnPlateau,
            # TensorBoard, or other metrics-based callbacks.
"
3	"            end = time.time()
            durations[index] = (end - start)
            status_codes[index] = req.status.code

"
6	"        i, u = i_and_u(pred, data.y, loader.dataset.num_classes, data.batch)
        iou = i.cpu().to(torch.float) / u.cpu().to(torch.float)
"
4	"steel drum
stethoscope
stole
stone wall
stopwatch, stop watch
"
3	"        with g:
            g.search(random_docs(10), output_fn=lambda x: validate(x, 'NumpyIndexer'))
"
6	"        pre_filter (callable, optional): A function that takes in an
            :obj:`torch_geometric.data.Data` object and returns a boolean
            value, indicating whether the data object should be included in the
"
4	"shopping basket
shopping cart
shovel
"
4	"# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
"
2	"
        # add injected HOROVOD_SPARK_PYTHONPATH to sys.path
        for p in reversed(ppath.split(os.pathsep)):
            sys.path.insert(1, p)  # don't put it in front which is usually .
"
4	"        elif phase in [""train""]:
            is_test = False
        else:
            raise ValueError(
                ""Phase %s is error, which must be one of train, dev, test, eval and predict.""
"
4	"    """""" Get the block args and global params for a given model """"""
    if model_name.startswith('efficientnet'):
        w, d, _, p = efficientnet_params(model_name)
"
4	"mountain tent
mouse, computer mouse
mousetrap
moving van
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: 4x-gpu-g4
"
2	"    model.summary()

    # Horovod: add Distributed Optimizer.
"
4	"                      padding_type=""SAME"",
                      conv_act=None,
"
4	"        block_size = 0
        for block_arg in block_args_copy:
            block_arg = block_arg._replace(
"
6	"    value = torch.rand(row.size(0))
    adj2 = SparseTensor(row=row, col=col, value=value, sparse_sizes=(4, 4))
    adj1 = adj2.set_value(None)

    conv = DNAConv(32, heads=4, groups=8, dropout=0.0)
"
4	"
        conv = self.conv_bn_layer(
            inputs,
            num_filters=oup,
            filter_size=k,
"
1	"
def test_repr(word_tokenizer: WordTokenizer):
"
1	"align_dir=align
api=v1

# download related
models=tedlium2.transformer.v1
"
4	"# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
"
1	"@pytest.mark.parametrize(""prenet_layers"", [0, 1])
@pytest.mark.parametrize(""postnet_layers"", [0, 1])
@pytest.mark.parametrize(""reduction_factor"", [1, 2, 3])
@pytest.mark.parametrize(
    ""spk_embed_dim, spk_embed_integration_type"",
"
4	"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"
4	"                input_filters=round_filters(block_arg.input_filters,
                                            self._global_params),
                output_filters=round_filters(block_arg.output_filters,
                                             self._global_params),
"
1	"import numpy as np
import pytest
import soundfile
import torch

"
4	"soft-coated wheaten terrier
West Highland white terrier
"
4	"        input=input,
        op_type='conv',
"
2	"        average:
            .. warning:: .. deprecated:: 0.19.0
"
2	"                                'HOROVOD_GLOO_RENDEZVOUS_PORT=[0-9]+ '
                                'HOROVOD_CONTROLLER=gloo '
                                'HOROVOD_CPU_OPERATIONS=gloo '
"
2	"        )

"
4	"bittern
crane
limpkin, Aramus pictus
"
2	"
    def _activate_workers(self, min_np):
        logging.info('wait for available slots: {}'.format(min_np))
        current_hosts = self.wait_for_available_slots(min_np)
"
2	"
    print('================')
    print('Data preparation')
    print('================')

"
4	"kit fox, Vulpes macrotis
Arctic fox, white fox, Alopex lagopus
grey fox, gray fox, Urocyon cinereoargenteus
"
4	"            input=conv, pool_type='avg', global_pooling=True, use_cudnn=False)

        if not is_test and self._global_params.dropout_rate:
"
1	"        f.write(""def /some/path/b.wav\n"")
    d = read_2column_text(p)
    assert d == {""abc"": ""/some/path/a.wav"", ""def"": ""/some/path/b.wav""}
"
4	"    def _depthwise_conv_norm(self, inputs, block_args, is_test, name=None):
        k = block_args.kernel_size
"
4	"crane
limpkin, Aramus pictus
"
1	"        key4 /some/path/d.npy
        ...
"
6	"    assert conv.__repr__() == 'CGConv(8, dim=3)'
    out = conv(x1, edge_index, value)
"
2	"from horovod.spark.task import task_service
from horovod.spark.driver import driver_service


"
2	"            # Child process will exit when pipe breaks
            child.wait(timeout=2 * safe_shell_exec.GRACEFUL_TERMINATION_TIME_S + 1)

            self.assertFalse(parent.is_running())
"
3	"import re
import unicodedata

from pkg_resources import resource_stream

"
4	"        assert (('s' in options and len(options['s']) == 1) or
                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))

"
4	"        res = list()
        for iter_id in range(loop_num):
"
2	"        raise HorovodInternalError(e)

"
5	"        layout.createText(s.talon, ""s"")
        x = x + layout.XS
"
2	"
### Changed

"
2	"        class NotificationReceiver:
            def __init__(self):
                self.events = []

"
1	"        # dynamic conv related
        self.use_bias = use_bias
"
2	"            for name, p in param_state.items():
                # Some parameter names may appear more than once, in which
                # case we ensure they have a unique identifier defined by
                # their order
"
4	"    divisor = global_params.depth_divisor
    min_depth = global_params.min_depth
    filters *= multiplier
    min_depth = min_depth or divisor
    new_filters = max(min_depth,
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
4	"Greater Swiss Mountain dog
Bernese mountain dog
Appenzeller
"
2	"    return _exec_command


"
4	"        conv = self.conv_bn_layer(
            inputs,
            num_filters=oup,
"
4	"
**NOTE:** 如使用GPU预测，则需要在启动服务之前，请设置CUDA\_VISIBLE\_DEVICES环境变量，否则不用设置。

## 第二步：发送预测请求

"
4	"    """"""
    Preprocess to yield image.
"
2	"        self._results.expect(thread)

"
0	"
    res = []
    col_w = 2 * prop.slab_outset
    pos_h = prop.floor_height / 2 + (prop.slab_thickness if prop.add_slab else 0)
"
1	"
    # display PYTHONPATH
    logging.info(""python path = "" + os.environ.get(""PYTHONPATH"", ""(None)""))

    # seed setting
"
1	"    else
        lmdatadir=data/local/lm_train
"
4	"                   use_se=True):
    model = EfficientNet(
        name='b6',
"
4	"half track
hammer
hamper
"
4	"cup
eggnog
alp
bubble
"
4	"#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
"
4	"sandal
sarong
sax, saxophone
scabbard
"
4	"import paddlehub as hub


"
2	"        sleep = interrupt_delay + safe_shell_exec.GRACEFUL_TERMINATION_TIME_S + 2.0
        start = time.time()
        self.do_test_safe_shell_exec('sleep {}'.format(sleep), 143, '', None, interrupt)
"
2	"        handle = allreduce_async_(tensor_compressed.data, name=name, op=Adasum)

        # reset stashed parameters
        for stashed, group in zip(stashed_params, self.param_groups):
"
4	"        n = filter_size * filter_size * fan_out
        param_attr = fluid.ParamAttr(
"
3	"    @property
    def compiled_if(self):
        """"""Get the bytecode of the if expression, faster for :func:`eval` """"""
        if self._compiled_if:
            return self._compiled_if
"
6	"            self.comp = Parameter(torch.Tensor(num_relations, num_bases))

"
2	"
SPARK_CONF_MAX_INT = '2147483647'
"
4	"bagel, beigel
pretzel
cheeseburger
hotdog, hot dog, red hot
mashed potato
"
4	"        w_start = (width - size) / 2
        h_start = (height - size) / 2
    else:
"
6	"import os
import os.path as osp
import pickle
"
6	"
        # propagate_type: (x: OptPairTensor, edge_weight: OptTensor)
        out = self.propagate(edge_index, x=x, edge_weight=edge_weight,
"
2	"# Horovod: initialize library.
hvd.init()
torch.manual_seed(args.seed)

"
4	"# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"
4	"miniature pinscher
Greater Swiss Mountain dog
"
4	"        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
        use_se=use_se)
    return model
"
4	"```

## API

```python
"
2	"        hosts = 'host-1:1,host-2:2,host-3:3'

"
1	"            help=""Specify g2p method if --token_type=phn"",
        )

"
4	"        API for image classification.

        Args:
            images (list[numpy.ndarray]): data of images, shape of each is [H, W, C], color space must be BGR.
"
2	"#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
"
4	"mouse, computer mouse
mousetrap
moving van
muzzle
"
3	"gpl-2.0: GNU General Public License v2.0
gpl-3.0: GNU General Public License v3.0
lgpl: GNU Lesser General Public License family
lgpl-2.1: GNU Lesser General Public License v2.1
"
1	"                new_data[k] = np.array([1])
            else:
                new_data[k] = np.array([2])
"
4	"```

**参数**

"
4	"
    def test_common_apis(self):
"
2	"      login: true
  timeout_in_minutes: 5
"
6	"            if item.size(0) == E:
                data[key] = item[edge_idx]
"
2	"
        hargs = HorovodArgs()
"
2	"  command: bash -c "" \$(cat /mpirun_command) python /horovod/examples/pytorch_mnist.py""
  plugins:
  - docker-compose#v2.6.0:
"
6	"                 edge_weight: OptTensor, normalization: Optional[str],
                 lambda_max, dtype: Optional[int] = None,
                 batch: OptTensor = None):
"
2	"

def run(func):
    """"""Decorator used to run the elastic training process.
"
3	"cc-by-sa-4.0: Creative Commons Attribution Share Alike 4.0
wtfpl: Do What The F*ck You Want To Public License
"
2	"    delete[] recvcounts;
    delete[] displcmnts;

"
1	"        vy = h.new_zeros(1).long()

        if recog_args.maxlenratio == 0:
            maxlen = h.shape[0]
        else:
"
2	"        else:
            return self._maybe_run_sync_bn(input)

"
4	"        conv = conv
    else:
        raise NotImplementedError(""activation: [%s] is not support"" % act)
"
4	"                        context_prog,
                        predicate=_if_exist)
                else:
                    exe.run(startup_prog)
                # trainable
"
2	"        hosts = 'host-1:2,host-2:2'

        exit_schedule = {
            str((1, 0)): [1],
"
4	"        for index, res in enumerate(results_1):
            self.assertTrue(res.keys(), results_2[index].keys())
            diff = list(res.values())[0] - list(results_2[index].values())[0]
            self.assertTrue((diff < 1e-5))

"
4	"switch, electric switch, electrical switch
syringe
table lamp
tank, army tank, armored combat vehicle, armoured combat vehicle
"
1	"import argparse
from collections import Counter
"
4	"#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
"
6	"    value = torch.rand(row.size(0), 3)
    adj = SparseTensor(row=row, col=col, value=value, sparse_sizes=(4, 4))
"
4	"        filter_size,
        groups=groups,
        name=name,
        stride=stride,
        padding=padding,
"
4	"    return model

"
2	"        assert current_hosts.count_available_slots() == 4

        # Now blacklist, our existing object should not change (immutable)
        host_manager.blacklist('a')
        assert current_hosts.available_hosts == {'a', 'b'}
"
4	"    def test_classifcation(self):
        results_1 = self.module.classify(paths=self.test_images, use_gpu=True)
"
2	"#
#     http://www.apache.org/licenses/LICENSE-2.0
#
"
1	"    del pyopenjtalk
except ImportError:
    pass
try:
    import pypinyin
"
2	"    def __init__(self, timestamp):
        self.timestamp = timestamp

"
4	"    elif act == 'tanh':
        conv = fluid.layers.tanh(conv, name=name + '_tanh')
    elif act == 'sigmoid':
        conv = fluid.layers.sigmoid(conv, name=name + '_sigmoid')
    elif act == 'swish':
"
0	"    yield
    pr.disable()

    ps = pstats.Stats(pr, stream=s)
"
2	"import pyspark

from horovod.common.util import gloo_built
from horovod.run.util.threads import in_thread
from horovod.spark.common.util import host_hash
"
4	"        """"""
        self.arg_config_group.add_argument(
            '--use_gpu',
            type=ast.literal_eval,
"
4	"great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias
tiger shark, Galeocerdo cuvieri
hammerhead, hammerhead shark
"
4	"    print(res)
    res = b6.classification(images=test_image)
    print(res)
"
3	"            f.index_lines(lines=['abbcs', 'efgh'], output_fn=validate)

    def test_bad_flow_customized(self):
        def validate(req):
            self.assertEqual(req.status.code, jina_pb2.Status.ERROR)
"
2	"    for handle in handles:
        synchronize(handle)
"
2	"        raise Exception('Illegal host hash provided. Are you using Open MPI 4.0.0+?')

    driver_client = driver_service.SparkDriverClient(driver_addresses, key, verbose=verbose)
"
6	"        '  (2): Linear(in_features=16, out_features=32, bias=True)\n'
        '), k=2)')
    out11 = conv(x1)
    assert out11.size() == (8, 32)
"
4	"dumbbell
Dutch oven
electric fan, blower
electric guitar
electric locomotive
"
4	"    def get_expected_image_width(self):
        return 224

    def get_expected_image_height(self):
"
4	"
```python
import paddlehub as hub
"
2	"        expected = '^Horovod detected that one or more processes exited with non-zero ' \
                   'status, thus causing the job to be terminated. The first process ' \
                   'to do so was:\nProcess name: 0\nExit code: 1$'
        self.do_test_spark_run_with_non_zero_exit(use_mpi=False, use_gloo=True,
"
2	"            if self._allreduce_delay[p] == 0:
                handle, ctx = self._allreduce_grad_async(p)
            self._handles[p] = (handle, ctx)
        return hook

"
4	"            self.directory, ""efficientnetb3_imagenet_infer_model"")
        label_file = os.path.join(self.directory, ""label_list.txt"")
        with open(label_file, 'r', encoding='utf-8') as file:
            self.label_list = file.read().split(""\n"")[:-1]
"
2	"        """"""
        Spark blacklisting will avoid restarting a failing task on the same executor.
        Since there are no more executors, the Horovod cluster will scale down.
        In test_auto_scale_no_spark_black_list we test the behaviour without blacklisting.
"
1	"

def test_repr(char_tokenizer: CharTokenizer):
    print(char_tokenizer)
"
2	"import platform
import queue
import time

"
4	"three-toed sloth, ai, Bradypus tridactylus
orangutan, orang, orangutang, Pongo pygmaeus
gorilla, Gorilla gorilla
chimpanzee, chimp, Pan troglodytes
gibbon, Hylobates lar
"
4	"dhole, Cuon alpinus
African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus
hyena, hyaena
red fox, Vulpes vulpes
kit fox, Vulpes macrotis
"
4	"                # trainable
                for param in context_prog.global_block().iter_parameters():
                    param.trainable = trainable
        return inputs, outputs, context_prog
"
4	"rocking chair, rocker
rotisserie
rubber eraser, rubber, pencil eraser
rugby ball
rule, ruler
"
2	"                               'https://github.com/horovod/horovod/issues/1995')

"
4	"restaurant, eating house, eating place, eatery
revolver, six-gun, six-shooter
rifle
rocking chair, rocker
rotisserie
"
2	"    """"""
    Test that horovod.spark.run times out when it does not start up fast enough using Gloo.
    """"""
"
1	"
@pytest.fixture(params=[None, "" ""])
def word_tokenizer(request):
    return WordTokenizer(delimiter=request.param)

"
4	"mountain bike, all-terrain bike, off-roader
mountain tent
mouse, computer mouse
mousetrap
moving van
"
2	"    In this version, normalization parameters are synchronized across workers during forward pass.
    This is very useful in situations where each GPU can fit a very small number of examples.

    See https://pytorch.org/docs/stable/nn.html#batchnorm2d for more details about BatchNorm.

"
2	"        df = df.join(weather, ['State', 'Date'])

        # Fix null values.
        df = df \
"
4	"abacus
abaya
academic gown, academic robe, judge's robe
"
6	"from typing import Tuple, Union
from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size
"
1	"        )
        self.weight_f = nn.Parameter(
            torch.Tensor(1, 1, self.kernel_size).uniform_(0, 1)
        )
"
2	"        self._worker_clients[(host, slot)] = WorkerNotificationClient(
            addresses, secret_key, self._verbose)

"
2	"            thread.join(sleep + 1.0)
            self.assertFalse(thread.is_alive(), 'thread should have terminated by now')
        duration = time.time() - start
        print('concurrent requests completed in {} seconds'.format(duration))

"
4	"                batch_image
            ]) if use_gpu else self.cpu_predictor.run([batch_image])
"
2	"    queue: cpu
- label: ':tensorflow: Single Keras MNIST (test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""\$(cat /oneccl_env) && echo '/mpirun_command_ofi' > /mpirun_command && python /horovod/examples/keras_mnist_advanced.py --epochs 3 --batch-size 64""
  plugins:
  - docker-compose#v2.6.0:
"
4	"                                      '_blocks.' + str(idx) + '.')

            idx += 1
"
1	"                num_cache_chunks=args.num_cache_chunks,
            )

"
6	"        For an example of using a pretrained DimeNet variant, see
        `examples/qm9_pretrained_dimenet.py
"
4	"        padding_type=padding_type,
        override_params=override_params,
        use_se=use_se)
    return model
"
4	"            blocks_args.append(BlockDecoder._decode_block_string(block_string))
        return blocks_args

    @staticmethod
"
4	"# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
"
3	"const urlParams = new URLSearchParams(window.location.search);
const myParam = urlParams.get('jh');
"
4	"sea cucumber, holothurian
wood rabbit, cottontail, cottontail rabbit
hare
Angora, Angora rabbit
"
5	"        layout.createText(s.foundations[0], ""s"")
        x, y = layout.XM + 4*w, self.height - layout.YS
"
1	"        ],
        default=None,
"
4	"doormat, welcome mat
drilling platform, offshore rig
drum, membranophone, tympan
"
2	"# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
"
4	"panpipe, pandean pipe, syrinx
paper towel
parachute, chute
"
6	"
    t = '(Tensor, Tensor, OptTensor, Size) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert jit(x1, edge_index, value).tolist() == out.tolist()
    assert jit(x1, edge_index, value, size=(4, 4)).tolist() == out.tolist()
"
6	"    assert out2.size() == (4, 32)
    assert torch.allclose(conv(x, adj2.t()), out2, atol=1e-6)

"
2	"            env[var] = os.environ[var]


"
4	"toyshop
tractor
"
2	"                  Note: settings.num_proc and settings.hosts must not be None.
        path: Optional path of the rankfile.
              Note: this file will be overwritten.
"
4	"            emb_1 = fluid.layers.embedding(
                input=text_1,
                size=[dict_dim, 128],
"
1	"
    def forward(self, query, key, value, mask):
"
4	"crutch
cuirass
dam, dike, dyke
desk
"
4	"volleyball
waffle iron
wall clock
"
2	"    # If there's no exception, execution results are in this queue.
    results = result_queue.get_nowait()
    return [results[index] for index in indices_in_rank_order]

"
2	"        for w in state.model.state_dict().values():
            np.testing.assert_allclose(w, np.ones_like(w))
        assert state.batch == 20
        assert state.epoch == 10

"
2	"                    os.chdir(test_dir)
                    sys.path = test_sys_path

                if work_dir_env_set:
"
2	"    Tests gloo_run with minimal settings.
    """"""
    def test_gloo_run_minimal(self):
        if not gloo_built:
"
4	"```

"
4	"        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
        use_se=use_se)
"
4	"grille, radiator grille
grocery store, grocery, food market, market
guillotine
hair slide
hair spray
"
4	"                name=name + '_offset',
                initializer=fluid.initializer.Uniform(low=-bound, high=bound))
        else:
"
2	"    queue: cpu
- label: ':docker: Build test-gpu-openmpi-py3_6-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_6_0-pyspark2_4_0'
  plugins:
  - docker-compose#6b0df8a98ff97f42f4944dbb745b5b8cbf04b78c:
      build: test-gpu-openmpi-py3_6-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_6_0-pyspark2_4_0
"
2	"
                cmd = ' '.join(command)
                run_elastic(self._exec, (cmd,), env={'HOROVOD_LOG_LEVEL': 'DEBUG'},
"
4	"steam locomotive
steel arch bridge
steel drum
stethoscope
"
3	"    def get_add_handler(self):
        """"""Open a binary gzip file for adding new vectors

        :return: a gzip file stream
"
4	"fire screen, fireguard
flagpole, flagstaff
flute, transverse flute
"
4	"        if not is_test and self._global_params.dropout_rate:
            pool = fluid.layers.dropout(
                pool,
                self._global_params.dropout_rate,
                dropout_implementation='upscale_in_train')
"
4	"Egyptian cat
cougar, puma, catamount, mountain lion, painter, panther, Felis concolor
lynx, catamount
leopard, Panthera pardus
snow leopard, ounce, Panthera uncia
"
4	"        if use_bn == False:
            return conv
        else:
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: 2x-gpu-g4
"
3	"    def build_advanced_index(self, vecs: 'np.ndarray'):
        from annoy import AnnoyIndex
"
3	"def set_hub_build_parser(parser=None):
    if not parser:
"
1	"        text2token.py -s 1 -n 1 -l ${nlsyms} data/${train_set}/text \
            | cut -f 2- -d"" "" > ${lmdatadir}/train_trans.txt
        text2token.py -s 1 -n 1 -l ${nlsyms} data/${train_dev}/text \
            | cut -f 2- -d"" "" > ${lmdatadir}/valid.txt
"
2	"# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
"
4	"def initial_type(name,
                 input,
                 op_type,
                 fan_out,
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
"
2	"

class NetworkTests(unittest.TestCase):
    """"""
"
2	"        return df


    def prepare_df(df):
"
4	"hatchet
holster
"
2	"            grad_bias = None

        return grad_input, grad_weight, grad_bias, None, None, None, None, None, None
"
2	"                                 for _, v in sorted(named_parameters)}
        self._handles = {}
        self._grad_accs = []
"
1	"        weight_new = weight_new.narrow(-1, int((k - 1) / 2), T)  # B x H x T x T(k)
        if self.use_kernel_mask:
            kernel_mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0)
"
4	"tarantula
wolf spider, hunting spider
tick
centipede
"
4	"            except:
                raise RuntimeError(
                    ""Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES as cuda_device_id.""
                )

"
2	"                                        use_mpi=use_mpi, use_gloo=use_gloo,
                                        verbose=2)
                self.assertListEqual([([0, 1], 0), ([0, 1], 1)], res)

    """"""
"
2	"      run: test-cpu-gloo-py3_7-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
"
2	"        with pytest.raises(RuntimeError, match=message):
            self._run(hosts=hosts, exit_schedule=exit_schedule,
                      extra_conf=[conf.SPARK_CONF_ALWAYS_RESTART_FAILED_TASK,
"
2	"
class LSFUtils:
    """"""LSF Utilities""""""
    _CSM_ALLOCATION_QUERY = ""/opt/ibm/csm/bin/csm_allocation_query""
"
4	"

"
6	"    jit = torch.jit.script(conv.jittable(t))
    assert torch.allclose(conv(x, adj1.t()), out1, atol=1e-6)
    assert torch.allclose(conv(x, adj2.t()), out2, atol=1e-6)

"
6	"        batch_size (int): The approximate number of samples per batch.
        num_steps (int, optional): The number of iterations per epoch.
"
4	"crash helmet
crate
crib, cot
Crock Pot
"
2	"            state.commit()

        # Benchmark
"
4	"#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
"
4	"                block_size += 1

"
4	"pitcher, ewer
plane, carpenter's plane, woodworking plane
planetarium
plastic bag
plate rack
"
4	"curly-coated retriever
golden retriever
Labrador retriever
"
4	"        cv2.imread(
            '/mnt/zhangxuefei/program-paddle/PaddleHub/hub_module/tests/image_dataset/classification/animals/dog.jpeg'
        )
    ]
    res = b1.classification(images=test_image)
"
1	"                        conv_wshare,
                        attention_dim,
"
4	"        self.add_module_config_arg()
        self.add_module_input_arg()
        args = self.parser.parse_args(argvs)
        results = self.classify(
            paths=[args.input_path],
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
"
4	"    if paths:
        for im_path in paths:
            each = OrderedDict()
            assert os.path.isfile(
                im_path), ""The {} isn't a valid file path."".format(im_path)
"
4	"microphone, mike
microwave, microwave oven
"
2	"            self.assertEqual(expected_stdout, stdout.getvalue())
        if expected_stderr is not None:
            self.assertEqual(expected_stderr, stderr.getvalue())
"
6	"from torch_sparse import SparseTensor, matmul
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.nn.conv.gcn_conv import gcn_norm
"
4	"        for iter_id in range(loop_num):
            batch_data = list()
            handle_id = iter_id * batch_size
"
2	"

"
4	"# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
"
1	"        sum(
            p.numel() * to_bytes(p.dtype) for p in model.parameters() if p.requires_grad
"
1	"    # Align with GPU (require batchsize > 0 in configuration file)
    $0 --ngpu 1 example.wav

Available models:
"
1	"                        ""hypo: "" + """".join([char_list[int(x)] for x in hyp[""yseq""][1:]])
                    )

"
4	"wool, woolen, woollen
worm fence, snake fence, snake-rail fence, Virginia fence
wreck
"
1	"                data[name] = value

            yield uid, data

        if count == 0:
"
2	"        """"""
        logging.debug('make Spark cluster provide hosts %s', hosts)

        # shut down missing works first
"
4	"        top_padding, bottom_padding = cal_padding(input.shape[2], stride,
                                                  filter_size)
        left_padding, right_padding = cal_padding(input.shape[2], stride,
                                                  filter_size)
"
4	"            use_bias=True,
            padding_type=self.padding_type,
"
1	"            warnings.warn(f""Duplicated: {key}"")

        if self.fd is None:
"
0	"        # -- check default size
        srt = sorted([v.co for v in verts], key=lambda co: co.xy.to_tuple(2))
        self.assertEqual((srt[-1] - srt[0]).xy.to_tuple(1), (4.0, 4.0))

"
6	"        add_self_loops (bool, optional): If set to :obj:`False`, will not add
            self-loops to the input graph. (default: :obj:`True`)
        bias (bool, optional): If set to :obj:`False`, the layer will not learn
            an additive bias. (default: :obj:`True`)
"
2	"      build: test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      image-repository: 823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite
      cache-from: test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0:823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite:SLUG-test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0-latest
      config: docker-compose.test.yml
"
4	"import paddlehub as hub
import cv2
"
2	"                mpi_flags, binding_args = horovod.run.mpi_run._get_mpi_implementation_flags(False)
                self.assertIsNotNone(mpi_flags)
                mpi_flags.append('-mca plm_rsh_no_tree_spawn true')
                mpi_flags.append('-mca plm_rsh_num_concurrent {}'.format(large_cluster_threshold))
"
2	"class HorovodJoinOp : public AsyncOpKernel {
public:
  explicit HorovodJoinOp(OpKernelConstruction* context)
      : AsyncOpKernel(context) {}
"
2	"        self.assertEqual(2, results[1]['size'])
        self.assertEqual(2, results[1]['rendezvous'])

        self.assertEqual(1, results[2]['start_rank'])
        self.assertEqual(2, results[2]['size'])
"
2	"            except threading.BrokenBarrierError:
                if self._barrier.broken:
                    # Timeout or other non-recoverable error, so exit
                    raise
"
4	"bath towel
bathtub, bathing tub, bath, tub
beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon
"
2	"                 activation='relu',
                 input_shape=(28, 28, 1)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
"
3	"            webbrowser.open(share_link, new=2)
        except:
"
2	"    queue: cpu
- label: ':tensorflow: Test TensorFlow 2.0 Keras MNIST (test-cpu-gloo-py3_8-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark3_0_0)'
  command: horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/tensorflow2_keras_mnist.py
"
4	"squirrel monkey, Saimiri sciureus
Madagascar cat, ring-tailed lemur, Lemur catta
indri, indris, Indri indri, Indri brevicaudatus
Indian elephant, Elephas maximus
"
2	"    Tests can run forked but not in parallel.
    Running tests in parallel would put CPU under pressure.

    Running tests forked will reduce memory footprint from 1.5GB for the master to 512MB.
    Each executor (slot) needs 512GB memory. There are at most 6 executors used in these tests.
"
2	"      build: test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      image-repository: 823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite
"
2	"    NAME = 'worker notification service'

"
4	"
        # Squeeze and Excitation
"
2	"    # keep logic in sync with is_gloo_used(...)
    verbose = verbosity is not None and verbosity >= 2
"
2	"class CommandExitCodeResponse(object):
    def __init__(self, terminated, exit_code):
        self.terminated = terminated
"
2	"        f.write(json.dumps(state_dict) + os.linesep)


@hvd.elastic.run
def train(state):
"
4	"                   override_params=None,
                   use_se=True):
    model = EfficientNet(
"
4	"coral fungus
agaric
gyromitra
"
2	"    automatic: true
  agents:
    queue: cpu
"
4	"park bench
parking meter
passenger car, coach, carriage
patio, terrace
"
4	"def crop_image(img, target_size, center):
    width, height = img.size
    size = target_size
    if center == True:
"
4	"import os
from unittest import TestCase, main
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
"
4	"        # Skip connection and drop connect
        input_filters, output_filters = block_args.input_filters, block_args.output_filters
"
2	"from horovod.run.elastic.rendezvous import PUT_WORKER_ADDRESSES
from horovod.run.http.http_client import put_data_into_kvstore


HOROVOD_GLOO_RENDEZVOUS_ADDR = 'HOROVOD_GLOO_RENDEZVOUS_ADDR'
"
2	"        with mock.patch(""horovod.run.mpi_run._get_mpi_implementation_flags"", side_effect=mpi_impl_flags):
            with mock.patch(""horovod.run.mpi_run.safe_shell_exec.execute"", return_value=0) as execute:
"
2	"            v = 1.0 if hvd.rank() == 0 else 2.0
            weights1 = [
                np.array([[v, v], [v, v]]),
"
6	"        for _ in range(towers):
            modules = [Linear((3 if edge_dim else 2) * self.F_in, self.F_in)]
            for _ in range(pre_layers - 1):
"
4	"indigo bunting, indigo finch, indigo bird, Passerina cyanea
robin, American robin, Turdus migratorius
"
4	"    def classify(self,
                 images=None,
                 paths=None,
                 batch_size=1,
"
4	"

def EfficientNetB0(is_test=False,
"
2	"    Args:
        func: a wrapped function taking any number of args or kwargs. The first argument
              must be a `horovod.common.elastic.State` object used to synchronize state across
"
2	"

@contextlib.contextmanager
def override_env(env):
"
4	"    if norm_type == 'batch_norm':
        param_attr = fluid.ParamAttr(
            name=name + '_weights', initializer=fluid.initializer.Constant(1.0))
        bias_attr = fluid.ParamAttr(
"
1	"    audio2 = audio2.astype(np.float64) / (np.iinfo(np.int16).max + 1)

    with SoundScpWriter(tmp_path, tmp_path / ""wav.scp"", dtype=np.int16) as writer:
        writer[""abc""] = 16, audio1
"
2	"
# Horovod: initialize model and optimizer state so we can synchronize across workers
for batch_idx, (images, labels) in enumerate(dataset.take(1)):
    training_step(images, labels, allreduce=False)
"
4	"        name='b5',
        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
"
2	"        Because commits are a heavy operation involving data copy (potentially from GPU to host), it is
        recommended to consider committing less frequently than once per batch. This allows users to tradeoff
"
4	"
                if num_data > 2:
"
4	"import argparse
import os

"
2	"    conf = SparkConf().setAppName('prediction') \
        .setExecutorEnv('LD_LIBRARY_PATH', os.environ.get('LD_LIBRARY_PATH')) \
        .setExecutorEnv('PATH', os.environ.get('PATH'))
    if args.processing_master:
"
4	"        valid_names = ['b' + str(i) for i in range(8)]
        assert name in valid_names, 'efficient name should be in b0~b7'
        model_name = 'efficientnet-' + name
        self._blocks_args, self._global_params = get_model_params(
"
4	"        program, feeded_var_names, target_vars = fluid.io.load_inference_model(
            dirname=self.default_pretrained_model_path, executor=exe)
"
6	"    y_mask = loader.dataset.y_mask
    ious = [[] for _ in range(len(loader.dataset.categories))]

"
4	"crossword puzzle, crossword
street sign
"
1	"                ""pyopenjtalk"",
                ""pyopenjtalk_kana"",
                ""pypinyin_g2p"",
"
2	"                  tf.float64, tf.bool]
        dims = [1, 2, 3]
        tests = []
        shape_tests = []
"
3	"    _STATUS_STATUSCODE,
  ],
  serialized_options=None,
"
2	"        env[secret.HOROVOD_SECRET_KEY] = codec.dumps_base64(key)

        # we also need to provide the current working dir to mpirun_exec_fn.py
        env['HOROVOD_SPARK_WORK_DIR'] = os.getcwd()
"
4	"
    def shortcut(self, input, data_residual):
        return fluid.layers.elementwise_add(input, data_residual)
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
"
4	"        padding_type=padding_type,
        override_params=override_params,
        use_se=use_se)
    return model
"
4	"    data = base64.b64decode(b64str.encode('utf8'))
    data = np.fromstring(data, np.uint8)
    data = cv2.imdecode(data, cv2.IMREAD_COLOR)
    return data
"
1	"ngpu=1         # number of gpus (""0"" uses cpu, otherwise use gpu)
debugmode=1
dumpdir=dump   # directory to dump full features
N=0            # number of minibatches to be used (mainly for debugging). ""0"" uses all minibatches.
verbose=0      # verbose option
"
2	"  - ecr#v1.2.0:
      login: true
"
2	"            if value is None:
                if key in env:
                    del env[key]
"
4	"Egyptian cat
cougar, puma, catamount, mountain lion, painter, panther, Felis concolor
lynx, catamount
"
2	"    """"""
    output = io.StringIO()
"
1	"
            # 8. Build iterator factories
            common_iter_kwargs = dict(
                iterator_type=args.iterator_type,
"
4	"toucan
drake
"
4	"hamper
hand blower, blow dryer, blow drier, hair dryer, hair drier
hand-held computer, hand-held microcomputer
"
4	"           filter_size=7,
           stride=1,
"
3	"

"
4	"odometer, hodometer, mileometer, milometer
oil filter
organ, pipe organ
"
1	"        schedulers: Sequence[Optional[AbsScheduler]],
        ngpu: int = 0,
        use_apex: bool = False,
    ):
        states = torch.load(
"
4	"            component.append(each)
    if images is not None:
"
2	"      run: test-cpu-gloo-py3_8-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark3_0_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
6	"
def test_gcn_conv_with_sparse_input_feature():
"
1	"        # check number of hypotheis
        if len(nbest_hyps) == 0:
"
4	"pencil box, pencil case
pencil sharpener
"
4	"        keep_prob = 1.0 - prob
        random_tensor = keep_prob + fluid.layers.uniform_random_batch_size_like(
            inputs, [-1, 1, 1, 1], min=0., max=1.)
"
4	"            stride=s,
            num_groups=oup,
            bn_act=None,
            padding_type=self.padding_type,
"
2	"            # in Elastic Horovod on Spark with auto-scaling
            # keys in task_addresses are in range(max_np or proc_num)
            # but not all keys may exist, so we don't do for index in range(proc_num)
            indices = list(self._task_addresses_for_tasks.keys())
"
4	"cardoon
mushroom
"
2	"            continue
        for addr in addrs:
            if addr.family == AF_INET and addr.address == '127.0.0.1':
                common_intfs.add(iface)
"
4	"            # Update block input and output filters based on depth multiplier.
            block_args = block_args._replace(
"
1	"        if self.has_children:
            raise RuntimeError(""This writer points out a directory"")
        if key in self.keys:
"
4	"badger
armadillo
three-toed sloth, ai, Bradypus tridactylus
orangutan, orang, orangutang, Pongo pygmaeus
"
4	"

"
6	"    assert jit(x, pos).tolist() == out1.tolist()

    torch.manual_seed(12345)
    assert jit(x, pos, batch).tolist() == out2.tolist()
"
4	"        padding = padding

    conv = fluid.layers.conv2d(
"
4	"lawn mower, mower
lens cap, lens cover
letter opener, paper knife, paperknife
"
2	"
        self._register_hooks()

    def set_backward_passes_per_step(self, passes):
"
2	"      push-retries: 5
  - ecr#v1.2.0:
"
4	"        use_se=use_se)
    return model


"
1	"            if name in self.debug_info:
                raise RuntimeError(f'""{name}"" is duplicated for data-key')
"
2	"import warnings

import pytest

from horovod.run.common.service.task_service import BasicTaskClient, BasicTaskService
"
4	"    ""EfficientNetB1 is a image classfication model, this module is trained with imagenet datasets."",
    version=""1.1.0"")
class EfficientNetB1ImageNet(hub.Module):
    def _initialize(self):
        self.default_pretrained_model_path = os.path.join(
"
4	"        assert num_data >= 1 and num_data <= 3, ""num_data(%d) must be 1, 2, or 3"" % num_data
        main_program = fluid.Program()
        startup_program = fluid.Program()
        with fluid.program_guard(main_program, startup_program):
"
1	"    ""tts"",
    classes=dict(tacotron2=Tacotron2, transformer=Transformer),
    type_check=AbsTTS,
    default=""tacotron2"",
"
4	"    conv = fluid.layers.conv2d(
        input,
"
2	"    message_queue_.push(std::move(message));
    messages.pop_front();
  }
"
0	"    suite.addTests(loader.loadTestsFromModule(test_utils))
    suite.addTests(loader.loadTestsFromModule(test_floorplan))
"
4	"    return model


"
4	"punching bag, punch bag, punching ball, punchball
purse
quill, quill pen
"
4	"        name='b4',
        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
"
4	"# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
5	"                    for node in nodes:
                        # print ('**game=%s' % node.text)
"
4	"# -*- coding:utf-8 -*-
# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
"
4	"strawberry
orange
lemon
fig
pineapple, ananas
"
6	"        out = torch.cat(outs, dim=1)

        return self.lin(out)
"
2	"        self.assertEqual(2, results[2]['size'])
        self.assertEqual(1, results[2]['rendezvous'])
        self.assertEqual(results[1]['hostname'], results[2]['hostname'])

"
2	"                                                     df.CompetitionOpenSinceMonth)))
        df = df.withColumn('CompetitionDaysOpen',
                           F.when(df.CompetitionOpenSinceYear > 1900,
                                  F.greatest(F.lit(0), F.least(F.lit(360 * 2), F.datediff(df.Date, df.CompetitionOpenSince))))
"
6	"    -multi-relational-data>`_ paper,
    containing 41,105 entities, 18 relations and 141,442 fact triplets,
    *e.g.*, furniture includes bed.

"
3	"
        if self.args.push:
            self.push(image.tags[0], self.readme_path)
"
2	"
    :param hosts_string: list of addresses and number of processes on each host.
        For example:
"
6	"            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,
                                         size, kwargs)

            msg_aggr_kwargs = self.inspector.distribute(
                'message_and_aggregate', coll_dict)
"
1	"        E2EASR.attention_add_arguments(parser)
        E2EASR.decoder_add_arguments(parser)
"
4	"drilling platform, offshore rig
drum, membranophone, tympan
drumstick
"
1	"        )

"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
"
2	"

"
4	"    def _encode_block_string(block):
        """"""Encodes a block to a string.""""""
        args = [
            'r%d' % block.num_repeat,
"
2	"    def save(self):
        new_state = {}
"
6	"        self.init_weight = Parameter(torch.Tensor(K, F_in, F_out))
        self.weight = Parameter(torch.Tensor(max(0, T - 1), K, F_out, F_out))
        self.root_weight = Parameter(torch.Tensor(T, K, F_in, F_out))
"
2	"    def test_rsh_with_zero_exit_code(self):
        self.do_test_rsh('true', 0)

    def test_rsh_with_non_zero_exit_code(self):
        self.do_test_rsh('false', 1)
"
2	"                host, local_rank = key.split(':')
                addresses, secret_key = codec.loads_base64(value)
                self._put_worker_addresses(host, int(local_rank), addresses, secret_key)

"
4	"
def cv2_to_base64(image):
"
4	"* trainable (bool): 计算图的参数是否为可训练的；
* pretrained (bool): 是否加载默认的预训练模型。
"
2	"  timeout_in_minutes: 10
  retry:
    automatic: true
  agents:
"
4	"                }

"
3	"    def set_chunk(self, chunk: 'jina_pb2.Chunk', chunk_info: Dict, new_chunk_id: bool = True):
        for k, v in chunk_info.items():
            if k == 'blob':
                if isinstance(v, jina_pb2.NdArray):
"
2	"                while state._host_messages.empty():
                    if int(time.time()) - start > args.discovery_wait:
                        raise TimeoutError('Timed out waiting for notifications from driver.')
                    time.sleep(0.1)

"
4	"                 padding_type='SAME',
                 override_params=None,
                 is_test=False,
                 use_se=True):
        valid_names = ['b' + str(i) for i in range(8)]
"
4	"                    dtype='float32',
                    param_attr=w_param_attrs)
                emb_2_name = emb_2.name
                data_list.append(text_2)
                emb_name_list.append(emb_2_name)
"
4	"            conv_name='_conv_stem',
            bn_name='_bn0')
"
2	"            while True:
                line = f.readline()
                if not line:
"
2	"    @mock.patch('horovod.run.gloo_run._get_min_start_hosts', return_value=1)
    def test_all_hosts_blacklisted(self, mock_get_min_start_hosts):
"
1	"                    raise RuntimeError(
                        f""All values must be converted to np.ndarray object ""
                        f'by preprocessing, but ""{name}"" is still {type(value)}.'
                    )
"
4	"pinwheel
pirate, pirate ship
"
4	"        x -= tmp
        x = np.exp(x)
        tmp = np.sum(x)
        x /= tmp
"
2	"        if mode == 'local':
            local_hosts = ['localhost', '127.0.0.1']
"
2	"      login: true
  timeout_in_minutes: 15
  retry:
    automatic: true
  agents:
"
4	"        self.padding_type = padding_type
        self.use_se = use_se

"
1	"@pytest.mark.parametrize(""dprenet_layers"", [0, 1])
@pytest.mark.parametrize(""postnet_layers"", [0, 1])
@pytest.mark.parametrize(
"
2	"        discovery = FixedHosts(slots)

        driver = ElasticDriver(mock.Mock(), discovery, min_np=2, max_np=4)
        driver.wait_for_available_slots(min_np=2)

"
4	"burrito
red wine
espresso
cup
"
4	"snowplow, snowplough
soap dispenser
soccer ball
sock
solar dish, solar collector, solar furnace
"
2	"

"
4	"           filter_size=7,
           stride=1,
           stddev=0.02,
"
6	"        F, M = self.rel_in_channels, self.out_channels
        (E, D), K = edge_attr.size(), self.kernel_size
"
4	"jean, blue jean, denim
jeep, landrover
jersey, T-shirt, tee shirt
"
2	"  plugins:
  - docker-compose#v2.6.0:
"
2	"# limitations under the License.
# ==============================================================================
"
4	"    if not multiplier:
        return filters
    divisor = global_params.depth_divisor
"
4	"                place = fluid.CPUPlace()
                exe = fluid.Executor(place)
"
4	"cassette player
castle
catamaran
CD player
"
6	"                raise ValueError(f'Unknown aggregator ""{aggregator}"".')
            outs.append(out)
        out = torch.cat(outs, dim=-1)

        deg = degree(index, dim_size, dtype=inputs.dtype)
"
2	"                                     op=hvd.Adasum if args.use_adasum else hvd.Average)

# Horovod: broadcast parameters & optimizer state.
hvd.broadcast_parameters(model.state_dict(), root_rank=0)
hvd.broadcast_optimizer_state(optimizer, root_rank=0)
"
4	"# -*- coding:utf-8 -*-
# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
"
2	"        # In order to ensure all workers raise the exception at the same time, we need to sync
        # the updated state across all the workers.
"
4	"
paddlehub >= 1.6.0

"
4	"        return im_mean

    def get_pretrained_images_std(self):
"
4	"        var = fluid.layers.reduce_mean(
            fluid.layers.square(input - mean), dim=[2, 3], keep_dim=True)
        if name is not None:
            scale_name = name + ""_scale""
"
6	"            out['adj_t'] = edge_index
            out['edge_index'] = None
"
6	"        \mathbf{x}_i, \underset{j \in \mathcal{N}(i)}{\bigoplus}
        h_{\mathbf{\Theta}} \left( \mathbf{x}_i, \mathbf{x}_j \right)
"
2	"        fn.assert_not_called()
        event.set()
        thread.join(1.0)
        self.assertFalse(thread.is_alive())
"
2	"    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):
        super().__init__(num_features, eps, momentum, affine, track_running_stats)

    def _check_input_dim(self, input):
"
4	"necklace
nipple
notebook, notebook computer
obelisk
oboe, hautboy, hautbois
"
4	"def cal_padding(img_size, stride, filter_size, dilation=1):
    """"""Calculate padding size.""""""
    if img_size % stride == 0:
        out_size = max(filter_size - stride, 0)
"
4	"            use_gpu (bool): Whether to use gpu.
            top_k (int): Return top k results.

"
4	"                global_vars = context_prog.global_block().vars
                inputs = {
                    key: global_vars[value]
"
2	"                handle, ctx = self._allreduce_grad_async(p)
                self._handles[p] = (handle, ctx)
        for p, (handle, ctx) in self._handles.items():
            output = synchronize(handle)
"
4	"                        return b

"
6	"    t = '(Tensor, SparseTensor, OptTensor) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
"
4	"jacamar
toucan
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
"
4	"将模型保存到指定路径。

**参数**

"
1	"            olens = olens.new([olen - olen % self.reduction_factor for olen in olens])
            max_olen = max(olens)
"
6	"    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:
        return matmul(adj_t, x, reduce=self.aggr)

"
1	"                ""lightconv2d"",
                ""dynamicconv"",
                ""dynamicconv2d"",
                ""light-dynamicconv2d"",
"
5	"            x = x0 + ((i+1) % 2) * layout.XS // 2
            y = y0 + i * layout.YS // 4
"
4	"carpenter's kit, tool kit
carton
car wheel
"
4	"from PIL import Image

__all__ = ['reader']

"
1	"            utts = s.readlines()
            n_utts = len(utts)
            for i in range(n_utts):
                utts[i] = utts[i].split()[0]
"
2	"  retry:
    automatic: true
  agents:
    queue: cpu
"
2	"    return path

"
4	"        if combined:
            model_filename = ""__model__"" if not model_filename else model_filename
            params_filename = ""__params__"" if not params_filename else params_filename
"
2	"lr = 0.001
model = tf.keras.Sequential([tf.keras.layers.Dense(2, activation='softmax')])
"
4	"pick, plectrum, plectron
pickelhaube
picket fence, paling
pickup, pickup truck
pier
"
4	"        width_padding = right_padding
        if top_padding != bottom_padding or left_padding != right_padding:
            height_padding = top_padding + stride
            width_padding = left_padding + stride
"
2	"        # Lookup default timeout from the environment variable.
        start_timeout = int(os.getenv('HOROVOD_SPARK_START_TIMEOUT', '600'))

    # nics needs to be a set
    if nics and not isinstance(nics, set):
"
2	"        """"""
        try:
            return super(SparkTaskService, self).wait_for_command_termination()
"
2	"        if size() == 1:
            return self._run_bn(input)
        return _SyncBatchNorm.apply(
            input, self.weight, self.bias, self.running_mean, self.running_var,
            self.eps, self.momentum)
"
2	"from __future__ import division
from __future__ import print_function

"
4	"coyote, prairie wolf, brush wolf, Canis latrans
dingo, warrigal, warragal, Canis dingo
"
4	"# -*- coding:utf-8 -*-
# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
"
1	"                    value = value.astype(self.float_dtype)
                elif value.dtype.kind == ""i"":
                    value = value.astype(self.int_dtype)
                else:
                    raise NotImplementedError(f""Not supported dtype: {value.dtype}"")
"
2	"    # error occurs in one thread, entire process will be terminated. Otherwise,
    # threads will keep running and ssh session.
    exec_command = _exec_command_fn(settings)
    launch_gloo(command, exec_command, settings, nics, env, server_ip)

"
4	"waffle iron
wall clock
"
2	"        # Run the job
        _launch_job(use_mpi, use_gloo, settings, driver, env, stdout, stderr)
    except:
        # Terminate Spark job.
"
2	"  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2
      config: docker-compose.test.yml
"
2	"#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
"
4	"            blocks_args.append(BlockDecoder._decode_block_string(block_string))
        return blocks_args

    @staticmethod
"
2	"    def skip_synchronize(self):
        """"""
        A context manager used to specify that optimizer.step() should
        not perform synchronization.
"
4	"cuirass
dam, dike, dyke
desk
desktop computer
"
4	"vizsla, Hungarian pointer
English setter
Irish setter, red setter
Gordon setter
"
1	"            ],
            help=""transformer encoder self-attention layer type"",
        )
        group.add_argument(
            ""--transformer-decoder-selfattn-layer-type"",
"
4	"    """""" Calculate and round number of filters based on depth multiplier. """"""
    multiplier = global_params.width_coefficient
"
4	"radio telescope, radio reflector
rain barrel
recreational vehicle, RV, R.V.
reel
reflex camera
"
1	"def main(args):
    """"""Run the main decoding function.""""""
    parser = get_parser()
    args, extra = parser.parse_known_args(args)
"
4	"swing
switch, electric switch, electrical switch
syringe
table lamp
tank, army tank, armored combat vehicle, armoured combat vehicle
"
2	"      const std::vector<TensorTableEntry>& entries, Timeline& timeline,
      const std::function<void()>& error_check_callback) {
"
4	"fiddler crab
king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica
"
1	"        load_num_sequence_text(p)

"
4	"        depth_coefficient=depth_coefficient,
        depth_divisor=8,
        min_depth=None)
"
6	"        i, u = i_and_u(pred, data.y, loader.dataset.num_classes, data.batch)
        iou = i.cpu().to(torch.float) / u.cpu().to(torch.float)
"
1	"        >>> get_human_readable_count(4e12)  # (four trillion)
        '4 T'
        >>> get_human_readable_count(5e15)  # (more than trillion)
"
4	"barn
barometer
barrel, cask
"
1	"                            f"" {f}:L{linenum}: {line})""
                        )
                    key, value = sps
                    keys.append(key)
"
3	"        self.logger.success(
            f'🎉 built {image.tags[0]} ({image.short_id}) uncompressed size: {get_readable_size(image.attrs[""Size""])}')
"
4	"        for op in ops:
            splits = re.split(r'(\d.*)', op)
"
4	"    return img


def process_image(img):
    img = resize_short(img, target_size=256)
"
4	"mailbag, postbag
mailbox, letter box
maillot
maillot, tank suit
manhole cover
"
3	"
    set_hub_base_parser(parser)
"
2	"        nics: Interfaces to include by MPI.
        env: Environment dictionary to use for running command.
"
4	"wig
window screen
window shade
"
2	"    train_dataset, batch_size=args.batch_size, sampler=train_sampler, **kwargs)

test_dataset = \
"
2	"    """"""Assign hosts with process capacities (slots) to ranks in the Horovod process.

    This function will try to allocate as many as possible processes on the same host to leverage
    local network.

"
4	"        Args:
            trainable (bool): Set parameters in program to be trainable.
"
4	"        return conv

    def conv_bn_layer(self,
                      input,
"
2	"

# adjust learning rate on reset
def on_state_reset():
    for param_group in optimizer.param_groups:
"
2	"        # exit(1)


@hvd.elastic.run
"
2	"        #            optimizer logic and g is the gradient
        #  delta = current-start
        #  allreduce_(delta)
        #  start += delta
        #  current = start
"
4	"    def mb_conv_block(self,
                      inputs,
                      block_args,
                      is_test=False,
                      drop_connect_rate=None,
"
6	"            an :obj:`torch_geometric.data.Data` object and returns a
            transformed version. The data object will be transformed before
            being saved to disk. (default: :obj:`None`)
"
4	"    * classification (paddle.fluid.framework.Variable): 分类结果，也就是全连接层的输出；
    * feature\_map (paddle.fluid.framework.Variable): 特征匹配，全连接层前面的那个张量。
* context\_prog(fluid.Program): 计算图，用于迁移学习。

"
2	"
import cloudpickle
import numpy as np
import tensorflow as tf
"
4	"                 op_type,
                 fan_out,
"
3	"        if 'port_expose' not in kwargs:
            kwargs['port_expose'] = self.port_expose
"
1	"    def __setitem__(self, key, value):
        assert isinstance(value, np.ndarray), type(value)
        p = self.dir / f""{key}.npy""
"
1	"        candidates = np.array(
            [logdelta[-1, len(y_int) - 1], logdelta[-1, len(y_int) - 2]]
        )
        prev_state = [len(y_int) - 1, len(y_int) - 2]
"
2	"
from horovod.run.common.util import network, secret
"
1	"            # maxlen >= 1
            maxlen = max(1, int(recog_args.maxlenratio * h.size(0)))
        minlen = int(recog_args.minlenratio * h.size(0))
        logging.info(""max output length: "" + str(maxlen))
"
2	"    .Output(""local_size: int32"")
    .SetIsStateful()
    .SetShapeFn([](shape_inference::InferenceContext* c) {
      c->set_output(0, c->Scalar());
"
6	"    t = '(Tuple[OptTensor, Tensor], Tensor, OptTensor) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
"
4	"    model = EfficientNet(
        name='b0',
"
2	"        with make_batch_reader('%s/train_df.parquet' % args.data_dir, num_epochs=None,

"
2	"            res = horovod.spark.run_elastic(fn, args=(2, 5, 4),
                                            num_proc=2, min_np=2, max_np=2,
"
2	"      run: test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
"
4	"            initializer=fluid.initializer.Constant(0.0),
            trainable=True)
        scale = helper.create_parameter(
            attr=scale_param, shape=input.shape[1:2], dtype=dtype)
        offset = helper.create_parameter(
"
4	"                        b = os.path.exists(
                            os.path.join(self.default_pretrained_model_path,
                                         var.name))
"
4	"punching bag, punch bag, punching ball, punchball
purse
quill, quill pen
quilt, comforter, comfort, puff
racer, race car, racing car
"
0	"        btools.utils.cone(self.bm)
        self.assertEquals(len(self.bm.faces), 96)
        self.assertEquals(len(self.bm.verts), 66)

    def test_cylinder(self):
"
2	"                         for slot_info in slots
                         if (host, slot_info.local_rank) not in active_slots]
        return pending_slots

"
1	"from espnet.nets.pytorch_backend.tacotron2.encoder import Encoder as EncoderPrenet
from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention
from espnet.nets.pytorch_backend.transformer.decoder import Decoder
from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding
from espnet.nets.pytorch_backend.transformer.embedding import ScaledPositionalEncoding
"
4	"# 打印预测结果
print(r.json()[""results""])
```

### 查看代码
"
2	"  retry:
    automatic: true
  agents:
"
4	"sandal
sarong
sax, saxophone
"
2	"            state.epoch = 11

            state.restore()

            for w1, w2 in zip(self.evaluate(vars1), weights1):
"
3	"import unittest

import numpy as np
from jina.executors.encoders.numeric.ica import FastICAEncoder
"
4	"from __future__ import print_function

"
1	"def test_NpyScpWriter(tmp_path: Path):
    array1 = np.random.randn(1)
    array2 = np.random.randn(1, 1, 10)
    with NpyScpWriter(tmp_path, tmp_path / ""feats.scp"") as writer:
        writer[""abc""] = array1
"
1	"@pytest.mark.skipif(
    LooseVersion(torch.__version__) < LooseVersion(""1.2""), reason=""require pytorch>=1.2""
"
4	"            bn_eps=self._bn_eps,
            name=name,
            conv_name=name + '_project_conv',
            bn_name='_bn2')
        return conv
"
6	"    global_nn = Seq(Lin(32, 32))

"
4	"radiator
radio, wireless
radio telescope, radio reflector
"
2	"                    return current_hosts
                if self._shutdown.is_set():
                    raise RuntimeError('Job has been shutdown, see above error messages for details.')
"
4	"            is_test = True
        elif phase in [""train""]:
            is_test = False
        else:
            raise ValueError(
"
2	"    def find_available_hosts_and_slots(self):
        stdout = io.StringIO()
        exit_code = safe_shell_exec.execute(self._discovery_script, stdout=stdout)
"
3	"    $(""#third"").removeClass(""active"");
    $(""#fourth"").removeClass(""active"");
})
"
0	"    resolution: IntProperty(
        name=""Resolution"",
        min=3,
        max=128,
"
4	"# -*- coding:utf-8 -*-
# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
"
4	"drumstick
dumbbell
Dutch oven
electric fan, blower
electric guitar
"
4	"        init=initial,
        use_bias=use_bias,
        filter_size=filter_size,
        stddev=stddev)

"
2	"# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"
2	"  command: bash -c ""\$(cat /oneccl_env) && echo '/mpirun_command_ofi' > /mpirun_command && \$(cat /mpirun_command) python /horovod/examples/pytorch_mnist.py""
  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
"
4	"        'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25',
        'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
"
6	"    assert out1.size() == (2, 64)
    assert out2.size() == (2, 64)
    assert conv((x1, x2), edge_index, (4, 2)).tolist() == out1.tolist()
    assert torch.allclose(conv((x1, x2), adj.t()), out1, atol=1e-6)
"
2	"            self.skipTest((""No GPUs available""))

        if os.environ.get('HOROVOD_MIXED_INSTALL'):
            # Skip if compiled with CUDA but without HOROVOD_GPU_OPERATIONS.
"
1	"        # report extra information
        if self.use_scaled_pos_enc:
            stats.update(
                encoder_alpha=self.encoder.embed[-1].alpha.data.item(),
                decoder_alpha=self.decoder.embed[-1].alpha.data.item(),
"
2	"                np.array([v, v])
            ]
            vars1 = [tf.Variable(arr) for arr in weights1]

            weights2 = [
"
2	"            slots = {'host-1': 2, 'host-2': 2}
            discovery.set(slots)

        def remove_host():
"
2	"    across multiple workers in parallel, as workers come and go from the system. When a new worker is added,
    its state needs to be brought to the same point as the other workers, which is done by synchronizing
    the state object before executing `func`.

    When a worker is added or removed, other workers will raise an exception to bring them back to such a sync
"
4	"flatworm, platyhelminth
nematode, nematode worm, roundworm
conch
snail
slug
"
2	"    def test_timeout_with_gloo(self):
        if not gloo_built():
            self.skipTest(""Gloo is not available"")
"
4	"manhole cover
maraca
"
4	"
def conv2d(input,
           num_filters=64,
           filter_size=7,
"
2	"  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
"
4	"lumbermill, sawmill
magnetic compass
mailbag, postbag
"
4	"Dungeness crab, Cancer magister
rock crab, Cancer irroratus
"
1	"        scores = torch.cat(scores, 0).view(ys.shape[0], -1)
        return scores, outstates

"
2	"#       can get reclaimed by the cluster manager
# requires SPARK_CONF_BLACKLIST_ENABLED
SPARK_CONF_REUSE_FAILED_EXECUTOR_IN_APP = ('spark.blacklist.application.maxFailedTasksPerExecutor', SPARK_CONF_MAX_INT)
SPARK_CONF_DONT_REUSE_FAILED_EXECUTOR_IN_APP = ('spark.blacklist.application.maxFailedTasksPerExecutor', '1')
"
4	"        is_test=is_test,
        padding_type=padding_type,
"
2	"                            # HOROVOD_SPARK_PYTHONPATH injected at sys.path[1]
                            expected_sys_path = copy.copy(test_sys_path)
"
3	"            raise ValueError(
                ""vectors' shape [%d, %d] does not match with indexers's dim: %d"" %
                (vectors.shape[0], vectors.shape[1], self.num_dim))
"
2	"            if controller == 'mpi' and run == 'cmd':
                self.assertIsNone(_run(hargs))
"
2	"                                '{mpi_flags}       '
                                'cmd').format(binding_args=' '.join(binding_args), mpi_flags=' '.join(mpi_flags))

"
1	"            encoder_input_layer = torch.nn.Embedding(
                num_embeddings=idim, embedding_dim=adim, padding_idx=self.padding_idx
            )
        self.encoder = Encoder(
"
2	"  retry:
    automatic: true
  agents:
"
2	"
            assert results[2]['start_rank'] == 2
"
2	"                0: [1, 2]
            }
            obj = expected_obj if hvd.rank() == 0 else {}

            obj = hvd.broadcast_object(obj, root_rank=0)
"
4	"balance beam, beam
balloon
ballpoint, ballpoint pen, ballpen, Biro
Band Aid
banjo
"
2	"import unittest
import warnings

from elastic_common import BaseElasticTests
"
1	"    with Transformer Network`_, which convert the sequence of tokens into the sequence
    of Mel-filterbanks.

"
6	"        self.conv1 = GraphConv(in_channels, hidden_channels)
        self.conv2 = GraphConv(hidden_channels, hidden_channels)
        self.conv3 = GraphConv(hidden_channels, hidden_channels)
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
"
4	"    else:
        raise NotImplementedError(""activation: [%s] is not support"" % act)

    return conv
"
2	"
    sz = tf.placeholder(tf.int32, [1], name='bcast_object_size')
    bcast_size = broadcast(sz, root_rank, name + '.sz')

"
4	"        pool = fluid.layers.pool2d(
            input=conv, pool_type='avg', global_pooling=True, use_cudnn=False)

        if not is_test and self._global_params.dropout_rate:
"
4	"```python
def save_inference_model(dirname,
                         model_filename=None,
                         params_filename=None,
"
4	"        valid_names = ['b' + str(i) for i in range(8)]
        assert name in valid_names, 'efficient name should be in b0~b7'
        model_name = 'efficientnet-' + name
"
4	"```python
def get_expected_image_height()
```

返回预处理的图片高度，也就是224。
"
2	"
        for col, mapping in vocab.items():
"
2	"        self.elastic_timeout = None
        self.reset_limit = None


"
4	"artichoke, globe artichoke
bell pepper
"
6	"    source = inspect.getsource(func)
    signature = inspect.signature(func)

    # Parse `# type: (...) -> ...` annotation. Note that it is allowed to pass
    # multiple `# type:` annotations in `forward()`.
"
3	"from tests.executors.encoders.numeric import NumericTestCase


class MyTestCase(NumericTestCase):
"
2	"        event = None
        fn = mock.Mock()
        with pytest.raises(ValueError, match=""^Event must not be None$""):
"
2	"    queue: cpu
- label: ':fire: Single PyTorch MNIST (test-cpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0)'
"
3	"    $("".ship > .icon"").addClass(""active"");
    $("".pay"").removeClass(""active"");
    $("".wrap"").removeClass(""active"");
"
2	"    # error occurs in one thread, entire process will be terminated. Otherwise,
    # threads will keep running and ssh session.
    iface = list(nics)[0]
"
1	"from typeguard import check_return_type


class DatadirWriter:
    """"""Writer class to create kaldi like data directory.
"
3	"                output_fn=functools.partial(
                    _validate,
                    start=start,
"
4	"        param_attr, bias_attr = init_fc_layer(class_dim, '_fc')
        out = fluid.layers.fc(
            pool,
            class_dim,
"
4	"            filter_size=1,
            bn_act='swish',
            bn_mom=self._bn_mom,
            bn_eps=self._bn_eps,
"
4	"American alligator, Alligator mississipiensis
triceratops
thunder snake, worm snake, Carphophis amoenus
ringneck snake, ring-necked snake, ring snake
"
4	"    @staticmethod
    def _decode_block_string(block_string):
        """""" Gets a block through a string notation of arguments. """"""
"
2	"            num_proc=4,
            hosts='localhost:2,127.0.0.1:2',
            output_filename='>output filename goes here<',
            run_func_mode=True
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
2	"- label: ':muscle: Test MXNet MNIST (test-cpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0)'
  command: bash -c "" OMP_NUM_THREADS=1 \$(cat /mpirun_command) python /horovod/examples/mxnet_mnist.py""
  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0
"
6	"    assert torch.allclose(conv((x1, None), adj.t()), out2, atol=1e-6)

    t = '(OptPairTensor, Tensor, Size, NoneType) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert jit((x1, x2), edge_index).tolist() == out1.tolist()
"
4	"
    def get_expected_image_height(self):
        return 224

    def get_pretrained_images_mean(self):
"
4	"            batch_data = list()
            handle_id = iter_id * batch_size
"
4	"grasshopper, hopper
cricket
walking stick, walkingstick, stick insect
cockroach, roach
mantis, mantid
"
1	"
from argparse import Namespace
import logging
import math

"
2	"    with open(filename_tmp, 'w') as f:
        f.write(str(value))

    # Atomic rename to prevent race conditions from reader
    os.rename(filename_tmp, filename)
"
4	"rock python, rock snake, Python sebae
Indian cobra, Naja naja
"
4	"                      name=None,
                      conv_name=None,
                      bn_name=None):
        conv = conv2d(
"
2	">     # Cluster for GPU inference.
>     GPU_INFERENCE_CLUSTER = 'local-cluster[2,1,1024]'  # or 'spark://hostname:7077'
"
4	"        initializer=fluid.initializer.Constant(value=0.0))
    return param_attr, bias_attr


def init_fc_layer(fout, name='fc'):
"
2	"    return [results[index] for index in indices_in_rank_order]


"
4	"        'efficientnet-b6': (1.8, 2.6, 528, 0.5),
        'efficientnet-b7': (2.0, 3.1, 600, 0.5),
"
4	"# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
"
2	"def _model_built(model):
    return model.built if hasattr(model, 'build') else True


"
2	"
        # Check that we have already reset the maximum number of allowed times
        if self._reset_limit is not None and self._reset_count >= self._reset_limit:
            logging.error('reset count {} has exceeded limit {} -> stop running'
"
4	"        left_padding, right_padding = cal_padding(input.shape[2], stride,
                                                  filter_size)
        height_padding = bottom_padding
"
4	"

def EfficientNetB6(is_test=False,
"
4	"# See the License for the specific language governing permissions and
# limitations under the License.
import os
from unittest import TestCase, main
"
3	"import numpy as np

from jina.executors.indexers import BaseIndexer
from tests import JinaTestCase

"
1	"if [ ! -n ""${text}"" ]; then
    echo ""Text is empty: ${text}""
    exit 1
"
4	"
import os
"
1	"        text: torch.Tensor,
        spembs: torch.Tensor = None,
"
1	"            return self.recog(enc_output, recog_args, char_list, rnnlm)

        logging.info(""total log probability: "" + str(nbest_hyps[0][""score""]))
"
4	"        self.cpu_predictor = create_paddle_predictor(cpu_config)

        try:
            _places = os.environ[""CUDA_VISIBLE_DEVICES""]
"
1	"#!/usr/bin/env python3

if __name__ == ""__main__"":
"
2	"    def _do_test_spark_run(self, args=(), kwargs={}, num_proc=1, extra_mpi_args=None,
                           env=None, use_mpi=None, use_gloo=None,
                           stdout=None, stderr=None, verbose=2,
                           cores=2, expected_np=1, expected_env=''):
"
4	"            paths=[args.input_path],
            batch_size=args.batch_size,
            use_gpu=args.use_gpu)
        return results

"
6	"            reset(nn)
        for nn in self.post_nns:
            reset(nn)
        self.lin.reset_parameters()

"
2	"
    def _eval_var(self, var):
"
4	"jay
magpie
chickadee
water ouzel, dipper
"
2	"if args.cuda:
    # Horovod: pin GPU to local rank.
    torch.cuda.set_device(hvd.local_rank())
"
4	"        depth_coefficient=depth_coefficient,
        depth_divisor=8,
"
1	"                preprocess_fn=cls.build_preprocess_fn(args, train=False),
                collate_fn=cls.build_collate_fn(args),
                num_iters_per_epoch=None,
                max_cache_size=args.valid_max_cache_size,
"
5	"        layout, s = Layout(self), self.s
        self.setSize(
            layout.XM+9*layout.XS,
            layout.YM+3*layout.YS+7*layout.YOFFSET+2*layout.TEXT_HEIGHT)
"
2	"        self._synchronized = True

    @contextmanager
"
4	"    def add_module_config_arg(self):
        """"""
        Add the command config options.
        """"""
"
1	"            tokens = sp.DecodePieces(list(line.strip()))
        vocabs |= set(tokens)
    return model, vocabs
"
4	"        padding_type=padding_type,
        override_params=override_params,
        use_se=use_se)
"
4	"    img = resize_short(img, target_size=256)
    img = crop_image(img, target_size=DATA_DIM, center=True)
    if img.mode != 'RGB':
        img = img.convert('RGB')
"
2	"    automatic: true
  agents:
    queue: 2x-gpu-g4
- label: ':factory: Elastic Spark TensorFlow Tests (test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c ""cd /horovod/test/integration && SPARK_HOME=/spark SPARK_DRIVER_MEM=512m HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format '[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s' --capture=no test_elastic_spark_tensorflow.py""
"
1	"                ""minlenratio"": minlenratio,
            }
"
4	"    'depth_coefficient',
    'depth_divisor',
    'min_depth',
"
6	"
    @property
"
4	"groom, bridegroom
scuba diver
"
2	"                    tensor = tensor % 2
                tensor = tf.cast(tensor, dtype=dtype)
                gathered = hvd.allgather(tensor)
"
2	"    automatic: true
  agents:
    queue: cpu
- label: ':muscle: Single MXNet MNIST (test-cpu-mpich-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c "" python /horovod/examples/mxnet_mnist.py --epochs 3""
"
4	"matchstick
maypole
maze, labyrinth
measuring cup
"
1	"                    linenum += 1
                    try:
"
4	"#    http://www.apache.org/licenses/LICENSE-2.0
#
"
2	"
    tmout = timeout.Timeout(start_timeout,
                            message='Timed out waiting for {activity}. Please '
                                    'check connectivity between servers. You '
"
4	"hair slide
hair spray
half track
hammer
"
2	"          'License :: OSI Approved :: Apache Software License',
          'Development Status :: 4 - Beta',
          'Intended Audience :: Developers',
          'Topic :: Scientific/Engineering :: Artificial Intelligence',
"
2	"
        # Expand dates.
"
4	"wild boar, boar, Sus scrofa
warthog
hippopotamus, hippo, river horse, Hippopotamus amphibius
ox
"
4	"                num_repeat=round_repeats(block_args.num_repeat,
                                         self._global_params))

            # The first block needs to take care of stride and filter size increase.
"
6	"
    def forward(self, x, edge_index, edge_attr, batch):
"
4	"ice lolly, lolly, lollipop, popsicle
French loaf
bagel, beigel
pretzel
cheeseburger
"
4	"        random_tensor = keep_prob + fluid.layers.uniform_random_batch_size_like(
            inputs, [-1, 1, 1, 1], min=0., max=1.)
        binary_tensor = fluid.layers.floor(random_tensor)
        output = inputs / keep_prob * binary_tensor
"
2	"            rank_results[slot_info.rank] = (slot_info, updated_slot_info)
            return 0, time.time()
"
4	"    def get_pretrained_images_std(self):
        im_std = np.array([0.229, 0.224, 0.225]).reshape(1, 3)
        return im_std

"
4	"
    @staticmethod
"
2	"        if hvd.rank() == 0:
            callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_file, monitor='val_exp_rmspe', mode='min',
                                                                save_best_only=True))
"
1	"from espnet2.iterators.abs_iter_factory import AbsIterFactory


class MultipleIterFactory(AbsIterFactory):
"
2	"                    help='use adasum algorithm to do reduction')

args = parser.parse_args()
"
4	"# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
"
6	"                tree[i] = union_to_optional_(e)
        return tree

    tree = union_to_optional_(tree)
    type_repr = re.sub(r'\'|\""', '', str(tree)[1:-1]).replace(', [', '[')
"
4	"class EfficientNetB6TestCase(TestCase):
    def setUp(self):
"
4	"            bn_eps=self._bn_eps,
            name='',
            conv_name='_conv_stem',
            bn_name='_bn0')

"
2	"                                              stdout=stdout, stderr=stderr, verbose=verbose)

                self.assertFalse(str(e.value).startswith('Timed out waiting for Spark tasks to start.'),
                                 'Spark timed out before mpi_run was called, test setup is broken.')
"
4	"# -*- coding:utf-8 -*-
# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
"
1	"            )
        elif len(self.path_name_type_list) != 0:
            uid_iter = (
"
6	"
    .. note::
        The ZINC dataset is provided via
"
4	"            emb_name_list = [emb_1_name]

"
2	"                    hvd.allgather(torch.tensor([hvd.rank(), state.epoch, state.batch, state.rendezvous]), 'state').tolist()
                    logging.info('rank %s: commit epoch %s batch %s', hvd.rank(), state.epoch, state.batch)
"
4	"                block_size += 1

        for block_args in self._blocks_args:

            # Update block input and output filters based on depth multiplier.
"
4	"beer glass
bell cote, bell cot
"
2	"    # Horovod: use test_sampler to determine the number of examples in
    # this worker's partition.
"
2	"            assert updated_slot_info.cross_rank == 0, rank

    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
    def test_worker_notification_manager(self):
"
1	"            label = np.expand_dims(label, 1)
            blanks = np.zeros((label.shape[0], 1), dtype=np.int64) + blank_id
            label = np.concatenate([blanks, label], axis=1)
            label = label.reshape(-1)
"
3	"        - :command:`jina hub build my_pod/` build the image
        - :command:`jina hub build my_pod/ --push` build the image and push to the public registry
        - :command:`jina hub pull jinahub/pod.dummy_mwu_encoder:0.0.6` to download the image
"
2	"                                             end=i == len(discovery_schedule) - 1))
        os.chmod(discovery_script, 0o755)
        yield discovery_script


"
2	"  command: bash -c "" \$(cat /mpirun_command) python /horovod/examples/pytorch_mnist.py""
  plugins:
  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
"
2	"from horovod.spark.task import task_info, task_service

"
3	"
        Always use ``self.chunks(d)`` instead of ``d.chunks`` to access filtered chunks
        """"""
"
4	"jersey, T-shirt, tee shirt
jigsaw puzzle
jinrikisha, ricksha, rickshaw
joystick
kimono
"
4	"maypole
maze, labyrinth
measuring cup
medicine chest, medicine cabinet
megalith, megalithic structure
"
2	"    queue: 2x-gpu-g4
- label: ':factory: Elastic Tests (test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
"
4	"basenji
pug, pug-dog
Leonberg
"
2	"        if args.epoch_wait > 0:
            time.sleep(args.epoch_wait)

        state.epoch += 1
        state.batch = 0
"
2	"            return GetTaskToTaskAddressesResponse(next_task_client.addresses())


"
2	"
    def _run(self, discovery_schedule=None, exit_schedule=None, exit_mode='exception',
             np=2, min_np=2, max_np=4, hosts=None, reset_limit=None):
        if not discovery_schedule and not hosts:
"
2	"# Copyright 2019 Uber Technologies, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
"
2	"#
#     http://www.apache.org/licenses/LICENSE-2.0
"
3	"
        # check the required field in manifest
        for r in required:
            if r not in manifest:
                raise ValueError(f'{r} is missing in the manifest.yaml, it is required')
"
2	"#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
4	"baseball
basketball
bassinet
"
2	"
# By default, Adasum doesn't need scaling up learning rate.
def lr_scaler():
    return hvd.size() if not args.use_adasum else 1
"
4	"Saint Bernard, St Bernard
Eskimo dog, husky
"
4	"        self.true_std = np.array([0.229, 0.224, 0.225]).reshape(1, 3).tolist()

"
4	"typewriter keyboard
umbrella
unicycle, monocycle
"
4	"# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
"
4	"            if drop_connect_rate:
                drop_connect_rate *= float(idx) / block_size
"
1	"            else None
            for i in range(num_splits)
        ]
        max_cache_size = max_cache_size / num_splits

"
6	"        \end{bmatrix}}_{\text{aggregators}},

"
2	"  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/keras_spark_rossmann_run.py --num-proc 2 --data-dir file:///data --epochs 3 --sample-rate 0.01""
  plugins:
"
4	"
    conv = fluid.layers.conv2d(
        input,
"
4	"        self.assertEqual(mean.tolist(), self.true_mean)
        self.assertEqual(std.tolist(), self.true_std)


"
2	"        :param driver: Spark driver service
        :type driver: SparkDriverService
"
2	"                    return [json.loads(line) for line in lines]

    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
    @mock.patch('horovod.run.gloo_run._get_min_start_hosts', return_value=1)
    def test_hosts_added_and_removed(self, mock_get_min_start_hosts):
"
4	"whippet
Ibizan hound, Ibizan Podenco
"
4	"wing
wok
"
1	"
        # copyied from e2e_asr
        alpha = self.mtlalpha
        if alpha == 0:
"
6	"    edge_index = torch.tensor([
        [0, 0, 1, 2, 3, 4],
        [0, 1, 0, 3, 4, 2],
    ])
"
6	"    out1 = conv(x, edge_index)
    assert out1.size() == (4, 32)
    assert torch.allclose(conv(x, adj1.t()), out1, atol=1e-6)
    out2 = conv(x, edge_index, value)
"
4	"leopard, Panthera pardus
snow leopard, ounce, Panthera uncia
jaguar, panther, Panthera onca, Felis onca
lion, king of beasts, Panthera leo
tiger, Panthera tigris
"
4	"coral fungus
agaric
gyromitra
"
2	"            rank_results[slot_info.rank] = notification_receiver.events
            return 0, time.time()

"
2	"- label: ':terminal: Test Horovodrun (test-mixed-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: horovodrun -np 2 -H localhost:2 python /horovod/examples/tensorflow_mnist.py
"
4	"from __future__ import division
from __future__ import print_function

import collections
import re
"
4	"Siberian husky
dalmatian, coach dog, carriage dog
affenpinscher, monkey pinscher, monkey dog
basenji
pug, pug-dog
"
2	"      login: true
  timeout_in_minutes: 5
"
4	"picket fence, paling
pickup, pickup truck
pier
piggy bank, penny bank
pill bottle
"
1	"        x[0].numpy(), n_fft=16, n_shift=4, n_mels=4, fs=16000, fmin=80, fmax=7600
    )
    np.testing.assert_allclose(y, y2, rtol=0, atol=1e-5)

"
4	"
        tmp = fluid.layers.elementwise_mul(x=(input - mean), y=scale, axis=1)
"
4	"wallet, billfold, notecase, pocketbook
wardrobe, closet, press
warplane, military plane
"
2	"    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(10, activation='softmax')
])
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
"
1	")
def test_ESPnetDataset_pipe_wav(pipe_wav):
"
4	"        else:
            bias_attr = False
    return param_attr, bias_attr


"
4	"
```python
def classify(images=None,
                   paths=None,
                   batch_size=1,
"
3	"class HubIO:
    """""" :class:`HubIO` provides the way to interact with Jina Hub registry.
    You can use it with CLI to package a directory into a Jina Hub image and publish it to the world.

    Examples:
"
1	"            num_workers=num_workers,
            collate_fn=collate_fn,
            pin_memory=ngpu > 0,

"
2	"Status GPUOpContext::FinalizeGPUQueue(const std::vector<TensorTableEntry>& entries, bool free_host_buffer /*= true*/,
                                      const std::function<void()>& error_check_callback) {
"
4	"    elif act == 'swish':
        conv = fluid.layers.swish(conv, name=name + '_swish')
    elif act == None:
        conv = conv
"
4	"                outputs = {
                    key: global_vars[value]
"
4	"配置好服务端，以下数行代码即可实现发送预测请求，获取预测结果

"
4	"
    @serving
    def serving_method(self, images, **kwargs):
        """"""
"
4	"            bn_name='_bn1')

        pool = fluid.layers.pool2d(
            input=conv, pool_type='avg', global_pooling=True, use_cudnn=False)
"
4	"        self.assertEqual(width, 224)
        self.assertEqual(height, 224)
        self.assertEqual(mean.tolist(), self.true_mean)
        self.assertEqual(std.tolist(), self.true_std)

"
4	"        raise NotImplementedError(
            'model name is not pre-defined: %s' % model_name)
    if override_params:
        global_params = global_params._replace(**override_params)
    return blocks_args, global_params
"
2	"  - docker-compose#6b0df8a98ff97f42f4944dbb745b5b8cbf04b78c:
      build: test-cpu-openmpi-py3_6-tf1_14_0-keras2_2_4-torch1_2_0-mxnet1_4_1-pyspark2_4_0
      image-repository: 823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite
"
6	"    x1 = torch.randn(4, 16)
    x2 = torch.randn(2, 16)
    edge_index = torch.tensor([[0, 1, 2, 3], [0, 0, 1, 1]])
    row, col = edge_index
"
2	"        google_trend_all = google_trend_all \
            .withColumn('State', F.when(google_trend_all.State == 'NI', 'HB,NI').otherwise(google_trend_all.State))

        # Expand dates.
        return expand_date(google_trend_all)
"
4	"horizontal bar, high bar
horse cart, horse-cart
hourglass
"
4	"import cv2

"
4	"        conv = fluid.layers.swish(
            self._depthwise_conv_norm(conv, block_args, is_test, name))

        # Squeeze and Excitation
        if has_se:
"
1	"        k = self.kernel_size

        # first liner layer
"
4	"    summary=
    ""EfficientNetB5 is a image classfication model, this module is trained with imagenet datasets."",
    version=""1.1.0"")
class EfficientNetB5ImageNet(hub.Module):
    def _initialize(self):
"
4	"        out = fluid.layers.fc(
            pool,
            class_dim,
"
6	"    adj = SparseTensor(row=row, col=col, sparse_sizes=(4, 4))

"
2	"        if controller == 'gloo' and not gloo_built():
            self.skipTest(""Gloo is not available"")
        if controller == 'mpi':
"
4	"go-kart
golf ball
"
4	"church, church building
cinema, movie theater, movie theatre, movie house, picture palace
cleaver, meat cleaver, chopper
cliff dwelling
cloak
"
3	"    def __init__(self, lang_code: str = 'en', checkpoint_name: str = None, *args, **kwargs):
        """"""

        :param lang_code: en - english (Trained on data from various sources); fr - french (Only Tatoeba data); it - italian (Only Tatoeba data)
        :param args:
"
4	"        'r2_k5_s22_e6_i24_o40_se0.25',
        'r3_k3_s22_e6_i40_o80_se0.25',
"
6	"    else:
        return f'{return_type.__module__}.{return_type.__name__}'
"
4	"

if __name__ == ""__main__"":
"
2	"                        last_date = r.Date
                    fields = r.asDict().copy()
                    fields[('After' if asc else 'Before') + col] = (r.Date - last_date).days
"
2	"                    [r * 3, r * 3 + 1],
                    [r * 4, r * 4 + 1]
                ])
                for r in range(hvd.size())
"
1	"    assert len(target) == len(desired)
    assert ""abc"" in target
    assert ""def"" in target
"
1	"        assert check_argument_types()
        self.dir = Path(outdir)
        self.dir.mkdir(parents=True, exist_ok=True)
        scpfile = Path(scpfile)
"
4	"        """""" Returns output of the final convolution layer """"""

        conv = fluid.layers.swish(self._conv_stem_norm(inputs, is_test=is_test))
"
2	"
        for state.epoch in range(state.epoch, epochs):
            logging.info('rank %s: start epoch %s at batch %s', hvd.rank(), state.epoch, state.batch)

            for state.batch in range(state.batch, batches_per_epoch):
"
4	"        override_params=override_params,
        use_se=use_se)
    return model

"
2	"    state.rendezvous += 1
    while state.epoch < args.epochs:
        print('epoch {} batch {}'.format(state.epoch, state.batch))

        while state.batch < args.batches_per_epoch:
"
2	"
class HostState(object):
    def __init__(self):
        self._event = threading.Event()

"
4	"        padding = [height_padding, width_padding]
    elif padding_type == ""DYNAMIC"":
"
1	"
if [ ${stage} -le 0 ] && [ ${stop_stage} -ge 0 ]; then
"
1	"            query (torch.Tensor): (batch, time1, d_model) input tensor
            key (torch.Tensor): (batch, time2, d_model) NOT USED
            value (torch.Tensor): (batch, time2, d_model) NOT USED
"
2	"def launch_gloo_elastic(command, exec_command, settings, env, get_common_interfaces, rendezvous):
    # Make the output directory if it does not exist
    if settings.output_filename:
"
4	"miniature pinscher
Greater Swiss Mountain dog
Bernese mountain dog
Appenzeller
"
2	"            break
    if remaining_slots != 0:
        raise ValueError('Not enough slots on the hosts to fulfill the {slots} requested.'.format(
            slots=settings.num_proc))

"
4	"# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
"
2	"    launch_gloo(command, exec_command, settings, nics, {}, server_ip)


"
4	"# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
"
2	"      return Status::OK();
    })
"
1	"            ""--g2p"",
            type=str_or_none,
            choices=[None, ""g2p_en"", ""pyopenjtalk"", ""pyopenjtalk_kana""],
            default=None,
            help=""Specify g2p method if --token_type=phn"",
"
4	"                efficientnet_b0 = EfficientNetB0(
                    override_params=override_params)
                output, feature_map = efficientnet_b0.net(
                    input=image,
                    class_dim=len(self.label_list),
"
4	"                drop_connect_rate *= float(idx) / block_size
            conv = self.mb_conv_block(conv, block_args, is_test,
                                      drop_connect_rate,
                                      '_blocks.' + str(idx) + '.')

"
2	"        exit_schedule = {
            str((1, 0)): [0, 2],
        }

"
4	"

class EfficientNet():
"
1	"        postnet_dropout_rate (float, optional): Dropout rate in postnet.
        use_masking (bool, optional):
            Whether to apply masking for padded part in loss calculation.
        use_weighted_masking (bool, optional):
"
4	"            default=1,
            help=""batch size."")
        self.arg_config_group.add_argument(
            '--top_k',
            type=ast.literal_eval,
"
6	"        data.adj_t.storage.rowptr()
        data.adj_t.storage.csr2csc()

        return data
"
1	"    pids=() # initialize pids
    for rtask in ${recog_set}; do
    (
        decode_dir=decode_${rtask}_$(basename ${decode_config%.*})_${lmtag}
"
1	"        super(EncoderMix, self).__init__(
            idim=idim,
            selfattention_layer_type=""selfattn"",
            attention_dim=attention_dim,
"
2	"                self.assertEqual('-c', args[1])
                exit_code = safe_shell_exec.execute(args[2], env)
                self.assertEqual(0, exit_code)
            else:
                actual = _run(hargs)
"
4	"coffee mug
coffeepot
coil, spiral, volute, whorl, helix
combination lock
computer keyboard, keypad
"
0	"    )


"
2	"    automatic: true
  agents:
"
1	")
def test_ESPnetDataset_sound_scp(sound_scp):
    dataset = IterableESPnetDataset(
"
6	"            if match is None:
                raise TypeError(
"
2	"                np.array([[1.0, 2.0], [3.0, 4.0]]),
                np.array([0.0, 0.0])
"
4	"        name=name,
        input=input,
"
4	"tench, Tinca tinca
goldfish, Carassius auratus
"
3	"    height: 70%;
    width: 25%;
    display: -webkit-box;
"
4	"padlock
paintbrush
pajama, pyjama, pj's, jammies
palace
panpipe, pandean pipe, syrinx
"
4	"                initializer=fluid.initializer.TruncatedNormal(scale=0.02),
                trainable=trainable)
            dict_dim = 240466
            emb_1 = fluid.layers.embedding(
"
2	"    Make sure to use ``optimizer.skip_synchronize()`` if you're calling ``synchronize()``
    in your code.
"
6	"    def __init__(self, cluster_data, **kwargs):
        self.cluster_data = cluster_data
"
2	"namespace stream_executor {
namespace cuda {
cudaStream_t AsCUDAStreamValue(Stream* stream);
} // namespace stream_executor
} // namespace cuda
"
4	"        'r1_k3_s11_e1_i32_o16_se0.25',
        'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25',
"
2	"# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
"
4	"                padding_type=self.padding_type,
                name=name,
                conv_name=name + '_expand_conv',
"
0	"

def create_columns(bm, face, prop):
    if not prop.add_columns:
        return
"
4	"
import paddle.fluid as fluid
import paddlehub as hub
"
2	"        settings.verbose = 2

        with mock.patch('horovod.spark.task.gloo_exec_fn.task_exec') as task_exec:
            gloo_exec_fn.main(driver, settings)

"
2	"    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
    def test_auto_scale_spark_blacklist(self, setting, _):
"
4	"    Args:
        images (list[numpy.ndarray]): images data, shape of each is [H, W, C].
        paths (list[str]): paths to images.

    Yield:
"
4	"                loc=0.0, scale=stddev))
        if use_bias == True:
            bias_attr = fluid.ParamAttr(
                name=name + ""_offset"",
                initializer=fluid.initializer.Constant(0.0))
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
"
4	"cairn, cairn terrier
Australian terrier
Dandie Dinmont, Dandie Dinmont terrier
Boston bull, Boston terrier
"
4	"                   padding_type='SAME',
                   override_params=None,
                   use_se=True):
    model = EfficientNet(
        name='b5',
"
4	"mailbag, postbag
mailbox, letter box
"
4	"umbrella
unicycle, monocycle
"
4	"oil filter
organ, pipe organ
oscilloscope, scope, cathode-ray oscilloscope, CRO
overskirt
"
4	"
https://github.com/PaddlePaddle/PaddleClas

"
4	"                 stddev=0.02):
    if init == ""kaiming"":
        if op_type == 'conv':
            fan_in = input.shape[1] * filter_size * filter_size
        elif op_type == 'deconv':
"
1	"    parser = configargparse.ArgumentParser(
        description=""Transcribe text from speech using ""
"
2	"  - docker-compose#v2.6.0:
      run: test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
"
4	"head cabbage
broccoli
"
2	"    def stop(self):
        self._driver.shutdown_tasks()
        super(SparkRendezvousServer, self).stop()
"
2	"    def make_metadata_dictionary(_train_data_schema):
        _metadata = {}
"
4	"                         override_params=None,
                         use_se=False):
    model = EfficientNet(
        name='b0',
        is_test=is_test,
"
1	"        checkpoint: Union[str, Path],
        model: torch.nn.Module,
        reporter: Reporter,
        optimizers: Sequence[torch.optim.Optimizer],
"
2	"        hipify_python.hipify(
                project_directory=this_dir,
                output_directory=this_dir,
                includes=(""torch/*.cc"",""torch/*.h""),
                show_detailed=True,
"
1	"        :param torch.nn.Module rnnlm: language model module
        :return: N-best decoding results
        :rtype: list
        """"""
        if recog_args.ctc_weight > 0.0:
"
2	"  timeout_in_minutes: 10
  retry:
    automatic: true
  agents:
"
3	"_ENVELOPE.fields_by_name['status'].message_type = _STATUS
_STATUS_DETAILS.fields_by_name['time'].message_type = google_dot_protobuf_dot_timestamp__pb2._TIMESTAMP
"
2	"                client.abort_command()
                self.do_test_spark_task_service_executes_command(client, file)

"
4	"        conv = self._project_conv_norm(conv, block_args, is_test, name)

        # Skip connection and drop connect
"
4	"library
lifeboat
lighter, light, igniter, ignitor
limousine, limo
liner, ocean liner
"
1	"        x = text
        spemb = spembs

"
1	"        k = self.kernel_size

        # first liner layer
        x = self.linear1(x)
"
6	"            items = batch[key]
            item = items[0]
"
4	"            bias_attr = fluid.ParamAttr(
                name=name + '_offset',
"
4	"        return conv

"
4	"beacon, lighthouse, beacon light, pharos
beaker
bearskin, busby, shako
beer bottle
beer glass
"
3	"  name: my-mwu-encoder
  py_modules: mwu_encoder.py
  workspace: ./

"
2	"
from common import capture


"
4	"        self._bn_eps = self._global_params.batch_norm_epsilon
        self.padding_type = padding_type
"
2	")doc"");

REGISTER_KERNEL_BUILDER(
    Name(""HorovodLocalRank"").Device(DEVICE_CPU).HostMemory(""local_rank""),
"
2	"        rank = 0
        for host, slots in validated_list:
"
4	"                      padding_type=""SAME"",
                      conv_act=None,
                      bn_act='swish',
"
4	"    """""" Round number of filters based on depth multiplier. """"""
    multiplier = global_params.depth_coefficient
"
2	"
    # Pass secret key through the environment variables.
"
1	"import pytest

from espnet2.text.char_tokenizer import CharTokenizer

"
2	"
        # Merge in weather.
        weather = weather_csv.join(state_names_csv, weather_csv.file == state_names_csv.StateName)
"
0	"        srt_x = sorted([v.co for v in verts], key=lambda co: co.x)
        self.assertEqual((srt_x[-1] - srt_x[0]).x, prop.radius * 2)

        srt_y = sorted([v.co for v in verts], key=lambda co: co.y)
"
1	"    download_models
    cmvn=$(find ${download_dir}/${models} -name ""cmvn.ark"" | head -n 1)
fi
if [ -z ""${align_model}"" ]; then
    download_models
"
4	"    print(res)
    res = b0.classify(images=test_image)
    print(res)
"
3	"    parser.add_argument('path', type=str, help='path to the directory containing '
                                               'Dockerfile, manifest.yml, README.md'
                                               'zero or more yaml config, '
"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
"
4	"                epsilon=bn_eps,
                name=bn_name,
                moving_mean_name=bn_name + '_mean',
                moving_variance_name=bn_name + '_variance',
"
4	"        w, d, _, p = efficientnet_params(model_name)
        blocks_args, global_params = efficientnet(
"
2	"            notification_manager.remove_listener(state)
    return wrapper
"
4	"grand piano, grand
greenhouse, nursery, glasshouse
grille, radiator grille
grocery store, grocery, food market, market
guillotine
"
4	"        data_out (numpy.ndarray): output data of network.
        label_list (list): list of label.
        top_k (int): Return top k results.
    """"""
    output = []
"
3	"                 *args,
                 **kwargs):
        """"""
"
4	"```

### 查看代码

https://github.com/PaddlePaddle/PaddleClas
"
3	"        with f:
            f.index(input_fn, output_fn=test_pruned, callback_on_body=True)

"
3	"    display: flex;
    line-height: 34px;
    color: rgba(51, 51, 51, 0.5);
"
1	"        self.use_bias = use_bias
        if self.use_bias:
"
2	"        event = threading.Event()
        fn = mock.Mock()
"
2	"# Horovod: adjust learning rate based on number of GPUs.
lr = 0.001
opt = tf.optimizers.Adam(lr * hvd.size())


"
4	"catamaran
CD player
cello, violoncello
cellular telephone, cellular phone, cellphone, cell, mobile phone
chain
"
2	"  retry:
    automatic: true
  agents:
    queue: cpu
- label: ':fire: Test PyTorch MNIST (test-cpu-openmpi-py3_6-tf1_14_0-keras2_2_4-torch1_2_0-mxnet1_4_1-pyspark2_4_0)'
"
4	"        init=initial,
        use_bias=use_bias,
        filter_size=filter_size,
        stddev=stddev)

"
2	"    automatic: true
  agents:
    queue: 2x-gpu-g4
"
3	"      serialized_options=None,
    ),
    _descriptor.MethodDescriptor(
"
2	"                                                extra_mpi_args=args.mpi_args,
                                                key=secret.make_secret_key(),
                                                start_timeout=tmout,
                                                output_filename=args.output_filename,
                                                run_func_mode=args.run_func is not None,
"
4	"house finch, linnet, Carpodacus mexicanus
junco, snowbird
indigo bunting, indigo finch, indigo bird, Passerina cyanea
robin, American robin, Turdus migratorius
bulbul
"
1	"        ${decode_cmd} JOB=1:${nj} ${expdir}/${decode_dir}/log/decode.JOB.log \
            asr_recog.py \
            --config ${decode_config} \
"
4	"                output_filters=round_filters(block_arg.output_filters,
                                             self._global_params),
                num_repeat=round_repeats(block_arg.num_repeat,
                                         self._global_params))
"
1	"            array, rate = soundfile.read(wav, always_2d=self.always_2d)
        else:
"
2	"        return

    params = []
"
4	"                input_filters=round_filters(block_arg.input_filters,
                                            self._global_params),
                output_filters=round_filters(block_arg.output_filters,
"
3	"__copyright__ = ""Copyright (c) 2020 Jina AI Limited. All rights reserved.""
__license__ = ""Apache-2.0""

import numpy as np
"
4	"thimble
thresher, thrasher, threshing machine
throne
"
2	"    callbacks = {}
    occurrences = collections.defaultdict(int)

    # Returns the full type structure of the possibly nested objects for recursive casting back
"
2	"        finally:
            self._wait_cond.release()

"
2	"            assert results[0]['start_rank'] == 0
            assert results[0]['size'] == slots
            assert results[0]['hostname'] == 'localhost'

"
2	"                custom_objects = {
                    'TestOptimizer': lambda **kwargs: hvd.DistributedOptimizer(
                        TestOptimizer(**kwargs))
                }
                new_model = hvd.load_model(fname, custom_objects=custom_objects)
"
4	"
返回预处理的图片高度，也就是224。

```python
def get_pretrained_images_mean()
"
1	"        ).view(B, T, C)

"
4	"def save_inference_model(dirname,
                         model_filename=None,
                         params_filename=None,
"
1	"        self.linear1 = nn.Linear(n_feat, n_feat * 2)
        self.linear2 = nn.Linear(n_feat * 2, n_feat)
        self.act = nn.GLU()
"
2	"            logging.error('failure count == {} -> stop running'.format(self._size))
            self._driver.stop()
"
2	"    Allreduce operations are executed after each gradient is computed by ``loss.backward()``
    in parallel with each other. The ``step()`` method ensures that all allreduce operations are
    finished before applying gradients to the model.

    DistributedOptimizer exposes the ``synchronize()`` method, which forces allreduce operations
"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
"
4	"bannister, banister, balustrade, balusters, handrail
barbell
barber chair
"
2	"                         nic=nic,
                         hostname=slot_info.hostname,
                         local_rank=slot_info.local_rank)

            notification_receiver = NotificationReceiver()
"
2	"                psutil.Process(os.getpid()).kill()

"
2	"  - docker-compose#6b0df8a98ff97f42f4944dbb745b5b8cbf04b78c:
      build: test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2
      image-repository: 823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite
      cache-from: test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2:823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite:SLUG-test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2-latest
"
4	"Band Aid
banjo
bannister, banister, balustrade, balusters, handrail
"
4	"miniature poodle
standard poodle
"
2	"            value = (self._service.addresses(), secret_key)
            put_data_into_kvstore(rendezvous_addr,
                                  rendezvous_port,
                                  PUT_WORKER_ADDRESSES,
                                  self._create_id(hostname, local_rank),
"
2	"- label: ':pytest: Run PyTests (test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2)'
  command: bash -c "" cd /horovod/test && (echo test_*.py | sed 's/[a-z_]*tensorflow2[a-z_.]*//g' | sed 's/test_interactiverun.py//g' | sed 's/test_spark_keras.py//g' | sed 's/test_spark_torch.py//g' | sed 's/test_spark.py//g' | sed 's/test_run.py//g' | xargs -n 1 \$(cat /mpirun_command) pytest -v --capture=no) && pytest --forked -v --capture=fd test_spark.py test_run.py""
  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2
"
2	"# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
2	"    if env is not None and not isinstance(env, dict):
        raise Exception('env argument must be a dict, not {type}: {env}'
                        .format(type=type(env), env=env))

"
2	"    """"""
    Executes the given function in a separate thread when event is set.
    That threat can be stopped by setting the optional stop event.
    The stop event is check regularly every check_interval_seconds.
    Exceptions will silently be swallowed when silent is True.
"
1	"            ys_in, olens_in = ys, olens

"
2	"            else:
                env[key] = value


"
1	"        # initialize parameters
        self._reset_parameters(
"
4	"fire screen, fireguard
flagpole, flagstaff
flute, transverse flute
folding chair
"
2	"            thread.join(5)

    def shutdown(self):
"
4	"        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
"
2	"    queue: cpu
- label: ':tensorflow: Test TensorFlow 2.0 Keras MNIST (test-cpu-openmpi-py3_6-tf2_0_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark2_4_0)'
"
4	"        im_std = np.array([0.229, 0.224, 0.225]).reshape(1, 3)
        return im_std

    def _set_config(self):
"
2	"        start_timeout = int(os.getenv('HOROVOD_SPARK_START_TIMEOUT', '600'))

    # nics needs to be a set
"
2	"            return 0, time.time()

        driver.start(np=2, create_worker_fn=exec_command)
        res = driver.get_results().worker_results
"
3	"}

#left-side {
"
3	"html, body {
    margin: 0;
    padding: 0;
"
3	"  ],
  extensions=[
  ],
"
4	"                      use_cudnn=True,
                      use_bn=True,
                      bn_mom=0.9,
"
4	"
def cal_padding(img_size, stride, filter_size, dilation=1):
    """"""Calculate padding size.""""""
    if img_size % stride == 0:
"
3	"      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='pod_id', full_name='jina.Status.Details.pod_id', index=1,
      number=2, type=9, cpp_type=9, label=1,
"
2	"
        host_slots = {}
        lines = set(stdout.getvalue().strip().split('\n'))
        for line in lines:
            host = line
"
4	"ballpoint, ballpoint pen, ballpen, Biro
Band Aid
"
2	"        #  None type parameter values
        raise ValueError('cannot broadcast torch.optim.LBFGS state')

    state_dict = optimizer.state_dict()

"
4	"jean, blue jean, denim
jeep, landrover
jersey, T-shirt, tee shirt
jigsaw puzzle
"
2	"            if state.batch % args.batches_per_commit == 0:
                state.commits += 1
                state.commit()

        if hvd.rank() == 0:
"
4	"jeep, landrover
jersey, T-shirt, tee shirt
jigsaw puzzle
"
2	"            if state.batch % args.batches_per_commit == 0:
                state.commits += 1
                state.commit()
"
4	"snow leopard, ounce, Panthera uncia
jaguar, panther, Panthera onca, Felis onca
lion, king of beasts, Panthera leo
"
4	"    return model


def EfficientNetB1(is_test=False,
"
2	"
    def test_spark_driver_host_discovery(self):
        with spark_driver_service(num_proc=4) as (driver, client, _):
"
4	"import numpy as np
import paddlehub as hub
"
2	"
    def zero_grad(self):
        if self._handles:
            raise AssertionError(""optimizer.zero_grad() was called after loss.backward() ""
                                 ""but before optimizer.step() or optimizer.synchronize(). ""
"
3	"_DOCUMENT_TAGSENTRY = _descriptor.Descriptor(
  name='TagsEntry',
  full_name='jina.Document.TagsEntry',
"
1	"        )
        if self.use_guided_attn_loss:
"
1	"from espnet.nets.pytorch_backend.transducer.vgg import VGG2L
from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention
from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding
from espnet.nets.pytorch_backend.transformer.encoder import Encoder
from espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer
"
4	"                   padding_type='SAME',
                   override_params=None,
                   use_se=True):
    model = EfficientNet(
"
2	"
    return hh.host_hash(salt='-'.join(salt))


"
4	"                self._expand_conv_norm(conv, block_args, is_test, name))

"
2	"        self.backend = backend
        self.state = state
        self.batches_per_commit = batches_per_commit
"
4	"            title=""Config options"",
            description=
            ""Run configuration for controlling module behavior, not required."")
"
2	"            'Please, make sure you are running on a cluster with jsrun installed or '
            'use one of the other launchers.')

    if nics and 'NCCL_SOCKET_IFNAME' not in env:
"
6	"            edge_attr = edge_attr.repeat(1, self.towers, 1)
            h = torch.cat([x_i, x_j, edge_attr], dim=-1)
        else:
"
5	"        layout.createText(s.foundations[0], 'ne')
        x, y = layout.XM, layout.YM+layout.YS
"
3	"                                        environment=_env)

        share_link = f'https://api.jina.ai/hub/?jh={urllib.parse.quote_plus(name)}'

        try:
"
2	"            .withColumn('Year', F.year(df.Date)) \
            .withColumn('Month', F.month(df.Date)) \
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
"
1	"def pyopenjtalk_g2p(text) -> List[str]:
    import pyopenjtalk

    # phones is a str object separated by space
    phones = pyopenjtalk.g2p(text, kana=False)
"
2	"  command: bash -c ""\$(cat /oneccl_env) && echo '/mpirun_command_ofi' > /mpirun_command && /etc/init.d/ssh start && cd /horovod/test/integration && pytest --forked -v --capture=fd test_static_run.py""
  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
"
4	"def EfficientNetB1(is_test=False,
                   padding_type='SAME',
                   override_params=None,
                   use_se=True):
"
1	"
class DynamicConvolution(nn.Module):
    """"""Dynamic Convolution layer.

"
2	"parser.add_argument('--num-iters', type=int, default=10,
                    help='number of benchmark iterations')
"
2	"  }

  if (nccl_async_err != ncclSuccess) {
    ncclCommAbort(*nccl_comm_);
    throw std::logic_error(std::string(""NCCL async error: "") + ncclGetErrorString(nccl_async_err));
"
4	"
        conv = self.conv_bn_layer(
            inputs,
            num_filters=oup,
"
2	"                wait_for_one(events)
                return 1, time.time()

            driver.record_ready(slot_info.hostname, slot_info.local_rank)
"
2	"                                  value)

    def register_listener(self, listener):
        self._listeners.add(listener)
"
1	"    dataset = IterableESPnetDataset(
        path_name_type_list=[(shape_file, ""data5"", ""rand_float"")],
"
2	"
        exit_schedule = {
            str((1, 0)): [1],
"
4	"briard
kelpie
komondor
"
4	"            fan_in = fan_out * filter_size * filter_size
        else:
"
2	"  - ecr#v1.2.0:
      login: true
"
4	"            each['org_im_path'] = im_path
            each['org_im'] = Image.open(im_path)
"
4	"                emb_3 = fluid.embedding(
                    input=text_3,
"
4	"skunk, polecat, wood pussy
badger
"
4	"jack-o'-lantern
jean, blue jean, denim
"
4	"        raise NotImplementedError(""norm tyoe: [%s] is not support"" % norm_type)


def conv2d(input,
           num_filters=64,
"
4	"        images (list[numpy.ndarray]): images data, shape of each is [H, W, C].
        paths (list[str]): paths to images.

"
4	"military uniform
milk can
minibus
"
2	"                    try:
                        client.run_command(command, command_env)
                        client.wait_for_command_termination()
                    finally:
"
2	"class SparkDriverHostDiscovery(HostDiscovery):
    def __init__(self, driver):
        """"""
"
4	"    data = cv2.imencode('.jpg', image)[1]
    return base64.b64encode(data.tostring()).decode('utf8')
"
4	"soup bowl
space bar
space heater
space shuttle
"
4	"            self.assertTrue(res.keys(), results_2[index].keys())
            diff = list(res.values())[0] - list(results_2[index].values())[0]
            self.assertTrue((diff < 1e-5))

        test_images = [cv2.imread(img) for img in self.test_images]
"
4	"
* trainable (bool): 计算图的参数是否为可训练的；
* pretrained (bool): 是否加载默认的预训练模型。
"
2	"    rendezvous.init(host_alloc_plan)
    run_command = get_run_command(command, server_ip, nics, global_rendezv_port)

    slot_info_to_command = _slot_info_to_command_fn(run_command, env)
"
2	"    """"""
    Actually tests that horovod.spark.run invokes mpi_run properly.

"
2	"      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
4	"            gpu_config.disable_glog_info()
            gpu_config.enable_use_gpu(
                memory_pool_init_size_mb=1000, device_id=0)
"
4	"        return conv

    def _project_conv_norm(self, inputs, block_args, is_test, name=None):
        final_oup = block_args.output_filters
"
4	"llama
weasel
"
2	"                      '{local_command}'\
                .format(host=host_name,
"
4	"rubber eraser, rubber, pencil eraser
rugby ball
rule, ruler
running shoe
safe
"
5	"        w1, w2 = 8*layout.XS+layout.XM, 2*layout.XS
        if w2 + 52*layout.XOFFSET > w1:
"
4	"__all__ = ['reader']

"
2	"import sys

"
2	"    MPI implementation specific command line flags. Patching this method allows to test mpi_run
    without an MPI implementation to be installed.

"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
"
0	"
        # Add the module to the system module cache.
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
"
4	"# -*- coding:utf-8 -*-
# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
"
4	"        block_args_copy = copy.deepcopy(self._blocks_args)
        idx = 0
        block_size = 0
"
4	"European gallinule, Porphyrio porphyrio
American coot, marsh hen, mud hen, water hen, Fulica americana
bustard
ruddy turnstone, Arenaria interpres
red-backed sandpiper, dunlin, Erolia alpina
"
1	"        use_kernel_mask=False,
        use_bias=False,
"
4	"# 发送HTTP请求
data = {'images':[cv2_to_base64(cv2.imread(""/PATH/TO/IMAGE""))]}
"
4	"red wine
espresso
"
2	"                             size=-1, local_size=-1, cross_size=-1)


"
4	"rocking chair, rocker
rotisserie
rubber eraser, rubber, pencil eraser
rugby ball
"
3	"    height: 54px;
    background-color: #009999;
"
2	"        discover_hosts = discovery.HostDiscoveryScript(args.host_discovery_script, args.slots)
    elif args.hosts:
        _, available_host_slots = hosts.parse_hosts_and_slots(args.hosts)
"
2	"        tf.config.experimental.set_memory_growth(gpu, True)
    if gpus:
"
2	"        raise ValueError('args must be a tuple, not {}, for a single argument use (arg,)'
                         .format(type(args)))

    if silent:
"
4	"                   use_se=True):
    model = EfficientNet(
        name='b2',
"
1	"):
    model = Tacotron2(
"
3	"        self._greetings = greetings
        self.logger.success('look at me! %s' % greetings)

    def encode(self, data: Any, *args, **kwargs) -> Any:
"
2	"        if not nics:
            raise Exception('Unable to find a set of common task-to-task communication interfaces: %s'
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
"
4	"            groups=num_groups,
            act=conv_act,
            padding_type=padding_type,
            use_cudnn=use_cudnn,
"
1	"                preprocess=preprocess,
                float_dtype=float_dtype,
                int_dtype=int_dtype,
            )
"
2	"    @mock.patch('horovod.run.gloo_run._get_min_start_hosts', return_value=1)
    def test_auto_scale_up(self, mock_get_min_start_hosts):
        discovery_schedule = [
            (0, ['host-1:1']),
"
2	"    automatic: true
  agents:
    queue: 2x-gpu-g4

"
2	"    pred_df = pred_df.withColumn('label_pred', argmax(pred_df.label_prob))
    evaluator = MulticlassClassificationEvaluator(predictionCol='label_pred', labelCol='label', metricName='accuracy')
    print('Test accuracy:', evaluator.evaluate(pred_df))

    spark.stop()
"
3	"    $(""#fourth"").addClass(""active"");
})


hljs.initHighlightingOnLoad();
"
4	"            inputs (dict): key is 'image', corresponding vaule is image tensor.
            outputs (dict): key is :
"
4	"jellyfish
sea anemone, anemone
brain coral
"
6	"        if self.name == 'CSL':
            return ['data.pt']
        else:
            return ['train_data.pt', 'val_data.pt', 'test_data.pt']
"
2	"    train_data_schema = train_data.schema.to_arrow_schema()
    train_rows, train_data_total_byte_size = _get_dataset_info(train_data, 'training',
                                                               train_data_path)

    # Write train metadata to filesystem
"
4	"parachute, chute
parallel bars, bars
park bench
parking meter
"
2	"

"
4	"    if need_crop:
        conv = conv[:, :, 1:, 1:]

    if norm is not None:
"
4	"curly-coated retriever
golden retriever
Labrador retriever
Chesapeake Bay retriever
"
2	"  agents:
    queue: cpu
- label: ':muscle: Test MXNet MNIST (test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2)'
  command: bash -c "" OMP_NUM_THREADS=1 \$(cat /mpirun_command) python /horovod/examples/mxnet_mnist.py""
  plugins:
"
2	"#
#     http://www.apache.org/licenses/LICENSE-2.0
"
4	"        raise NotImplementedError(""norm tyoe: [%s] is not support"" % norm_type)


def conv2d(input,
           num_filters=64,
"
4	"gown
grand piano, grand
greenhouse, nursery, glasshouse
grille, radiator grille
"
2	"        def hook(*ignore):
            if p in self._handles and self._handles[p][0] is not None:
"
2	"        if not mpi_available():
            self.skipTest(""MPI is not available"")
"
4	"        conv = fluid.layers.tanh(conv, name=name + '_tanh')
    elif act == 'sigmoid':
        conv = fluid.layers.sigmoid(conv, name=name + '_sigmoid')
    elif act == 'swish':
"
4	"#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
"
4	"        ]
        self.true_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3).tolist()
        self.true_std = np.array([0.229, 0.224, 0.225]).reshape(1, 3).tolist()

"
2	"  plugins:
  - docker-compose#v2.6.0:
"
4	"        """"""Encodes a block to a string.""""""
        args = [
            'r%d' % block.num_repeat,
"
4	"lacewing, lacewing fly
dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk
damselfly
admiral
"
4	"oxygen mask
packet
paddle, boat paddle
paddlewheel, paddle wheel
"
4	"            out = postprocess(
                data_out=predictor_output[0].as_ndarray(),
                label_list=self.label_list,
"
4	"                num_repeat=round_repeats(block_args.num_repeat,
                                         self._global_params))

            # The first block needs to take care of stride and filter size increase.
"
2	"
        self.assertEqual(2, len(results))

"
1	"
if [ ${stage} -le 1 ] && [ ${stop_stage} -ge 1 ]; then
    ### Task dependent. You have to design training and dev sets by yourself.
    ### But you can utilize Kaldi recipes in most cases
"
6	"    t = '(Tensor, SparseTensor, OptTensor) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert torch.allclose(conv(x, adj.t()), out, atol=1e-6)
"
4	"

def EfficientNetB0(is_test=False,
                   padding_type='SAME',
                   override_params=None,
"
0	"            obj.face_maps.remove(fmap)

"
4	"**NOTE:** 如使用GPU预测，则需要在启动服务之前，请设置CUDA\_VISIBLE\_DEVICES环境变量，否则不用设置。

## 第二步：发送预测请求

"
2	"
    if start_timeout is None:
        # Lookup default timeout from the environment variable.
"
4	"                                            self._global_params),
                output_filters=round_filters(block_args.output_filters,
                                             self._global_params),
"
2	"    :return: driver ip. We make sure all workers can connect to this ip.
    """"""
"
3	"    # @@protoc_insertion_point(class_scope:jina.Chunk.TagsEntry)

"
1	"    [(None, ""add""), (2, ""add""), (2, ""concat"")],
)
@pytest.mark.parametrize(""loss_type"", [""L1+L2"", ""L1""])
"
1	"                batch_size=args.batch_size,
                batch_bins=args.batch_bins,
                batch_type=args.batch_type,
"
2	"if LooseVersion(pyspark.__version__) < LooseVersion('3.0.0'):
    from pyspark.ml.feature import OneHotEncoderEstimator as OneHotEncoder
else:
    from pyspark.ml.feature import OneHotEncoder
"
4	"Blenheim spaniel
papillon
toy terrier
Rhodesian ridgeback
Afghan hound, Afghan
"
4	"            conv, alpha=relufactor, name=name + '_leaky_relu')
    elif act == 'tanh':
        conv = fluid.layers.tanh(conv, name=name + '_tanh')
"
5	"    import black_hole_solver
    bh_solve_lib_obj = black_hole_solver.BlackHoleSolver()
    use_bh_solve_lib = True
except BaseException:
"
3	"!NmslibIndexer
with:
  ref_indexer:
    !NumpyIndexer
    metas:
"
2	"            bn.cuda(hvd.local_rank())

            ts = ts.cuda(hvd.local_rank()).float()
            ts1 = ts.clone().requires_grad_()
            ts2 = ts.clone().requires_grad_()
"
1	"            hyps = remained_hyps
            if len(hyps) > 0:
                logging.debug(""remeined hypothes: "" + str(len(hyps)))
"
1	"        f""--vocab_size={vocabsize} ""
        f""--model_prefix={model_prefix} ""
        f""--input_sentence_size={input_sentence_size}""
    )
    sp = spm.SentencePieceProcessor()
"
4	"        w_start = (width - size) / 2
        h_start = (height - size) / 2
    else:
        w_start = np.random.randint(0, width - size + 1)
"
3	"
import numpy as np

from ..frameworks import BaseTextTFEncoder
"
4	"cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM
cassette
cassette player
"
1	"            x.view(1, B * T, C), weight_f, padding=self.padding_size, groups=B * T
        )
        xf = xf.view(B, T, C)

"
2	"    return ','.join(hosts)

"
2	"                    tensor = tensor % 2
                tensor = tf.cast(tensor, dtype=dtype)
                gathered = hvd.allgather(tensor)

"
2	"        hvd.init()

        with tf.device(""/cpu:0""):
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
4	"beaver
guinea pig, Cavia cobaya
sorrel
zebra
hog, pig, grunter, squealer, Sus scrofa
"
2	"  plugins:
  - docker-compose#6b0df8a98ff97f42f4944dbb745b5b8cbf04b78c:
"
6	"
        x = global_add_pool(x, batch)
        return self.mlp(x)


"
3	"

class MyTestCase(JinaTestCase):
"
2	"                                              get_rank=rank,
                                              **kwargs)

    def save(self):
"
2	"#
#     http://www.apache.org/licenses/LICENSE-2.0
#
"
4	"import numpy as np
import paddle.fluid as fluid
"
2	"  retry:
    automatic: true
  agents:
    queue: cpu
"
4	"```

## API
"
4	"remote control, remote
restaurant, eating house, eating place, eatery
revolver, six-gun, six-shooter
rifle
"
2	"        if LooseVersion(pa.__version__) < LooseVersion('0.17.0'):
            self._hdfs_kwargs['driver'] = driver
"
4	"miniature poodle
standard poodle
"
1	"import pytest

"
2	"        if len(unnamed_param_ids):
            raise ValueError('named_parameters was specified, but one or more model '
                             'parameters were not named. Python object ids: '
"
3	"    gp5.add_argument('--skip-on-error', type=OnErrorSkip.from_string, choices=list(OnErrorSkip),
                     default=OnErrorSkip.NONE,
                     help='skip strategy on error message. ')
"
2	"        self.local_rank = local_rank
        """"""Local rank.""""""
"
0	"            object_contents = git(""show"", object_hash)
            item.date_time = time.gmtime(timestamp)[:6]
            out.writestr(item, object_contents)
"
3	"    $(""#first"").removeClass(""active"");
    $(""#second"").removeClass(""active"");
    $(""#third"").removeClass(""active"");
"
2	"    elif args.num_proc:
        conf.setMaster('local[{}]'.format(args.num_proc))
    spark = SparkSession.builder.config(conf=conf).getOrCreate()

    # Setup our store for intermediate data
"
1	"export PATH=$PWD/utils/:$KALDI_ROOT/tools/openfst/bin:$KALDI_ROOT/tools/sctk/bin:$PWD:$PATH
[ ! -f $KALDI_ROOT/tools/config/common_path.sh ] && echo >&2 ""The standard file $KALDI_ROOT/tools/config/common_path.sh is not present -> Exit!"" && exit 1
. $KALDI_ROOT/tools/config/common_path.sh
export LC_ALL=C
"
4	"                   override_params=None,
                   use_se=True):
    model = EfficientNet(
        name='b1',
"
4	"

def EfficientNetB3(is_test=False,
"
4	"                moving_variance_name=bn_name + '_variance',
                param_attr=param_attr,
                bias_attr=bias_attr)

"
4	"        loop_num = int(np.ceil(total_num / batch_size))

        res = list()
        for iter_id in range(loop_num):
"
4	"doormat, welcome mat
drilling platform, offshore rig
drum, membranophone, tympan
"
1	"        self.dtype = dtype

        self.data = {}
"
4	"rugby ball
rule, ruler
running shoe
safe
safety pin
"
4	"            diff = list(res.values())[0] - list(results_2[index].values())[0]
            self.assertTrue((diff < 1e-5))

        test_images = [cv2.imread(img) for img in self.test_images]
"
4	"basenji
pug, pug-dog
Leonberg
Newfoundland, Newfoundland dog
Great Pyrenees
"
2	"    task_indices = driver_client.task_host_hash_indices(host_hash)
    task_index = task_indices[local_rank]
    task_addresses = driver_client.all_task_addresses(task_index)
    task_client = task_service.SparkTaskClient(task_index, task_addresses, key, verbose=verbose)
"
4	"bookcase
bookshop, bookstore, bookstall
"
2	"      image-repository: 823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite
      cache-from: test-gpu-openmpi-py3_6-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_6_0-pyspark2_4_0:823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite:SLUG-test-gpu-openmpi-py3_6-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_6_0-pyspark2_4_0-latest
      config: docker-compose.test.yml
"
3	"                t.join()

            success = status_codes.count(200)
"
3	"__copyright__ = ""Copyright (c) 2020 Jina AI Limited. All rights reserved.""
__license__ = ""Apache-2.0""

"
4	"microphone, mike
microwave, microwave oven
military uniform
"
1	"
            fig.tight_layout()
            fig.savefig(output_dir / f""probs/{key}.png"")
            fig.clf()

"
2	"        hargs.np = 4
        hargs.hosts = ','.join(['{}:2'.format(host) for host in hosts])
"
1	"
local/featprepare.py

"
1	"        if shuffle is None:
            shuffle = self.shuffle

"
4	"scorpion
black and gold garden spider, Argiope aurantia
barn spider, Araneus cavaticus
garden spider, Aranea diademata
black widow, Latrodectus mactans
"
4	"coffeepot
coil, spiral, volute, whorl, helix
combination lock
computer keyboard, keypad
"
4	"            offset_name = name + ""_offset""
        scale_param = fluid.ParamAttr(
            name=scale_name,
            initializer=fluid.initializer.Constant(1.0),
"
4	"refrigerator, icebox
remote control, remote
restaurant, eating house, eating place, eatery
revolver, six-gun, six-shooter
"
4	"        height_padding = bottom_padding
        width_padding = right_padding
        if top_padding != bottom_padding or left_padding != right_padding:
            height_padding = top_padding + stride
            width_padding = left_padding + stride
"
4	"scoreboard
screen, CRT screen
"
4	"malinois
briard
kelpie
komondor
Old English sheepdog, bobtail
"
1	"        )

"
2	"    ]))
# Horovod: use DistributedSampler to partition the test data.
test_sampler = torch.utils.data.distributed.DistributedSampler(
    test_dataset, num_replicas=hvd.size(), rank=hvd.rank())
"
2	"        self.max_np = None
        self.slots = None
"
2	"parser = argparse.ArgumentParser(description='TensorFlow Synthetic Benchmark',
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('--fp16-allreduce', action='store_true', default=False,
                    help='use fp16 compression during allreduce')
"
4	"        self.default_pretrained_model_path = os.path.join(
            self.directory, ""efficientnetb2_imagenet_infer_model"")
        label_file = os.path.join(self.directory, ""label_list.txt"")
        with open(label_file, 'r', encoding='utf-8') as file:
"
4	"volcano
ballplayer, baseball player
groom, bridegroom
"
6	"    t = '(OptPairTensor, Tensor, OptTensor, Size) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert jit((x1, x2), edge_index, value).tolist() == out1.tolist()
    assert jit((x1, x2), edge_index, value,
               size=(4, 2)).tolist() == out1.tolist()
"
0	"        col = layout.column(align=True)
        col.row(align=True).prop(self, ""type"", expand=True)
"
4	"horse cart, horse-cart
hourglass
iPod
iron, smoothing iron
jack-o'-lantern
"
2	"      login: true
  timeout_in_minutes: 10
  retry:
    automatic: true
"
1	"            if k in data:
                raise RuntimeError(f""{k} is duplicated ({path}:{linenum})"")
"
2	"    "":fire: Single PyTorch MNIST (${test})"" \
    ""bash -c \""${oneccl_env} python /horovod/examples/pytorch_mnist.py --epochs 3\""""
"
3	"            *args,
            **kwargs):
        """"""
        :param model_url: the url of the model (TensorFlow Hub). For supported models see
"
4	"        assert type(images), ""images is a list.""
        for im in images:
            each = OrderedDict()
            each['org_im'] = Image.fromarray(im[:, :, ::-1])
"
1	"        use_kernel_mask=False,
        use_bias=False,
    ):
        """"""Construct Lightweight Convolution layer.""""""
        super(LightweightConvolution, self).__init__()
"
2	"      run: test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
4	"damselfly
admiral
"
2	"# Changelog

All notable changes to this project will be documented in this file.

"
3	"
    .. note::
        ``if`` accepts a single expression. It can be used for filter doc and chunk
"
4	"        name='b5',
        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
"
4	"                emb_name_list.append(emb_2_name)

"
4	"
def round_filters(filters, global_params):
"
4	"        x -= tmp.reshape((x.shape[0], 1))
        x = np.exp(x)
"
4	"                name_prefix = '@HUB_{}@'.format(self.name)
                inputs = {'image': name_prefix + image.name}
                outputs = {
"
4	"            vocab[parts[0]] = int(parts[1])

    return vocab
"
2	"        broadcast_object_fn = broadcast_object if not backend or _executing_eagerly() else broadcast_object_with_session

        super(TensorFlowKerasState, self).__init__(bcast_object=broadcast_object_fn,
                                                   get_rank=rank,
                                                   **kwargs)
"
1	"    y, _ = layer(x, torch.LongTensor([100]))
    y = y.numpy()[0]
    y2 = np.log10(spectrogram(x[0].numpy(), n_fft=16, n_shift=4))
    np.testing.assert_allclose(y, y2, rtol=0, atol=1e-4)

"
4	"goblet
go-kart
"
6	"                tree[i + 1] = tree[i + 1][:-1]
            elif e == 'Union' and 'NoneType' in n:
"
4	"        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
        use_se=use_se)
    return model
"
4	"                image = fluid.layers.data(
                    name=""image"", shape=[3, 224, 224], dtype=""float32"")
"
2	"    finally:
        driver.shutdown()


def get_common_interfaces(settings, all_host_names, remote_host_names=None, fn_cache=None):
"
4	"            act='swish',
            name=name + '_se_reduce')
        x_squeezed = conv2d(
            x_squeezed,
            num_filters=oup,
"
4	"# limitations under the License.

"
4	"            num_filters=num_squeezed_channels,
            filter_size=1,
            use_bias=True,
            padding_type=self.padding_type,
"
6	"
    t = '(PairOptTensor, PairTensor, PairTensor, Tensor) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
"
2	"void Controller::Initialize() {
  response_cache_.clear();
"
4	"### 查看代码

"
2	"                for r in rows:
                    if last_store != r.Store:
                        last_store = r.Store
"
2	"The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).

"
2	"    def expand_date(df):
        df = df.withColumn('Date', df.Date.cast(T.DateType()))
        return df \
"
2	"  retry:
    automatic: true
  agents:
    queue: cpu
- label: ':muscle: Test MXNet MNIST (test-cpu-mpich-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
"
6	"
        if hyperedge_index.numel() == 0:
            num_edges = 0
"
4	"    data = base64.b64decode(b64str.encode('utf8'))
    data = np.fromstring(data, np.uint8)
    data = cv2.imdecode(data, cv2.IMREAD_COLOR)
    return data

"
2	"  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
"
2	"parser.add_argument('--num-warmup-batches', type=int, default=10,
                    help='number of warm-up batches that don\'t count towards benchmark')
parser.add_argument('--num-batches-per-iter', type=int, default=10,
"
1	"        >>> writer = SoundScpWriter('./data/', './data/feat.scp')
        >>> writer['aa'] = 16000, numpy_array
"
4	"    def setUp(self):
        self.module = hub.Module(name='efficientnetb3_imagenet')
        self.test_images = [
            ""../image_dataset/classification/animals/dog.jpeg"",
"
2	"class HostsUpdatedInterrupt(RuntimeError):
    """"""Internal interrupt event indicating that the set of hosts in the job has changed.
"
2	"            # either terminate on task shutdown or command termination
            shutdown_thread = in_thread(driver_client.wait_for_task_shutdown)

            while shutdown_thread.is_alive():
                # Once the command started we wait for its termination
"
1	")
def test_ESPnetDataset_feats_scp(feats_scp,):
    dataset = IterableESPnetDataset(
"
2	"    # Data reader parameters
    train_reader_worker_count = estimator.getTrainReaderNumWorker()
    val_reader_worker_count = estimator.getValReaderNumWorker()

"
6	"    x = torch.randn(4, 16)
    edge_index = torch.tensor([[0, 1, 2, 3], [0, 0, 1, 1]])
    row, col = edge_index
    adj = SparseTensor(row=row, col=col, sparse_sizes=(4, 4))

"
6	"        self.add_self_loops = add_self_loops

        self._cached_edge_index = None
        self._cached_adj_t = None
"
6	"        torch.save(self.collate([data]), self.processed_paths[0])

    def __repr__(self):
        return f'{self.__class__.__name__}()'

"
2	"    else:
        if settings.verbose >= 1:
            logging.info('Running %d processes...', num_proc)
"
1	"        labels = make_pad_mask(olens - 1).to(ys.device, ys.dtype)
        labels = F.pad(labels, [0, 1], ""constant"", 1.0)
"
4	"boathouse
bobsled, bobsleigh, bob
bolo tie, bolo, bola tie, bola
bonnet, poke bonnet
"
4	"# You may obtain a copy of the License at
#
"
4	"            seq_len_used = fluid.layers.squeeze(seq_len, axes=[1])

            # Add embedding layer.
"
1	"MIN_VALUE = float(numpy.finfo(numpy.float32).min)


class LightweightConvolution(nn.Module):
    """"""Lightweight Convolution layer.
"
6	"                out = mean_squares - mean * mean
                if aggregator == 'std':
                    out = torch.sqrt(torch.relu(out) + 1e-5)
            else:
"
2	"  retry:
    automatic: true
  agents:
    queue: 2x-gpu-g4
"
2	">     else:
>         if args.processing_master:
>             conf.setMaster(args.processing_master)

"
4	"kimono
knee pad
knot
lab coat, laboratory coat
ladle
"
2	"                        ssh_port_arg=ssh_port_arg,
                        local_command=quote('cd {pwd} > /dev/null 2>&1 ; {local_command}'
"
2	"    try:
        # wait for all the hosts to register with the service service.
        if settings.verbose >= 2:
            print('Waiting for the hosts to acknowledge.')
"
1	"
    def __init__(
        self,
        # network structure related
        idim: int,
"
4	"    type=""CV/image_classification"",
    author=""paddlepaddle"",
    author_email=""paddle-dev@baidu.com"",
    summary=
"
4	"        'r3_k5_s11_e6_i80_o112_se0.25',
        'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
"
4	"packet
paddle, boat paddle
paddlewheel, paddle wheel
padlock
"
4	"motor scooter, scooter
mountain bike, all-terrain bike, off-roader
mountain tent
"
3	"import glob
import tempfile
import urllib.parse
import webbrowser
"
4	"            ""../image_dataset/classification/animals/dog.jpeg"",
            ""../image_dataset/keypoint_detection/girl2.jpg""
        ]
        self.true_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3).tolist()
        self.true_std = np.array([0.229, 0.224, 0.225]).reshape(1, 3).tolist()
"
4	"
```python
"
1	"
    def __repr__(self):
"
1	"            help=""kernel size for lightweight/dynamic convolution: ""
            'Encoder side. For example, ""21_23_25"" means kernel length 21 for '
            ""First layer, 23 for Second layer and so on."",
        )
"
4	"def round_filters(filters, global_params):
    """""" Calculate and round number of filters based on depth multiplier. """"""
    multiplier = global_params.width_coefficient
"
2	"        for name, (exit_code, timestamp) in res.items():
            exit_code_sum += exit_code
"
1	"
            if args.valid_batch_size is None:
                args.valid_batch_size = args.batch_size

"
2	"    ]

"
2	"            run_func = None
        else:
            self.fail('unknown run argument {}'.format(run))

        with self.horovod_args(mode, controller, run_func=run_func, command=command) as (hargs, exec):
"
2	"parser.add_argument('--discovery-wait', type=int, default=3,
                    help='number of seconds the worker waits for an expected host discovery')
parser.add_argument('--exit-schedule', default='{}',
                    help='JSON string mapping from (epoch, batch) to list of ranks to exit at that time')
"
4	"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from __future__ import absolute_import
"
4	"Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis
African crocodile, Nile crocodile, Crocodylus niloticus
American alligator, Alligator mississipiensis
triceratops
"
4	"spider monkey, Ateles geoffroyi
squirrel monkey, Saimiri sciureus
Madagascar cat, ring-tailed lemur, Lemur catta
indri, indris, Indri indri, Indri brevicaudatus
"
6	"    assert out2.size() == (4, 32)
    assert torch.allclose(conv(x, adj2.t()), out2, atol=1e-6)

    t = '(Tensor, Tensor, OptTensor) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
"
2	"    def wait_for_command_termination(self):
        """"""
        Waits for command termination. Ensures this method takes at least
        self._minimum_command_lifetime_s seconds to return after command started.
"
2	"  plugins:
  - docker-compose#v2.6.0:
"
2	"    queue: 4x-gpu-g4
- label: ':pytest: Run PyTests (test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
"
0	"    create_object_material,
    bmesh_from_active_object,
    set_material_for_active_facemap,
)

"
2	"    # Save data frames as Parquet files.
    train_df.write.parquet('%s/train_df.parquet' % args.data_dir, mode='overwrite')
    val_df.write.parquet('%s/val_df.parquet' % args.data_dir, mode='overwrite')
    test_df.write.parquet('%s/test_df.parquet' % args.data_dir, mode='overwrite')

"
2	"
def _default_session():
    return ops.get_default_session() if not _IS_TF2 else None
"
2	"    response_list.add_response(std::move(response));
    LOG(TRACE) << ""Created response of size "" << tensor_size;
"
2	"                    value = i % 2

                # tf.equal() does not support tf.uint16 as of TensorFlow 1.2,
                # so need to cast rank_tensor to tf.int32.
"
4	"        x_squeezed = conv2d(
            x_squeezed,
            num_filters=oup,
            filter_size=1,
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: cpu
"
2	"#     http://www.apache.org/licenses/LICENSE-2.0
#
"
4	"                list(main_program.global_block().vars.keys()))
            prefix_name = ""@HUB_{}@"".format(self.name)
            add_vars_prefix(
                program=main_program, prefix=prefix_name, vars=variable_names)
"
4	"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"
4	"    'depth_coefficient',
    'depth_divisor',
    'min_depth',
"
2	"- label: ':fire: Single PyTorch MNIST (test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""\$(cat /oneccl_env) && echo '/mpirun_command_ofi' > /mpirun_command && python /horovod/examples/pytorch_mnist.py --epochs 3""
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
"
2	"    return spark_thread


def _launch_job(use_mpi, use_gloo, settings, driver, env, stdout=None, stderr=None):
    nics = driver.get_common_interfaces()
"
4	"dugong, Dugong dugon
sea lion
Chihuahua
Japanese spaniel
Maltese dog, Maltese terrier, Maltese
"
1	"    return message

"
2	"        import os
        from petastorm import make_batch_reader
        from petastorm.tf_utils import make_petastorm_dataset
"
4	"plastic bag
plate rack
plow, plough
plunger, plumber's helper
Polaroid camera, Polaroid Land camera
"
3	"    if_expression = 'doc.doc_id % 2 ==0'

"
4	"    blocks_args = BlockDecoder.decode(blocks_args)

    global_params = GlobalParams(
        batch_norm_momentum=0.99,
"
3	"        from sklearn.decomposition import FastICA
        if not self.model:
            self.model = FastICA(
"
4	"            trainable=True)
        offset_param = fluid.ParamAttr(
            name=offset_name,
"
6	"from typing import Union, Tuple, List
from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size
"
4	"desktop computer
dial telephone, dial phone
diaper, nappy, napkin
digital clock
digital watch
"
2	"                driver.record_ready(slot_info.hostname, slot_info.local_rank)

            driver.record_ready(slot_info.hostname, slot_info.local_rank)
            updated_slot_info = driver.get_slot_info(slot_info.hostname, slot_info.local_rank)
"
4	"                epsilon=bn_eps,
                name=bn_name,
                moving_mean_name=bn_name + '_mean',
"
1	"        for line in lines:
            key = line.rstrip().split(maxsplit=1)[0]
            if prev_key is not None and prev_key != key:
"
0	"            btools.utils.restricted_offset([1, 1], [0.5, 0.5], [0, 0]), (0, 0))

"
2	"            return

        if exit_code == 0:
            rendezvous_id = self._worker_registry.record_success(slot_info.hostname, slot_info.local_rank)
"
4	"            inputs (dict): key is 'image', corresponding vaule is image tensor.
            outputs (dict): key is :
                'classification', corresponding value is the result of classification.
                'feature_map', corresponding value is the result of the layer before the fully connected layer.
"
3	"    margin-top: -350px;
    opacity: 0;
    display: -webkit-box;
    display: flex;
    -webkit-box-align: center;
"
4	"sandal
sarong
"
0	"
        # -- restrict max
        self.assertEqual(
"
4	"missile
mitten
mixing bowl
mobile home, manufactured home
Model T
"
2	"if LooseVersion(pyspark.__version__) < LooseVersion('3.0.0'):
    from pyspark.ml.feature import OneHotEncoderEstimator as OneHotEncoder
else:
    from pyspark.ml.feature import OneHotEncoder
"
4	"coucal
bee eater
"
1	"                xs, masks = e(xs, masks, cache=c)
                new_cache.append(xs)
            new_cache_sd.append(new_cache)
            if self.normalize_before:
"
4	"coffeepot
coil, spiral, volute, whorl, helix
combination lock
"
4	"slug
sea slug, nudibranch
"
4	"table lamp
tank, army tank, armored combat vehicle, armoured combat vehicle
tape player
teapot
teddy, teddy bear
"
2	"            next_hosts = epoch_to_hosts.get(state.epoch + 1, default_hosts)
            if args.discovery_wait > 0 and current_hosts != next_hosts:
                print('host changes: {} -> {}'.format(current_hosts, next_hosts))
                start = int(time.time())
                while state._host_messages.empty():
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
"
4	"                % phase)

        context_prog = fluid.Program()
        startup_prog = fluid.Program()
"
4	"bittern
crane
"
4	"whippet
Ibizan hound, Ibizan Podenco
"
0	"        self.clean_bmesh()

        btools.utils.create_cube_without_faces(
"
4	"        helper = fluid.layer_helper.LayerHelper(""instance_norm"", **locals())
        dtype = helper.input_dtype()
        epsilon = 1e-5
        mean = fluid.layers.reduce_mean(input, dim=[2, 3], keep_dim=True)
        var = fluid.layers.reduce_mean(
"
2	"    queue: cpu
- label: ':tensorflow: Test Keras MNIST (test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2)'
  command: bash -c "" \$(cat /mpirun_command) python /horovod/examples/keras_mnist_advanced.py""
  plugins:
  - docker-compose#v2.6.0:
"
2	"      run: test-cpu-mpich-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
"
0	"    if item.endswith(""/"") :
        items = sorted(set(
"
2	"        optimizer.step()

        #Uncomment below lines to see how loss and gradients change with Adasum
        #if hvd.rank() == 0:
"
2	"            if args.exit_mode == 'exception':
                raise RuntimeError('check_rank and exit epoch={} batch={} start_rank={} rank={}'
                                   .format(epoch, batch, start_rank, hvd.rank()))
"
2	"                                    steps_per_epoch=int(train_rows / args.batch_size / hvd.size()),
                                    validation_steps=int(val_rows / args.batch_size / hvd.size()),
                                    callbacks=callbacks,
                                    verbose=verbose,
                                    epochs=args.epochs)
"
3	"
class DummyCrafter(BaseDocCrafter):
"
4	"DATA_DIM = 224
img_mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))
img_std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))
"
2	"  - docker-compose#v2.6.0:
      run: test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2
"
3	"                return None
            self.train(_train_data)
"
4	"
import math
import warnings

"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: cpu
"
4	"    """"""Calculate padding size.""""""
    if img_size % stride == 0:
        out_size = max(filter_size - stride, 0)
    else:
"
4	"

def EfficientNetB1(is_test=False,
                   padding_type='SAME',
                   override_params=None,
"
2	"state = hvd.elastic.TensorFlowKerasState(mnist_model, opt, batch=0)
state.register_reset_callbacks([on_state_reset])

train(state)

"
4	"Staffordshire bullterrier, Staffordshire bull terrier
American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier
Bedlington terrier
Border terrier
Kerry blue terrier
"
4	"crane
limpkin, Aramus pictus
European gallinule, Porphyrio porphyrio
American coot, marsh hen, mud hen, water hen, Fulica americana
bustard
"
4	"

def round_filters(filters, global_params):
    """""" Calculate and round number of filters based on depth multiplier. """"""
    multiplier = global_params.width_coefficient
"
4	"            top_k (int): Return top k results.

        Returns:
            res (list[dict]): The classfication results.
        """"""
"
2	"config.gpu_options.visible_device_list = str(hvd.local_rank())
K.set_session(tf.Session(config=config))

lr = 1.0
batch_size = 128
"
3	"        """"""A wrapper of docker push """"""
        name = name or self.args.name
        check_registry(self.args.registry, name, _repo_prefix)
        self._check_docker_image(name)
"
4	"def efficientnet(width_coefficient=None,
                 depth_coefficient=None,
"
2	"        return slots

"
4	"            dirname=self.default_pretrained_model_path, executor=exe)

"
2	"    if key in exit_schedule:
        ranks_to_exit = exit_schedule[key]
        if start_rank in ranks_to_exit:
"
2	"        stderr = '<stderr>'
        settings = hvd_settings.Settings(
            verbose=0,
            extra_mpi_args='>mpi-extra args go here<',
"
1	"from typing import List
from typing import Tuple
"
4	"library
lifeboat
lighter, light, igniter, ignitor
limousine, limo
"
4	"                    param_attr=w_param_attrs)
                emb_2_name = emb_2.name
                data_list.append(text_2)
                emb_name_list.append(emb_2_name)

"
2	"
# Horovod: pin GPU to be used to process local rank (one GPU per process)
if args.cuda:
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
"
2	"from horovod.common.elastic import run_fn, ObjectState
from horovod.torch.mpi_ops import init, rank, shutdown
"
2	"
        hvd.init()
"
4	"        id_skip = block_args.id_skip  # skip connection and drop connect
        conv = inputs
        if block_args.expand_ratio != 1:
            conv = fluid.layers.swish(
"
2	"    spark_context = pyspark.SparkContext._active_spark_context
    if spark_context is None:
"
2	"    queue: 2x-gpu-g4
- label: ':tensorflow: Test TensorFlow MNIST (test-mixed-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c "" \$(cat /mpirun_command) python /horovod/examples/tensorflow_mnist.py""
"
2	"
# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.
if hvd.rank() == 0:
"
0	"        # -- remove facemap materials:
        for idx in reversed(list(tag_remove_indices)):
            obj.facemap_materials.remove(idx)
"
2	"                 {'host-1': 4, 'host-2': 8, 'host-3': 4}]
        mock_discovery = mock.Mock()
"
1	"Transformer speech recognition model for single-channel multi-speaker mixture speech.

It is a fusion of `e2e_asr_mix.py` and `e2e_asr_transformer.py`. Refer to:
    https://arxiv.org/pdf/2002.03921.pdf
1. The Transformer-based Encoder now consists of three stages:
"
2	"        for col in cols:
            values = [r[0] for r in df.select(col).distinct().collect()]
            col_type = type([x for x in values if x is not None][0])
"
4	"        return output

    def _expand_conv_norm(self, inputs, block_args, is_test, name=None):
"
2	"        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x)
"
4	"orange
lemon
fig
"
1	"        text_lengths: torch.Tensor,
        speech: torch.Tensor,
        speech_lengths: torch.Tensor,
"
2	"  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/keras_spark_rossmann_estimator.py --num-proc 2 --work-dir /work --data-dir file:///data --epochs 3 --sample-rate 0.01""
  plugins:
  - docker-compose#v2.6.0:
"
4	"            title=""Config options"",
            description=
"
2	"    .Output(""rank: int32"")
    .SetIsStateful()
    .SetShapeFn([](shape_inference::InferenceContext* c) {
"
4	"albatross, mollymawk
grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus
killer whale, killer, orca, grampus, sea wolf, Orcinus orca
dugong, Dugong dugon
"
1	"
        >>> reader = SoundScpReader('wav.scp')
        >>> rate, array = reader['key1']

"
4	"maze, labyrinth
measuring cup
medicine chest, medicine cabinet
megalith, megalithic structure
microphone, mike
"
2	")
dataset = dataset.repeat().shuffle(10000).batch(128)
"
2	"                                                max_np=args.max_np,
                                                elastic_timeout=args.elastic_timeout,
"
4	"

def base64_to_cv2(b64str):
    data = base64.b64decode(b64str.encode('utf8'))
    data = np.fromstring(data, np.uint8)
"
1	"    source $MAIN_ROOT/tools/venv/bin/activate
fi
export PATH=$MAIN_ROOT/utils:$MAIN_ROOT/espnet/bin:$PATH
"
4	"
https://github.com/PaddlePaddle/PaddleClas

### 依赖

"
4	"Samoyed, Samoyede
Pomeranian
chow, chow chow
keeshond
Brabancon griffon
"
4	"candle, taper, wax light
cannon
canoe
can opener, tin opener
cardigan
"
3	"        check_name(manifest['name'])
        # check_image_type
        check_image_type(manifest['type'])
"
4	"    if override_params:
        global_params = global_params._replace(**override_params)
    return blocks_args, global_params

"
4	"Dungeness crab, Cancer magister
rock crab, Cancer irroratus
fiddler crab
king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica
"
4	"import cv2
import os

"
2	"        mock_discovery = mock.Mock()
        mock_discovery.find_available_hosts_and_slots.side_effect = [
"
2	"        # In elastic all tasks wait for task shutdown signal from driver.
        # With Gloo all tasks wait for the command to start and terminate.
        # With MPI task with first index executes orted which will run mpirun_exec_fn for all tasks.
        if is_elastic:
"
4	"        self.assertEqual(mean.tolist(), self.true_mean)
        self.assertEqual(std.tolist(), self.true_std)


if __name__ == '__main__':
"
4	"import numpy as np
import paddlehub as hub


class EfficientNetB5TestCase(TestCase):
"
2	"    queue: cpu
- label: ':factory: Elastic Spark TensorFlow Tests (test-cpu-gloo-py3_8-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark3_0_0)'
  command: bash -c ""cd /horovod/test/integration && SPARK_HOME=/spark SPARK_DRIVER_MEM=512m HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format '[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s' --capture=no test_elastic_spark_tensorflow2.py""
  plugins:
"
3	"      name='time', full_name='jina.Status.Details.time', index=5,
      number=6, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
"
4	"                padding_idx=dict_dim - 1,
                dtype='float32',
                param_attr=w_param_attrs)
"
4	"impala, Aepyceros melampus
gazelle
Arabian camel, dromedary, Camelus dromedarius
llama
weasel
"
2	"
DISCOVER_HOSTS_FREQUENCY_SECS = 1.0
ELASTIC_TIMEOUT_SECS = 600


"
3	"            raise TypeError(
                ""keys' dtype %s does not match with indexers keys's dtype: %s"" %
                (keys.dtype.name, self.key_dtype))
"
4	"        if has_se:
            num_squeezed_channels = max(
"
0	"        btools.utils.plane(self.bm)
        self.assertEquals(len(self.bm.faces), 1)
        self.assertEquals(len(self.bm.verts), 4)
"
2	"# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
"
4	"headers = {""Content-type"": ""application/json""}
url = ""http://127.0.0.1:8866/predict/efficientnetb6_imagenet""
r = requests.post(url=url, headers=headers, data=json.dumps(data))

"
6	"        if isinstance(in_channels, int):
            in_channels = (in_channels, in_channels)
"
0	"        btools.utils.cylinder(self.bm)
        self.assertEquals(len(self.bm.faces), 11)
        self.assertEquals(len(self.bm.verts), 20)

    def test_cube_without_faces(self):
"
2	"
        for dtype, dim, first_join_rank in itertools.product(dtypes, dims, first_join_ranks):
            with tf.device(""/gpu:%d"" % local_rank):
                if local_rank == first_join_rank:
"
4	"bow
bow tie, bow-tie, bowtie
brass, memorial tablet, plaque
brassiere, bra, bandeau
"
4	"computer keyboard, keypad
confectionery, confectionary, candy store
container ship, containership, container vessel
convertible
corkscrew, bottle screw
"
2	"    If `steps_per_epoch` is set, then this callback will also ensure that the number of steps
    in the first epoch following a reset is shortened by the number of batches already processed.
    """"""

    def __init__(self, state):
"
6	"                normal: Union[Tensor, PairTensor],
                edge_index: Adj) -> Tensor:  # yapf: disable
        """"""""""""
"
4	"        depth_divisor=8,
        min_depth=None)

    return blocks_args, global_params
"
4	"#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
4	"$ hub serving start -m efficientnetb1_imagenet
```

这样就完成了一个在线图像识别服务化API的部署，默认端口号为8866。
"
4	"wild boar, boar, Sus scrofa
warthog
"
4	"    return blocks_args, global_params


def get_model_params(model_name, override_params):
"
2	"    automatic: true
  agents:
    queue: 2x-gpu-g4
"
6	"        self.conv1 = FastRGCNConv(in_channels, 16, num_relations, num_bases=30)
        self.conv2 = FastRGCNConv(16, out_channels, num_relations,
                                  num_bases=30)
"
2	"    # HDFS driver to use with Petastorm.
    PETASTORM_HDFS_DRIVER = 'libhdfs'

    # ================ #
    # DATA PREPARATION #
"
2	"                    command_args += ['--reset-limit', str(reset_limit)]

                command_args += ['python', self._training_script, '--logfile', logfile]
                if discovery_schedule:
"
4	"    def _encode_block_string(block):
        """"""Encodes a block to a string.""""""
        args = [
            'r%d' % block.num_repeat,
"
4	"wall clock
wallet, billfold, notecase, pocketbook
wardrobe, closet, press
warplane, military plane
"
2	"            if event.is_set():
                return
        time.sleep(0.01)
"
2	"
# Training settings
parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
parser.add_argument('--batch-size', type=int, default=64, metavar='N',
"
4	"
    return blocks_args, global_params

"
2	"  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
"
2	"            wait_for_one(events)
            return 1, time.time()

        driver.start(np=2, create_worker_fn=exec_command)
"
2	"    if nics and not isinstance(nics, set):
        nics = set(nics)

    tmout = timeout.Timeout(start_timeout,
"
4	"    model = EfficientNet(
        name='b7',
"
4	"Siamese cat, Siamese
Egyptian cat
cougar, puma, catamount, mountain lion, painter, panther, Felis concolor
lynx, catamount
leopard, Panthera pardus
"
4	"ski mask
sleeping bag
slide rule, slipstick
sliding door
slot, one-armed bandit
"
2	"

def on_state_reset():
"
4	"albatross, mollymawk
grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus
"
3	"
#second {
    height: 0 !important;
    overflow: scroll;
    padding-top: 300px;
"
2	"### Deprecated

"
5	"        if use_fc_solve_lib:
            args += ['--reset', '-opt', ]
        else:
"
2	"    if not gloo_built(verbose=(settings.verbose >= 2)):
        raise ValueError('Gloo support is required to use elastic training, but has not been built.  Ensure CMake is '
                         'installed and reinstall Horovod with HOROVOD_WITH_GLOO=1 to debug the build error.')

"
4	"daisy
yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum
corn
acorn
hip, rose hip, rosehip
"
1	"@pytest.fixture
def feats_scp(tmp_path):
    p = tmp_path / ""feats.scp""
"
2	"        mocked_run_controller.assert_called_once()

"
2	"                                     named_parameters=model.named_parameters(),
                                     compression=compression,
"
4	"lighter, light, igniter, ignitor
limousine, limo
"
2	"  agents:
    queue: cpu
- label: ':docker: Build test-cpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0'
"
3	"        else:
            np.testing.assert_almost_equal(retr_idx, idx)
"
1	"        return PhonemeTokenizer(
            g2p_type=g2p_type,
            non_linguistic_symbols=non_linguistic_symbols,
            space_symbol=space_symbol,
            remove_non_linguistic_symbols=remove_non_linguistic_symbols,
"
6	"            print(f'[{i+1}/{len(train_loader)}] Loss: {total_loss / 10:.4f} '
                  f'Train Acc: {correct_nodes / total_nodes:.4f}')
"
1	"def model_summary(model: torch.nn.Module) -> str:
    message = ""Model structure:\n""
    message += str(model)
    num_params = get_human_readable_count(
        sum(p.numel() for p in model.parameters() if p.requires_grad)
"
4	"                num_filters=oup,
                filter_size=1,
                bn_act=None,
                bn_mom=self._bn_mom,
"
4	"thatch, thatched roof
theater curtain, theatre curtain
thimble
thresher, thrasher, threshing machine
throne
"
2	"#
# Unless required by applicable law or agreed to in writing, software
"
6	"            func = getattr(self.base_class, func_name)
            arg_types = parse_types(func)[0][0]
"
4	"microphone, mike
microwave, microwave oven
military uniform
"
4	"brass, memorial tablet, plaque
brassiere, bra, bandeau
breakwater, groin, groyne, mole, bulwark, seawall, jetty
breastplate, aegis, egis
"
3	"    with resource_stream('jina', '/'.join(('resources', 'hub-builder', 'osi-approved.yml'))) as fp:
        approved = yaml.load(fp)
"
6	"        if self.root is not None:
            uniform(self.root.size(0), self.root)
"
4	"        results_2 = self.module.classify(paths=self.test_images, use_gpu=False)
        for index, res in enumerate(results_1):
            self.assertTrue(res.keys(), results_2[index].keys())
"
4	"dial telephone, dial phone
diaper, nappy, napkin
digital clock
digital watch
dining table, board
"
1	"                    reversed(range(len(self.decoder.decoders)))
                ):
"
4	"file, file cabinet, filing cabinet
fireboat
"
4	"        drop_connect_rate=drop_connect_rate,
        num_classes=1000,
"
4	"crate
crib, cot
Crock Pot
croquet ball
"
2	"# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"
2	"
    safe_shell_exec.execute(cmd)
"
2	"    def abort_command(self):
        self._send(AbortCommandRequest())

"
2	"    HorovodReturnScalarOp<int, common::horovod_local_size>);
#if HOROVOD_GPU_BROADCAST
"
3	"             .join(['encode1', 'encode2']))
        with f:
            f.index(input_fn, output_fn=print, callback_on_body=True)

"
2	"                expected_cmd = ('mpirun '
                                '--allow-run-as-root --tag-output '
"
4	"    model = EfficientNet(
        name='b4',
        is_test=is_test,
        padding_type=padding_type,
"
4	"lipstick, lip rouge
Loafer
lotion
"
4	"            self.assertEqual(len(res.keys()), 2)

"
4	"             top_k=1):
```

**参数**
"
4	"        if id_skip and block_args.stride == 1 and input_filters == output_filters:
            if drop_connect_rate:
"
1	"
    ${cuda_cmd} --gpu ${ngpu} ${lmexpdir}/train.log \
        lm_train.py \
"
6	"    model = model.to(device)
    loader = DataLoader(test_dataset, batch_size=256)

    maes = []
    for data in loader:
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: 2x-gpu-g4
"
4	"sidewinder, horned rattlesnake, Crotalus cerastes
trilobite
harvestman, daddy longlegs, Phalangium opilio
"
4	"        top_k (int): Return top k results.
    """"""
    output = []
    for result in data_out:
"
4	"                'classification', corresponding value is the result of classification.
                'feature_map', corresponding value is the result of the layer before the fully connected layer.
            context_prog (fluid.Program): program for transfer learning.
"
2	"            - 'worker-0:2,worker-1:2'
            - '10.11.11.11:4,10.11.11.12:4'
    :return: a list of HostInfo objects describing host to slot mappings
"
4	"        self.arg_config_group.add_argument(
            '--use_gpu',
            type=ast.literal_eval,
            default=False,
"
4	"#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
"
1	"
""""""TTS-Transformer related modules.""""""

from typing import Dict
"
2	"        self._host_manager = HostManager(discovery)
        self._min_np = min_np
        self._max_np = max_np
"
1	"

class SoundScpWriter:
    """"""Writer class for 'wav.scp'

"
1	"                        # Read to the last in order to sort keys
                        # because the order can be changed if num_workers>=1
"
4	"slot, one-armed bandit
snorkel
snowmobile
snowplow, snowplough
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
"
2	"def _make_broadcast_group_fn():
    if _executing_eagerly():
        # Eager mode will parallelize independent control flow
"
4	"swing
switch, electric switch, electrical switch
"
4	"                      name=None):
        # Expansion and Depthwise Convolution
        oup = block_args.input_filters * block_args.expand_ratio  # number of output channels
        has_se = self.use_se and (block_args.se_ratio is
"
4	"pedestal, plinth, footstall
pencil box, pencil case
pencil sharpener
"
4	"king snake, kingsnake
garter snake, grass snake
water snake
"
6	"            z = torch.cat([x_i, x_j], dim=-1)
        else:
            z = torch.cat([x_i, x_j, edge_attr], dim=-1)

"
1	"            help=""use bias term in lightweight/dynamic convolution"",
        )

"
2	"
from horovod._keras import elastic as _impl
from horovod.tensorflow.elastic import TensorFlowKerasState

"
4	"basketball
bassinet
bassoon
"
4	"pick, plectrum, plectron
pickelhaube
picket fence, paling
pickup, pickup truck
"
4	"
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

"
4	"Maltese dog, Maltese terrier, Maltese
Pekinese, Pekingese, Peke
Shih-Tzu
Blenheim spaniel
papillon
"
4	"**返回**

"
6	"    assert conv((x1, x2), adj.t()).tolist() == out.tolist()

"
2	"from elastic_spark_common import BaseElasticSparkTests

"
6	"    jit = torch.jit.script(conv.jittable(t))
    assert jit((x1, x2), edge_index, value).tolist() == out1.tolist()
    assert jit((x1, x2), edge_index, value, (4, 2)).tolist() == out1.tolist()
    assert jit((x1, None), edge_index, value, (4, 2)).tolist() == out2.tolist()
"
1	"fi

# It takes about one day. If you just want to do end-to-end ASR without LM,
# you can skip this and remove --rnnlm option in the recognition (stage 5)
if [ -z ${lmtag} ]; then
"
2	"      hvd_context, ready_event,
      JOIN_TENSOR_NAME, device,
"
1	"            f.write(str(num_splits))
    logging.info(f""N lines of split text: {set(counter.values())}"")
"
2	"        kwargs: Additional properties to sync, will be exposed as attributes of the object.
    """"""
"
2	"        thread.join(1.0)
        self.assertFalse(thread.is_alive())

"
4	"crutch
cuirass
dam, dike, dyke
desk
desktop computer
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 10
"
2	"import unittest

from horovod.common.util import gloo_built, mpi_built
"
4	"
        if not is_test and self._global_params.dropout_rate:
            pool = fluid.layers.dropout(
                pool,
"
4	"                input_filters=round_filters(block_arg.input_filters,
                                            self._global_params),
                output_filters=round_filters(block_arg.output_filters,
"
2	"# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
"
4	"        return 224

    def get_pretrained_images_mean(self):
        im_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3)
"
4	"eft
spotted salamander, Ambystoma maculatum
axolotl, mud puppy, Ambystoma mexicanum
bullfrog, Rana catesbeiana
"
1	"
    def recog(self, enc_output, recog_args, char_list=None, rnnlm=None, use_jit=False):
        """"""Recognize input speech of each speaker.
"
2	"                                                       match_intf=match_intf)

    def notify_hosts_updated(self, timestamp):
        self._send(HostsUpdatedRequest(timestamp))

"
1	"
        if key not in self.chilidren:
            w = DatadirWriter((self.path / key))
            self.chilidren[key] = w
            self.has_children = True
"
4	"rocking chair, rocker
rotisserie
"
2	"
def js_run(settings, nics, env, command, stdout=None, stderr=None):
"
3	"        with f:
            f.index_lines(lines=['abbcs', 'efgh'], output_fn=validate)
            f.index_lines(lines=['abbcs', 'efgh'], output_fn=validate)

    def test_except_with_replicas(self):
"
4	"    model = EfficientNet(
        name='b6',
        is_test=is_test,
        padding_type=padding_type,
"
2	"            if option_key == 'params':
                continue

"
2	"#
#     http://www.apache.org/licenses/LICENSE-2.0
"
4	"        if 0 < block.se_ratio <= 1:
            args.append('se%s' % block.se_ratio)
        if block.id_skip is False:
            args.append('noskip')
        return '_'.join(args)
"
4	"    def get_vocab_path(self):
        return self.vocab_path


"
4	"shopping cart
shovel
shower cap
shower curtain
ski
"
4	"    'drop_connect_rate',
])

BlockArgs = collections.namedtuple('BlockArgs', [
"
6	"                    if isinstance(size, Tensor):
                        for j, size in enumerate(size.tolist()):
                            tmp = f'{key}_{j}_batch'
"
4	"muzzle
nail
neck brace
"
4	"                    text_3 = fluid.data(
                        name='text_3',
                        shape=[-1, max_seq_len],
"
6	"    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,
                edge_attr: OptTensor = None, size: Size = None):
"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
"
4	"power drill
prayer rug, prayer mat
printer
prison, prison house
projectile, missile
"
2	"        nics = set()
        for iface, addrs in net_if_addrs().items():
            if settings.nics and iface not in settings.nics:
                continue
"
4	"# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
"
4	"        element['image'] = process_image(element['org_im'])
        yield element

"
4	"manhole cover
maraca
"
1	"    nj=32
    if [[ $(get_yaml.py ${train_config} model-module) = *transformer* ]]; then
"
3	"        else:
            for chunk in d.chunks:
                if eval(self.compiled_if):
                    yield chunk
"
4	"        :return: a list of BlockArgs namedtuples of block args
        """"""
"
2	"        return df


    def lookup_columns(df, vocab):
        def lookup(mapping):
"
2	"        super(StaticRunTests, self).__init__(*args, **kwargs)
        warnings.simplefilter('module')

    @contextlib.contextmanager
    def horovod_args(self, mode, controller, run_func=None, command=None):
"
1	"
    def __call__(self, text: str) -> str:
"
4	"green mamba
sea snake
"
2	"        """"""

"
2	"from __future__ import print_function

import os
"
1	"        This function takes query, key and value but uses only quert.
        This is just for compatibility with self-attention layer (attention.py)

        Args:
"
2	"
@hvd.elastic.run
"
2	"
        # And again
        host_manager.update_available_hosts()
        current_hosts = host_manager.current_hosts
        assert current_hosts.available_hosts == {'b'}
"
4	"bustard
ruddy turnstone, Arenaria interpres
red-backed sandpiper, dunlin, Erolia alpina
"
4	"
def resize_short(img, target_size):
    percent = float(target_size) / min(img.size[0], img.size[1])
    resized_width = int(round(img.size[0] * percent))
"
4	"            ""../image_dataset/keypoint_detection/girl2.jpg""
        ]
        self.true_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3).tolist()
"
4	"barbershop
barn
barometer
barrel, cask
"
2	"        max_np = num_proc

    # start Spark driver service and launch settings.num_proc Spark tasks
    key = secret.make_secret_key()
"
2	"      run: test-cpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
"
1	"        self.lm.NullContextWrite(state)
        return state

"
4	"    return model


"
1	"
# make shellcheck happy
train_cmd=
decode_cmd=
"
4	"            padding_type=self.padding_type,
            bn_eps=self._bn_eps,
            name='',
            conv_name='_conv_stem',
            bn_name='_bn0')
"
2	"      run: test-cpu-oneccl-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
"
4	"            add_vars_prefix(
                program=main_program, prefix=prefix_name, vars=variable_names)
"
2	"                                       [i * 17] + [0] * (dim - 1),
                                       [17] + [-1] * (dim - 1))
                if dtype != tf.bool:
                    value = i
                else:
"
4	"            bn_act='swish',
            bn_mom=self._bn_mom,
            bn_eps=self._bn_eps,
            padding_type=self.padding_type,
"
2	"            last_epoch = epoch
            self._shutdown.wait(0.1)
"
6	"        b: PairOptTensor = (None, None)
        if isinstance(batch, Tensor):
            b = (batch, batch)
        elif isinstance(batch, tuple):
"
2	"""""""overlapping_rs: allow
cpu_index_using: logical

rank: 0: { hostname: host1; cpu: {0-3} ; gpu: * ; mem: * }
rank: 1: { hostname: host1; cpu: {4-7} ; gpu: * ; mem: * }
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
"
2	"            for col in cols:
                rdd = rdd.mapPartitions(add_elapsed_column(col, asc))
            df = rdd.toDF()
        return df
"
0	"    def tearDown(self):
        # -- restore test_prop to previous state
        for key, val in self.defaults.items():
            setattr(bpy.context.scene.test_prop, key, val)

"
2	"  agents:
    queue: 2x-gpu-g4
"
6	"    assert jit((x1, x2), edge_index, value).tolist() == out1.tolist()
    assert jit((x1, x2), edge_index, value,
"
4	"standard schnauzer
Scotch terrier, Scottish terrier, Scottie
"
1	"
    echo ""make a non-linguistic symbol list""
    touch ${nlsyms}

"
4	"        'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25',
        'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25',
"
4	"            block_arg = block_arg._replace(
                input_filters=round_filters(block_arg.input_filters,
"
1	"
    def forward_one_step(self, xs, masks, cache=None):
        """"""Encode input frame.

"
2	"    key = settings.key

"
4	"# You may obtain a copy of the License at
#
"
1	"    @staticmethod
    def resume(
"
1	"    )

"
5	"            s.rows += self._createFirTree(layout, x0, y0)
            x0 += 2.5*layout.XS
"
4	"traffic light, traffic signal, stoplight
book jacket, dust cover, dust jacket, dust wrapper
menu
plate
"
6	"

class GNNBenchmarkDataset(InMemoryDataset):
"
0	"
        @btools.utils.crash_safe
        def run_failed():
"
2	"        if self.initial_lr is None:
            warnings.warn('Parameter `initial_lr` will be required in v0.21.0', DeprecationWarning)

"
4	"        h_start = np.random.randint(0, height - size + 1)
    w_end = w_start + size
"
4	"gondola
gong, tam-tam
gown
grand piano, grand
"
4	"can opener, tin opener
cardigan
car mirror
"
1	"                    local_best_scores, local_best_ids = torch.topk(
                        local_att_scores, ctc_beam, dim=1
                    )
"
4	"
if __name__ == '__main__':
    b4 = EfficientNetB4ImageNet()
"
4	"breakwater, groin, groyne, mole, bulwark, seawall, jetty
breastplate, aegis, egis
broom
bucket, pail
"
4	"
### 依赖

"
2	"#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
"
2	"@contextlib.contextmanager
def _temp_discovery_script(logfile, discovery_schedule):
    with temppath() as discovery_script:
        with open(discovery_script, 'w') as f:
            f.write(DISCOVERY_SCRIPT_TEMPLATE.format(logfile=logfile) + os.linesep)
"
5	"                        hints.append([1, src, None])
                    else:
                        hints.append([1, game.s.talon, None])
                    m = bh_solve_lib_obj.get_next_move()
        else:
"
2	"    mpi_impl_flags, impl_binding_args = _get_mpi_implementation_flags(settings.tcp_flag, env=env)

"
2	"        # Make sure pyarrow is referenced before anything else to avoid segfault due to conflict
        # with TensorFlow libraries.  Use `pa` package reference to ensure it's loaded before
        # functions like `deserialize_model` which are implemented at the top level.
        # See https://jira.apache.org/jira/browse/ARROW-3346
        pa
"
1	"    with pytest.raises(ValueError):
        load_num_sequence_text(p)
"
2	"                            extra_conf=[conf.SPARK_CONF_ALWAYS_RESTART_FAILED_TASK])

"
2	"    queue: cpu
- label: ':factory: Elastic Spark Torch Tests (test-cpu-gloo-py3_7-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""cd /horovod/test/integration && SPARK_HOME=/spark SPARK_DRIVER_MEM=512m HOROVOD_LOG_LEVEL=DEBUG pytest --forked -v --log-cli-level 10 --log-cli-format '[%(asctime)-15s %(levelname)s %(filename)s:%(lineno)d %(funcName)s()] %(message)s' --capture=no test_elastic_spark_torch.py""
"
1	"        '1 K'
        >>> get_human_readable_count(2e6)   # (two million)
        '2 M'
        >>> get_human_readable_count(3e9)   # (three billion)
        '3 B'
"
1	"        x = self.linear1(x)

        # GLU activation
"
2	"        self.cross_size = cross_size

    def to_response_string(self):
        return ','.join(str(v) for v in [self.rank, self.size,
"
0	"import unittest

"
4	"tiger shark, Galeocerdo cuvieri
hammerhead, hammerhead shark
electric ray, crampfish, numbfish, torpedo
"
6	"        self.avg_deg: Dict[str, float] = {
            'lin': deg.mean().item(),
            'log': (deg + 1).log().mean().item(),
            'exp': deg.exp().mean().item(),
"
4	"        assert isinstance(string_list, list)
        blocks_args = []
        for block_string in string_list:
"
2	"@tf.function
def training_step(images, labels, allreduce=True):
    with tf.GradientTape() as tape:
        probs = mnist_model(images, training=True)
"
1	"    p = tmp_path / ""shape.txt""
    with p.open(""w"") as f:
"
2	"
            # Busy wait for the number of available slots to decrease
            while driver._host_manager.current_hosts.count_available_slots() > 2:
                time.sleep(0.01)

"
4	"                efficientnet_b4 = EfficientNetB4(
                    override_params=override_params)
"
6	"            x_l = self.lin_l(x_l).view(-1, H, C)
            alpha_l = (x_l * self.att_l).sum(dim=-1)
            if x_r is not None:
"
4	"pencil box, pencil case
pencil sharpener
perfume, essence
Petri dish
photocopier
"
2	"        return host_slots


"
6	"            data.edge_index = None

        value = None
        for key in ['edge_weight', 'edge_attr', 'edge_type']:
            if data[key] is not None:
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
2	"            .withColumn('Promo2SinceWeek', F.coalesce(df.Promo2SinceWeek, F.lit(1)))

        # Days & months competition was open, cap to 2 years.
        df = df.withColumn('CompetitionOpenSince',
                           F.to_date(F.format_string('%s-%s-15', df.CompetitionOpenSinceYear,
"
4	"Granny Smith
strawberry
"
5	"        self.setSize(
            layout.XM + rows*layout.XS,
            layout.YM + 2*layout.YS + playcards*layout.XOFFSET)
"
4	"        assert name in valid_names, 'efficient name should be in b0~b7'
        model_name = 'efficientnet-' + name
"
0	"    prop.frame_thickness = min(prop.frame_thickness, min(length, width) / 2)

    # -- subdivide the face along the shortest side
    func = [subdivide_face_vertically, subdivide_face_horizontally][width > length]
    sections = [[length / 2] * 3, [width / 2] * 3][width > length]
"
4	"motor scooter, scooter
mountain bike, all-terrain bike, off-roader
"
2	"
        if not gloo_built():
            self.skipTest(""Gloo is not available"")

"
2	"        hostname, slots = host.strip().split(':')
        host_names.append(hostname)
        host_to_slots[hostname] = int(slots)
    return host_names, host_to_slots
"
2	"            self.synchronize()
        self._synchronized = False
        return super(self.__class__, self).step(closure)
"
3	"                    self._prev_requests = [getattr(v.request, v.request.WhichOneof('body')) for v in
                                           self._prev_messages]
"
4	"            width_padding = left_padding + stride
            need_crop = True
        padding = [height_padding, width_padding]
    elif padding_type == ""VALID"":
        height_padding = 0
"
1	"    :param float positional_dropout_rate: dropout rate after adding positional encoding
    :param str or torch.nn.Module input_layer: input layer type
    :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding
"
4	"notebook, notebook computer
obelisk
oboe, hautboy, hautbois
ocarina, sweet potato
odometer, hodometer, mileometer, milometer
"
2	"                                                    start_timeout=tmout,
                                                    nics=nics,
                                                    run_func_mode=True)

"
4	"chain
chainlink fence
chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour
"
1	"                     [1, 1, 1, 0, 0]]], dtype=torch.uint8)

        """"""
        y_masks = make_non_pad_mask(olens).to(next(self.parameters()).device)
        s_masks = subsequent_mask(y_masks.size(-1), device=y_masks.device).unsqueeze(0)
"
2	"        def get_pid(logfile):
            # Wait until the script has written its PID to the logfile
            wait(lambda: os.path.exists(logfile), timeout=5)
            with open(logfile, 'r') as f:
                return int(f.read())
"
4	"        width_coefficient=width_coefficient,
        depth_coefficient=depth_coefficient,
        depth_divisor=8,
        min_depth=None)

"
4	"    res = b4.classification(images=test_image)
    print(res)
"
1	"

@pytest.mark.skipif(
    LooseVersion(torch.__version__) < LooseVersion(""1.2""), reason=""require pytorch>=1.2""
"
6	"    local_nn = Seq(Lin(16 + 4, 32), ReLU(), Lin(32, 32))
    global_nn = Seq(Lin(32, 32))
"
1	"from typeguard import check_argument_types

from espnet2.text.abs_tokenizer import AbsTokenizer
"
2	"    (r, w) = ctx.Pipe()

"
6	"
        x_r = x[1]
        if x_r is not None:
            out += self.lin_r(x_r)
"
2	"    :param daemon: event thread is a daemon thread if set to True, otherwise stop event must be given
    :param silent: swallows exceptions raised by target silently
    :return: thread
"
1	"            y: previous char
            next_token: next token need to be score
            state: previous state
            x: encoded feature

"
2	"

class HostsUpdatedRequest(object):
    """"""Notifies worker that the set of available hosts/slots has changed.""""""
"
3	"from ...decorators import batching, as_ndarray


class UniversalSentenceEncoder(BaseTextTFEncoder):
"
6	"        \right)

    with
"
4	"    img = resize_short(img, target_size=256)
    img = crop_image(img, target_size=DATA_DIM, center=True)
"
2	"state = hvd.elastic.KerasState(model, batch=100, epoch=0)
state.register_reset_callbacks([on_state_reset])

callbacks = [
    # Horovod: elastic training callbacks to update and commit state.
"
2	"        self._workers = []
        self._shutdown = threading.Event()
        self._logfile_monitor = None
"
4	"PaddleHub Serving可以部署一个在线图像识别服务。

## 第一步：启动PaddleHub Serving

运行启动命令：
"
2	"                        # this raises above exception, but allows us to catch execute's arguments
                        horovod.spark.run(fn, args=args, kwargs=kwargs,
                                          num_proc=num_proc, start_timeout=10,
                                          use_mpi=False, use_gloo=True,
                                          extra_mpi_args=extra_mpi_args, env=env,
"
2	"                            extra_conf=[conf.SPARK_CONF_ALWAYS_RESTART_FAILED_TASK,
                                        conf.SPARK_CONF_BLACKLIST_DISABLED])

"
2	"            # Skip if compiled with CUDA but without HOROVOD_GPU_OPERATIONS.
            self.skipTest(""Not compiled with HOROVOD_GPU_OPERATIONS"")
"
2	"        serialized_rsh_settings = actual_rsh_command.split(' ')[-1]
        actual_rsh_settings = codec.loads_base64(serialized_rsh_settings)
"
4	"
    def _drop_connect(self, inputs, prob, is_test):
        if is_test:
            return inputs
        keep_prob = 1.0 - prob
"
1	"    LooseVersion(torch.__version__) < LooseVersion(""1.2""), reason=""require pytorch>=1.2""
)
def test_ESPnetDataset_csv_float(csv_float):
"
4	"water snake
vine snake
"
2	">     if GPU_INFERENCE_ENABLED:
>         if GPU_INFERENCE_CLUSTER:
>             conf.setMaster(GPU_INFERENCE_CLUSTER)
>         conf = set_gpu_conf(conf)
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: cpu
"
6	"    if edge_attr is None:
        edge_attr = torch.ones(edge_index.size(1), device=edge_index.device)

"
2	"

state = hvd.elastic.TorchState(model, optimizer, batch=0, epoch=0, commits=0, rendezvous=0)
state.register_reset_callbacks([on_state_reset])
"
6	"            if cache is None:
                edge_index, edge_weight = gcn_norm(  # yapf: disable
                    edge_index, edge_weight, x.size(self.node_dim), False,
                    self.add_self_loops, dtype=x.dtype)
                if self.cached:
"
4	"velvet
vending machine
vestment
viaduct
"
4	"

class BlockDecoder(object):
    """""" Block Decoder for readability, straight from the official TensorFlow repository """"""
"
1	"from espnet2.fileio.npy_scp import NpyScpWriter
from espnet2.fileio.sound_scp import SoundScpWriter
"
3	"    Designed with ASR outputs in mind, DeepSegment uses BiLSTM + CRF for automatic sentence boundary detection. It significantly outperforms the standard libraries (spacy, nltk, corenlp ..) on imperfect text and performs similarly for perfectly punctuated text.

    Example: 'I am Batman i live in gotham'
            ->  # ['I am Batman', 'i live in gotham']
"
4	"           relufactor=0.0,
           use_bias=False,
           padding_type=None,
           initial=""normal"",
"
2	"# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

import tensorflow as tf
"
4	"burrito
red wine
espresso
"
4	"
class EfficientNet():
    def __init__(self,
                 name='b0',
"
1	"                     [1, 1, 0, 0, 0],
                     [1, 1, 1, 0, 0],
                     [1, 1, 1, 1, 0],
"
2	"                raise RuntimeError('No hosts from previous set remaining, unable to broadcast state.')

        self._host_assignments = host_assignments
"
1	"
            # 2. Load the entry from each line and create a dict
"
4	"        height_padding = 0
        width_padding = 0
        padding = [height_padding, width_padding]
"
1	"            labels[:, -1] = 1.0  # make sure at least one frame has 1

        # caluculate loss values
"
1	"        Returns:
            Tensor: Batch of integrated hidden state sequences (B, Tmax, adim).
"
4	"import paddle.fluid as fluid
from efficientnetb6_imagenet.layers import conv2d, init_batch_norm_layer, init_fc_layer
"
4	"
    def _conv_stem_norm(self, inputs, is_test):
        out_channels = round_filters(32, self._global_params)
        bn = self.conv_bn_layer(
            inputs,
"
4	"trolleybus, trolley coach, trackless trolley
trombone
tub, vat
turnstile
typewriter keyboard
"
4	"racer, race car, racing car
racket, racquet
"
1	"

"
4	"            if num_data > 1:
                text_2 = fluid.data(
"
2	"  - ecr#v1.2.0:
      login: true
"
6	"        for adj, y in zip(adjs, ys):
            row, col = torch.from_numpy(adj.row), torch.from_numpy(adj.col)
            edge_index = torch.stack([row, col], dim=0).to(torch.long)
"
1	"    elif phoneme_tokenizer.g2p_type == ""pyopenjtalk_kana"":
        input = ""昔は俺も若かった""
        output = [""ム"", ""カ"", ""シ"", ""ワ"", ""オ"", ""レ"", ""モ"", ""ワ"", ""カ"", ""カ"", ""ッ"", ""タ""]
    elif phoneme_tokenizer.g2p_type == ""pypinyin_g2p"":
"
2	"def _run(args):
    # If LSF is used, use default values from job config
    if lsf.LSFUtils.using_lsf():
        if not args.np:
            args.np = lsf.LSFUtils.get_num_processes()
"
1	"                        ]
                    )
                    prev_state = [s, s - 1, s - 2]
                logdelta[t, s] = np.max(candidates) + lpz[t][y_int[s]]
                state_path[t, s] = prev_state[np.argmax(candidates)]
"
2	"    def _to_numpy(self, var):
        return var.numpy()

    def _load_model(self):
        for var, value in zip(self.variables, self._values):
"
4	"
**参数**

"
4	"loudspeaker, speaker, speaker unit, loudspeaker system, speaker system
loupe, jeweler's loupe
"
1	"

def pyopenjtalk_g2p_kana(text) -> List[str]:
"
1	"        # define final projection
        self.feat_out = torch.nn.Linear(adim, odim * reduction_factor)
        self.prob_out = torch.nn.Linear(adim, reduction_factor)

"
4	"yawl
yurt
web site, website, internet site, site
comic book
"
4	"vizsla, Hungarian pointer
English setter
Irish setter, red setter
Gordon setter
"
4	"Great Pyrenees
Samoyed, Samoyede
Pomeranian
chow, chow chow
keeshond
"
1	"            if args.api == ""v2"":
                raise NotImplementedError(f""--api {args.api} is not supported"")
"
4	"
    Args:
"
4	"        tmp = fluid.layers.elementwise_add(tmp, offset, axis=1)
        return tmp
    else:
        raise NotImplementedError(""norm tyoe: [%s] is not support"" % norm_type)
"
2	"def run_elastic(fn, args=(), kwargs={}, num_proc=None, min_np=None, max_np=None,
                start_timeout=None, elastic_timeout=None, reset_limit=None, env=None, verbose=1, nics=None):
    """"""
    Runs Elastic Horovod in Spark.  Runs `num_proc` processes executing `fn` using the same amount of Spark tasks.
"
2	"        probs = model(data, training=True)
        loss = tf.losses.categorical_crossentropy(target, probs)

    # Horovod: add Horovod Distributed GradientTape.
    if allreduce:
"
6	"    assert jit((x1, None), edge_index, value,
               size=(4, 2)).tolist() == out2.tolist()

    t = '(OptPairTensor, SparseTensor, OptTensor, Size) -> Tensor'
"
6	"            'Static graphs not supported in `DynamicEdgeConv`.'

        b: PairOptTensor = (None, None)
        if isinstance(batch, Tensor):
            b = (batch, batch)
"
2	"    def forward(self, input, weight, bias, running_mean, running_var, eps, momentum):
        input = input.contiguous()

        size = input.numel() // input.size(1)
"
3	"            elif isinstance(v, list) or isinstance(v, tuple):
                chunk.ClearField(k)
                getattr(chunk, k).extend(v)
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
"
4	"            num_squeezed_channels = max(
                1, int(block_args.input_filters * block_args.se_ratio))
            conv = self.se_block(conv, num_squeezed_channels, oup, name)

"
4	"maypole
maze, labyrinth
"
4	"remote control, remote
restaurant, eating house, eating place, eatery
revolver, six-gun, six-shooter
rifle
"
4	"def get_model_params(model_name, override_params):
    """""" Get the block args and global params for a given model """"""
    if model_name.startswith('efficientnet'):
        w, d, _, p = efficientnet_params(model_name)
        blocks_args, global_params = efficientnet(
"
2	"            state.commit()

"
4	"# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
"
1	"from espnet2.text.sentencepiece_tokenizer import SentencepiecesTokenizer


@pytest.fixture
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
"
4	"
def EfficientNetB2(is_test=False,
                   padding_type='SAME',
                   override_params=None,
                   use_se=True):
"
1	"
    def forward(self, query, key, value, mask):
        """"""Forward of 'Dynamic 2-Dimentional Convolution'.

"
6	"    x1 = torch.randn(4, 16)
    x2 = torch.randn(2, 16)
    edge_index = torch.tensor([[0, 1, 2, 3], [0, 0, 1, 1]])
    row, col = edge_index
    adj = SparseTensor(row=row, col=col, sparse_sizes=(4, 4))
"
2	"        self._bcast_object = bcast_object
        self._rank = get_rank
        self._host_messages = queue.Queue()
        self._last_updated_timestamp = 0
        self._reset_callbacks = []
"
4	"#
# Licensed under the Apache License, Version 2.0 (the ""License"");
"
4	"        name='b7',
        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
"
4	"
        ops = block_string.split('_')
        options = {}
        for op in ops:
"
4	"brown bear, bruin, Ursus arctos
American black bear, black bear, Ursus americanus, Euarctos americanus
"
4	"def EfficientNetB4(is_test=False,
                   padding_type='SAME',
                   override_params=None,
                   use_se=True):
"
4	"hognose snake, puff adder, sand viper
green snake, grass snake
king snake, kingsnake
"
4	"                add_vars_prefix(context_prog, name_prefix)
                add_vars_prefix(startup_prog, name_prefix)
"
4	"            batch_size=args.batch_size,
            use_gpu=args.use_gpu)
        return results

    def add_module_config_arg(self):
"
4	"tree frog, tree-frog
tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui
loggerhead, loggerhead turtle, Caretta caretta
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
"
4	"Lhasa, Lhasa apso
flat-coated retriever
curly-coated retriever
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
"
1	"            x = x + self.bias.view(1, -1, 1)
        x = x.transpose(1, 2)  # B x T x C
        x = torch.cat((x, xf), -1)  # B x T x Cx2
"
2	"train(state)
test()
"
4	"wombat
jellyfish
sea anemone, anemone
brain coral
"
2	"        driver.start(np=2, create_worker_fn=exec_command)
        res = driver.get_results().worker_results
        driver.stop()

"
4	"        x /= tmp
    return x
"
2	"        # Delta optimizer implements this logic:
        #  start = current.copy()
        #  step() -> computes 'current - \alpha.f(g)' where f is
"
4	"                    padding_idx=dict_dim - 1,
                    dtype='float32',
                    param_attr=w_param_attrs)
"
4	"from __future__ import print_function

import math
"
4	"        self._blocks_args, self._global_params = get_model_params(
            model_name, override_params)
"
2	"        if state.batch == args.num_batches_per_commit:
            state.batch = 0
            state.commit()


"
4	"tow truck, tow car, wrecker
toyshop
tractor
"
4	"monastery
monitor
moped
"
1	"    def forward(self, query, key, value, mask):
        """"""Forward of 'Lightweight Convolution'.

"
2	"            self.assertFalse(child.is_running())

"
6	"            elif scaler == 'inverse_linear':
                out = out * (self.avg_deg['lin'] / deg)
"
4	"
    def get_pretrained_images_mean(self):
        im_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3)
        return im_mean

"
1	"        ys_in = torch.cat(
            [ys.new_zeros((ys.shape[0], 1, ys.shape[2])), ys[:, :-1]], dim=1
        )
"
4	"                name=""seq_len"", shape=[1], dtype='int64', lod_level=0)
            seq_len_used = fluid.layers.squeeze(seq_len, axes=[1])

            # Add embedding layer.
            w_param_attrs = fluid.ParamAttr(
"
2	"            #   2                       1 / 0   1 / 1

            # host with start rank 0 got removed in epoch 1
"
4	"            if block_args.num_repeat > 1:
                block_args = block_args._replace(
"
3	"        f = tempfile.NamedTemporaryFile('w', delete=False).name
        with open(f, 'w', encoding='utf8') as fp:
            fp.writelines(revised_dockerfile)

        for k in revised_dockerfile:
"
1	"from espnet2.fileio.sound_scp import SoundScpReader


def test_SoundScpReader(tmp_path: Path):
"
4	"red-breasted merganser, Mergus serrator
goose
black swan, Cygnus atratus
tusker
"
4	"    if override_params:
        global_params = global_params._replace(**override_params)
    return blocks_args, global_params


"
4	"tiger beetle
ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle
ground beetle, carabid beetle
"
0	"    """""" Remove all facemaps that don't have any faces assigned
    """"""
"
4	"cup
eggnog
alp
bubble
cliff, drop, drop-off
"
4	"            padding_type=self.padding_type,
            name='',
            conv_name='_conv_head',
            bn_name='_bn1')

"
4	"paddlepaddle >= 1.6.2

paddlehub >= 1.6.0

"
2	"               codec.dumps_base64(driver.addresses()),
               codec.dumps_base64(settings))

    exec_command = _exec_command_fn(driver, key, settings, env)
"
4	"
这样就完成了一个在线图像识别服务化API的部署，默认端口号为8866。
"
4	"fur coat
garbage truck, dustcart
gasmask, respirator, gas helmet
gas pump, gasoline pump, petrol pump, island dispenser
"
4	"            bias_attr = False

"
3	"            proxy = urllib.request.ProxyHandler({'http': args.download_proxy, 'https': args.download_proxy})
            opener.add_handler(proxy)
        urllib.request.install_opener(opener)
        # head check
"
0	"import bpy
import btools
"
2	"    Tests generate_jsrun_rankfile.
    """"""
"
2	"    :param events: events to abort the command, only if background is True
    :return exit code if background is False
    """"""
    if ':' in host_hash:
"
6	"        out = model(data.x, data.edge_index, data.edge_attr, data.batch)
        total_error += (out.squeeze() - data.y).abs().sum().item()
    return total_error / len(loader.dataset)

"
2	"                  HorovodGlobalState* global_state)
      : GPUAllgather(gpu_context, global_state),
        nccl_op_context_(nccl_context, global_state, Communicator::GLOBAL),
        global_state_(global_state){};
"
2	"493a505,529
>     def set_gpu_conf(conf):
>         # This config will change depending on your cluster setup.
>         #
"
3	"        :param ref_indexer: Bootstrap the current indexer from a ``ref_indexer``. This enables user to switch
                            the query algorithm at the query time.

        """"""
"
4	"def resize_short(img, target_size):
    percent = float(target_size) / min(img.size[0], img.size[1])
    resized_width = int(round(img.size[0] * percent))
    resized_height = int(round(img.size[1] * percent))
"
4	"crutch
cuirass
"
3	"
$("".wrap"").click(function () {
    $("".wrap"").addClass(""active"");
    $("".wrap > .icon"").addClass(""active"");
"
2	"
    driver.wait_for_task_to_task_address_updates(settings.start_timeout)
"
4	"
paddlehub >= 1.6.0

"
4	"
        context_prog = fluid.Program()
        startup_prog = fluid.Program()
        with fluid.program_guard(context_prog, startup_prog):
            with fluid.unique_name.guard():
"
1	"        logging.basicConfig(
            level=logging.WARN,
"
2	"
        self.assertEqual(0, results[1]['start_rank'])
        self.assertEqual(3, results[1]['size'])
        self.assertEqual(2, results[1]['rendezvous'])
"
1	"

"
4	"suspension bridge
swab, swob, mop
sweatshirt
"
4	"grasshopper, hopper
cricket
walking stick, walkingstick, stick insect
cockroach, roach
"
6	"                        start = self.__slices__[key][i][j].item()
                        end = self.__slices__[key][i + 1][j].item()
                        item = item.narrow(dim, start, end - start)

"
2	"
from distutils.version import LooseVersion

import tensorflow as tf

"
2	"            env = {}

        def fn():
            return 1

"
3	"  'MutablepodSpawnRequest': _reflection.GeneratedProtocolMessageType('MutablepodSpawnRequest', (_message.Message,), {
    'DESCRIPTOR': _SPAWNREQUEST_MUTABLEPODSPAWNREQUEST,
    '__module__': 'jina_pb2'
"
2	"        """"""

"
6	"    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,
                edge_attr: OptTensor = None, size: Size = None) -> Tensor:
"
2	"      run: test-gpu-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
"
4	"            name=name + '_se_expand')
        se_out = inputs * fluid.layers.sigmoid(x_squeezed)
"
2	"    queue: 2x-gpu-g4
- label: ':spark: Spark Keras Rossmann Run (test-mixed-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/keras_spark_rossmann_run.py --num-proc 2 --data-dir file:///data --epochs 3 --sample-rate 0.01""
  plugins:
  - docker-compose#v2.6.0:
"
2	"    # Furthermore, attempting to access the state dict would result in
    # an error.
    if len(state_dict['state']) == 0:
"
6	"            update_kwargs = self.inspector.distribute('update', coll_dict)
            return self.update(out, **update_kwargs)
"
4	"    author=""paddlepaddle"",
    author_email=""paddle-dev@baidu.com"",
    summary=
    ""EfficientNetB7 is a image classfication model, this module is trained with imagenet datasets."",
"
2	"    This is handled in elastic mode as a recoverable error, and will result in a reset event.
    """"""
    pass


"
2	"        self.assertTrue(os.path.exists(file))

"
6	"        out = torch.zeros(x_r.size(0), self.out_channels, device=x_r.device)

        weight = self.weight
        if self.num_bases is not None:  # Basis-decomposition =================
            weight = (self.comp @ weight.view(self.num_bases, -1)).view(
"
3	"                raise ValueError(f'image {name} does not match with label info in the image')
        except KeyError:
            self.logger.error('missing key in the label of the image')
            raise

"
1	"    parser.add_argument(""--scps"", required=True, help=""Input texts"", nargs=""+"")
    parser.add_argument(""--names"", help=""Output names for each files"", nargs=""+"")
    parser.add_argument(""--num_splits"", help=""Split number"", type=int)
    parser.add_argument(""--output_dir"", required=True, help=""Output directory"")
    return parser
"
2	"    with open(args.local_checkpoint_file, 'wb') as f:
        f.write(best_model_bytes)
    print('Written checkpoint to %s' % args.local_checkpoint_file)

"
0	"    runner = unittest.TextTestRunner(verbosity=3)
    runner.run(suite)
"
4	"            seq_len_used = fluid.layers.squeeze(seq_len, axes=[1])

            # Add embedding layer.
"
2	"  timeout_in_minutes: 10
  retry:
    automatic: true
  agents:
    queue: cpu
"
1	"                else:
                    candidates = np.array(
                        [
                            logdelta[t - 1, s],
"
3	"ecl-2.0: Educational Community License v2.0
epl-1.0: Eclipse Public License 1.0
eupl-1.1: European Union Public License 1.1
"
2	"  - ecr#v1.2.0:
      login: true
"
2	"class CommitStateCallbackImpl(object):
    def __init__(self, backend, state, batches_per_commit, *args):
        super(CommitStateCallbackImpl, self).__init__(*args)
"
4	"marimba, xylophone
mask
matchstick
"
3	"            for d in req.docs:
                self.assertNotEqual(d.text, '')

        f = (Flow(read_only=True).add(yaml_path='yaml/datauriindex.yml', timeout_ready=-1))

"
4	"
def efficientnet_params(model_name):
"
4	"malinois
briard
"
4	"            '--batch_size',
            type=ast.literal_eval,
            default=1,
            help=""batch size."")
        self.arg_config_group.add_argument(
"
4	"king penguin, Aptenodytes patagonica
albatross, mollymawk
grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus
"
4	"# limitations under the License.

"
4	"class EfficientNetB0TestCase(TestCase):
    def setUp(self):
"
2	"# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
"
4	"        args = [
            'r%d' % block.num_repeat,
"
4	"            filter_size=3,
            stride=2,
"
2	"            rendezvous_port = rendezvous_port if rendezvous_port is not None else \
                int(os.environ.get(HOROVOD_GLOO_RENDEZVOUS_PORT))
            nic = nic or os.environ.get(HOROVOD_GLOO_IFACE)
            hostname = hostname or os.environ.get(HOROVOD_HOSTNAME)
            local_rank = local_rank if local_rank is not None else \
"
4	"projectile, missile
projector
puck, hockey puck
punching bag, punch bag, punching ball, punchball
purse
"
4	"    for result in data_out:
        result_i = softmax(result)
        output_i = {}
        indexs = np.argsort(result_i)[::-1][0:top_k]
"
4	"                      conv_act=None,
                      bn_act='swish',
"
6	"    assert conv.lin_r.weight.data_ptr == conv2.lin_r.weight.data_ptr

"
4	"tub, vat
turnstile
typewriter keyboard
"
2	"            parent.wait(timeout=safe_shell_exec.GRACEFUL_TERMINATION_TIME_S)
            p.wait()

"
4	"# -*- coding:utf-8 -*-
# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.
"
1	"    def permutationDFS(self, source, start):
        """"""Get permutations with DFS.

"
4	"weevil
fly
bee
ant, emmet, pismire
"
4	"
def conv2d(input,
           num_filters=64,
"
1	"    sp.load(model)

    with input_text.open(""r"") as f:
        vocabs = {""<unk>"", ""▁""}
        for line in f:
"
2	"        host_name = slot_info.hostname

        host_address = network.resolve_host_address(host_name)
        local_addresses = network.get_local_host_addresses()
"
2	"from __future__ import absolute_import
from __future__ import division
"
4	"hyena, hyaena
red fox, Vulpes vulpes
kit fox, Vulpes macrotis
"
4	"
    @staticmethod
    def encode(blocks_args):
        """"""
"
2	"                    max_difference = tf.reduce_max(tf.abs(summed - multiplied))

                    if size <= 3 or dtype in [tf.int32, tf.int64]:
"
2	"      cache-from: test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0:823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite:SLUG-test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0-latest
      config: docker-compose.test.yml
"
0	"
    # -- arch the edges
    res = prop.resolution // 2
    radius = min(length, width) / 2
"
4	"        results_4 = self.module.classify(
            images=test_images, use_gpu=True, top_k=2)
        for res in results_4:
            self.assertEqual(len(res.keys()), 2)
"
4	"bullet train, bullet
butcher shop, meat market
cab, hack, taxi, taxicab
"
4	"            each['org_im_width'], each['org_im_height'] = each['org_im'].size
            component.append(each)
    if images is not None:
"
2	"    automatic: true
  agents:
"
4	"            bn_name = name + bn_name
            param_attr, bias_attr = init_batch_norm_layer(bn_name)
"
4	"        else:
            bias_attr = False
"
2	"            yield g, m


@contextlib.contextmanager
def mpi_implementation_flags(flags=[""--mock-mpi-impl-flags""],
"
1	"    ):
        assert check_argument_types()
        if len(path_name_type_list) == 0:
            raise ValueError(
"
1	"

class Transformer(AbsTTS):
"
2	"
        self._wait_hosts_cond.acquire()
        try:
"
1	"    for key, data in dataset:
        if key == ""a"":
            assert all((data[""data8""]) == np.array([1.4, 3.4], dtype=np.float32))
"
2	"            # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.
            hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, initial_lr=scaled_lr, verbose=verbose),

            # Reduce LR if the metric is not improved for 10 epochs, and stop training
            # if it has not improved for 20 epochs.
"
4	"    multiplier = global_params.width_coefficient
    if not multiplier:
        return filters
    divisor = global_params.depth_divisor
    min_depth = global_params.min_depth
"
3	"        return result

    @property
    def raw_ndarray(self):
        if self._raw_ndarray is None:
"
4	"head cabbage
broccoli
cauliflower
zucchini, courgette
spaghetti squash
"
2	"    std::vector<size_t> count_vec(recvcounts.get(), recvcounts.get() + size_);
    opts.setOutput(buffer.get(), count_vec);
"
1	"            # calculate for encoder-decoder
            if ""encoder-decoder"" in self.modules_applied_guided_attn:
                att_ws = []
                for idx, layer_idx in enumerate(
                    reversed(range(len(self.decoder.decoders)))
"
2	"        assert driver._host_manager.current_hosts.count_available_slots() >= 16
        driver.stop()

        # Notify coordinator 2 times, as the first time we are below min_np and the existing host assignments
        # are empty
"
2	"            if exit_code != 0:
                print(
                    'Launching horovod task function was not '
"
2	"    /* Cleanup */
    for (size_t ec = 0; ec < entries.size(); ++ec) {
"
4	"cliff, drop, drop-off
coral reef
geyser
"
1	"@pytest.mark.parametrize(
    ""loader_type"", [""text_int"", ""text_float"", ""csv_int"", ""csv_float"", ""dummy""]
)
def test_load_num_sequence_text(loader_type: str, tmp_path: Path):
"
4	"            conv = fluid.layers.swish(
                self._expand_conv_norm(conv, block_args, is_test, name))

"
2	"        self.host = host_hash
        """"""Host hash.""""""
"
1	"        n_feat (int): the number of features
        dropout_rate (float): dropout_rate
        kernel_size_str (str): kernel size (length)
        lnum (inst): index of layer
        use_kernel_mask (bool): Use causal mask or not for convolution kernel
"
3	"            d.traceback = traceback.format_exc()
            d.time.GetCurrentTime()
            self.logger.error(ex, exc_info=True)
"
1	"if [ ${stage} -le 3 ] && [ ${stop_stage} -ge 3 ]; then
    echo ""stage 3: LM Preparation""
    ### Removed wsj texts. By default we use the 65000 words pretrained wsj LM.
"
2	"            labels = qu(features)
        optimizer.zero_grad()
        outputs = net(features)
"
4	"        top_k (int): Return top k results.
    """"""
"
4	"pillow
ping-pong ball
pinwheel
"
2	"        return driver_service.get_common_interfaces(settings, current_hosts.host_assignment_order)

    exec_command = _exec_command_fn(settings)
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: 2x-gpu-g4
"
2	"
    Args:
        model: Keras model.
        optimizer: Optional optimizer, can be compiled into model instead.
        kwargs: Additional properties to sync, will be exposed as attributes of the object.
"
2	"            self.do_test_rsh(command, 143, events=events)
            duration = time.time() - start

            self.assertGreaterEqual(duration, event_delay)
            self.assertLess(duration, sleep - 1.0, 'sleep should not finish')
"
4	"
    Yield:
        each (collections.OrderedDict): info of original image, preprocessed image.
"
4	"mantis, mantid
cicada, cicala
leafhopper
lacewing, lacewing fly
dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk
"
2	"      elastic_tensorflow=""test_elastic_tensorflow2.py""
      elastic_spark_tensorflow=""test_elastic_spark_tensorflow2.py""
  fi
"
2	"  - docker-compose#v2.6.0:
      run: test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2
      config: docker-compose.test.yml
      pull-retries: 3
"
4	"howler monkey, howler
titi, titi monkey
"
4	"                batch_image
            ]) if use_gpu else self.cpu_predictor.run([batch_image])
            out = postprocess(
                data_out=predictor_output[0].as_ndarray(),
"
2	"            result = procs.mapPartitionsWithIndex(mapper).collect()
            result_queue.put(result)
"
2	"            assert exit_code == 0, name

        assert len(rank_results) == 2
"
4	"pajama, pyjama, pj's, jammies
palace
panpipe, pandean pipe, syrinx
paper towel
parachute, chute
"
4	"    img /= img_std
    return img


"
4	"        use_se=use_se)
    return model


"
4	"        num_classes=1000,
        width_coefficient=width_coefficient,
"
6	"            :class:`torch_geometric.nn.conv.MessagePassing`.
    """"""
    def __init__(self, in_channels: int, out_channels: int,
                 aggregators: List[str], scalers: List[str], deg: Tensor,
                 edge_dim: Optional[int] = None, towers: int = 1,
"
6	"                modules += [ReLU()]
                modules += [Linear(self.F_in, self.F_in)]
            self.pre_nns.append(Sequential(*modules))

            in_channels = (len(aggregators) * len(scalers) + 1) * self.F_in
"
4	"hair spray
half track
hammer
hamper
hand blower, blow dryer, blow drier, hair dryer, hair drier
"
4	"scabbard
scale, weighing machine
school bus
schooner
scoreboard
"
4	"        """"""
        self.arg_input_group.add_argument(
            '--input_path', type=str, help=""path to image."")

"
4	"        API for image classification.

        Args:
            images (list[numpy.ndarray]): data of images, shape of each is [H, W, C], color space must be BGR.
"
4	"spotted salamander, Ambystoma maculatum
axolotl, mud puppy, Ambystoma mexicanum
"
4	"screen, CRT screen
screw
screwdriver
seat belt, seatbelt
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
"
2	"    net = Net(args.mode)
    optimizer = torch.optim.SGD(
        net.parameters(),
        lr=args.learning_rate,
"
4	"coucal
bee eater
"
2	"      run: test-cpu-gloo-py3_7-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
2	"        actual_secret = actual_env.pop(secret.HOROVOD_SECRET_KEY, None)

"
4	"ice lolly, lolly, lollipop, popsicle
French loaf
"
4	"                 - 1(default): There's only one data to be feeded in the model, e.g. the module is used for text classification task.
                 - 2: There are two data to be feeded in the model, e.g. the module is used for text matching task (point-wise).
"
2	"
        if args.epoch_wait > 0:
"
2	"                                           args_list,
                                           block_until_all_done=False)
"
4	"hourglass
iPod
iron, smoothing iron
"
4	"tabby, tabby cat
tiger cat
Persian cat
Siamese cat, Siamese
"
2	"- label: ':fire: Test PyTorch MNIST (test-gpu-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c "" \$(cat /mpirun_command) python /horovod/examples/pytorch_mnist.py""
  plugins:
"
4	"
    Args:
"
4	"                batch_image
            ]) if use_gpu else self.cpu_predictor.run([batch_image])
"
1	"    exit 1
fi
"
2	"    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
    @mock.patch('horovod.run.gloo_run._get_min_start_hosts', return_value=1)
    def test_reset_limit(self, mock_get_min_start_hosts):
        discovery_schedule = [
            (0, ['localhost:2']),
"
2	"    queue: cpu
- label: ':docker: Build test-cpu-oneccl-ofi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0'
  plugins:
  - docker-compose#6b0df8a98ff97f42f4944dbb745b5b8cbf04b78c:
"
4	"geyser
lakeside, lakeshore
"
2	"    queue: 2x-gpu-g4
- label: ':terminal: Test Horovodrun (test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
"
2	"def get_local_intfs(nic=None):
    common_intfs = set()
    for iface, addrs in net_if_addrs().items():
        if nic and iface != nic:
"
4	"            bias_attr = fluid.ParamAttr(
                name=name + ""_offset"",
                initializer=fluid.initializer.Constant(0.0))
        else:
"
2	"        driver.start(np=2, create_worker_fn=exec_command)
        res = driver.get_results().worker_results
        driver.stop()

        assert len(res) == 2
"
1	"        if mask is not None and not self.use_kernel_mask:
            mask = mask.transpose(-1, -2)
            x = x.masked_fill(mask == 0, 0.0)

        # second linear layer
"
2	"
    def __init__(self, *args, **kwargs):
        training_script = os.path.join(os.path.dirname(__file__), 'data/elastic_torch_main.py')
"
2	"58a59,61
>     # Location of discovery script on local filesystem.
>     DISCOVERY_SCRIPT = 'get_gpu_resources.sh'
"
3	"
    def test_prune_driver(self):
        f = (
"
4	"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"
4	"mitten
mixing bowl
"
4	"patas, hussar monkey, Erythrocebus patas
baboon
macaque
langur
colobus, colobus monkey
"
4	"stethoscope
stole
stone wall
stopwatch, stop watch
"
4	"            self.gpu_predictor = create_paddle_predictor(gpu_config)

    def context(self,
                trainable=True,
"
2	"data = tf.random.uniform([args.batch_size, 224, 224, 3])
target = tf.random.uniform([args.batch_size, 1], minval=0, maxval=999, dtype=tf.int64)


@tf.function
"
2	"        if not log:
            return None

        timer = time.time()
        timeout = 5.0
"
4	"grocery store, grocery, food market, market
guillotine
hair slide
"
4	"
paddlepaddle >= 1.6.2

paddlehub >= 1.6.0
"
4	"anemone fish
sturgeon
"
4	"bullfrog, Rana catesbeiana
tree frog, tree-frog
tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui
loggerhead, loggerhead turtle, Caretta caretta
"
2	"                    label_prob = row.label_prob.toArray().tolist()
                    assert label_prob[int(row.label_pred)] == max(label_prob)

    @pytest.mark.skipif(LooseVersion(tf.__version__) == '1.14.0',
                        reason='This test segfaults with Tensorflow 1.14.0: '
"
3	"
    def __init__(
            self,
            model_url: str = 'https://tfhub.dev/google/universal-sentence-encoder/4',
"
4	"            use_bias=True,
            padding_type=self.padding_type,
"
6	"        assert x[0].dim() == 2, 'Static graphs not supported in `GravNetConv`.'

"
4	"tiger, Panthera tigris
cheetah, chetah, Acinonyx jubatus
brown bear, bruin, Ursus arctos
"
4	"
```python
def get_expected_image_width()
```
"
4	"$ hub serving start -m efficientnetb7_imagenet
```

这样就完成了一个在线图像识别服务化API的部署，默认端口号为8866。

"
4	"brass, memorial tablet, plaque
brassiere, bra, bandeau
"
2	"            data, target = data.cuda(), target.cuda()
        output = model(data)
        # sum up batch loss
        test_loss += F.nll_loss(output, target, size_average=False).item()
        # get the index of the max log-probability
"
2	"                results[i] = read_data_from_kvstore(driver_ip, run_func_server_port,
                                                    'runfunc_result', str(i))
"
2	"
class WorkerStateRegistry(object):
    def __init__(self, driver, host_manager, reset_limit=None, verbose=False):
        self._driver = driver
"
3	"    def __init__(self, pruned=('text',), level='doc', *args, **kwargs):
        super().__init__(pruned, level, *args, **kwargs)

"
2	"                pass
    else:
        fn = target

    bg = threading.Thread(target=fn, args=args, name=name)
"
2	">                 K.set_session(tf.Session(config=config))
>             else:
>                 # Do not use GPUs for prediction, use single CPU core per task.
>                 config = tf.ConfigProto(device_count={'GPU': 0})
>                 config.inter_op_parallelism_threads = 1
"
4	"                }
                add_vars_prefix(context_prog, name_prefix)
                add_vars_prefix(startup_prog, name_prefix)
"
6	"        else:
            x = x.view(-1, 1, self.F_in).repeat(1, self.towers, 1)

        # propagate_type: (x: Tensor, edge_attr: OptTensor)
"
2	"        # Broadcast options like learning rate
        for option_key, option_value in group.items():
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
4	"lotion
loudspeaker, speaker, speaker unit, loudspeaker system, speaker system
loupe, jeweler's loupe
lumbermill, sawmill
magnetic compass
"
5	"        layout.createText(s.talon, 'n')
        x += layout.XS
"
4	"standard poodle
Mexican hairless
timber wolf, grey wolf, gray wolf, Canis lupus
white wolf, Arctic wolf, Canis lupus tundrarum
"
2	"    except RuntimeError as e:
        raise HorovodInternalError(e)

"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
2	"    :param command: command to execute
    :param env: environment variables to use
    :return: (output, exit code) or None on failure
"
2	"                                         label_cols=['label'],
                                         batch_size=args.batch_size,
"
4	"#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
"
6	"            data_list.append(data)
        return data_list

    def __repr__(self):
        return '{}({})'.format(self.name, len(self))
"
2	"      run: test-gpu-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
4	"

if __name__ == '__main__':
"
2	"# You may obtain a copy of the License at
#
"
2	"                ('spark.worker.resource.gpu.amount', str(gpus)),
                ('spark.task.resource.gpu.amount', '1'),
                ('spark.executor.resource.gpu.amount', str(gpus))
            ])
"
5	"        layout.createText(s.talon, 'n')
        layout.createRoundText(s.talon, 'nnn')
        x -= layout.XS
"
4	"    return int(math.ceil(multiplier * repeats))

"
4	"
    global_params = GlobalParams(
        batch_norm_momentum=0.99,
"
2	"                            self.assertEqual(expected_task_exec_kwargs, task_exec_kwargs, msg)

    def test_gloo_exec_fn(self):
"
4	"wild boar, boar, Sus scrofa
warthog
hippopotamus, hippo, river horse, Hippopotamus amphibius
ox
"
4	"#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
4	"dock, dockage, docking facility
dogsled, dog sled, dog sleigh
dome
"
1	"                data_path_and_name_and_type=args.valid_data_path_and_name_and_type,
                shape_files=args.valid_shape_file,
                batch_size=args.valid_batch_size,
                batch_bins=args.valid_batch_bins,
"
4	"        indexs = np.argsort(result_i)[::-1][0:top_k]
        for index in indexs:
            label = label_list[index].split(',')[0]
            output_i[label] = float(result_i[index])
        output.append(output_i)
"
1	"
    def encode(self, x):
        """"""Encode acoustic features.

        :param ndarray x: source acoustic feature (T, D)
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
"
4	"    @staticmethod
    def encode(blocks_args):
        """"""
        Encodes a list of BlockArgs to a list of strings.
"
2	"  plugins:
  - docker-compose#v2.6.0:
      run: test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
"
4	"
        results_4 = self.module.classify(
            images=test_images, use_gpu=True, top_k=2)
"
4	"reel
reflex camera
refrigerator, icebox
remote control, remote
restaurant, eating house, eating place, eatery
"
2	"

def sequence(lst):
"
4	"        self._bn_mom = self._global_params.batch_norm_momentum
        self._bn_eps = self._global_params.batch_norm_epsilon
"
4	"        return im_std

"
2	"    target = torch.LongTensor(batch_size).random_() % 2

    v = 1.0
"
4	"            use_gpu = True
        except:
            use_gpu = False
        if use_gpu:
            gpu_config = AnalysisConfig(self.default_pretrained_model_path)
"
2	"            self.assertEqual(expected_command, actual_command)

"
4	"                   padding_type='SAME',
                   override_params=None,
"
4	"        """"""
        Add the command config options.
"
4	"        bound = 1 / math.sqrt(fan_in)
        param_attr = fluid.ParamAttr(
            name=name + ""_weights"",
            initializer=fluid.initializer.Uniform(low=-bound, high=bound))
"
4	"                 op_type,
                 fan_out,
                 init=""google"",
                 use_bias=False,
"
4	"           use_bias=False,
           padding_type=None,
           initial=""normal"",
           use_cudnn=True):

"
2	"  command: horovodrun -np 2 -H localhost:2 --gloo python /horovod/examples/keras_mnist_advanced.py
  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
"
1	"        with pytest.raises(TypeError):
            sub[""bb""] = 1
        with pytest.raises(RuntimeError):
"
2	"                url = self._url_re.match(line)
                if url:
"
1	"
. ./path.sh || exit 1;
. ./cmd.sh || exit 1;

"
4	"vending machine
vestment
viaduct
violin, fiddle
"
2	"        return vocab

"
2	"
    @contextmanager
    def skip_synchronize(self):
        raise AssertionError(""Skipping synchronization is not supported when using Adasum optimizer."")

"
4	"Band Aid
banjo
bannister, banister, balustrade, balusters, handrail
barbell
barber chair
"
2	"                 {'host-1': 4, 'host-2': 8},
                 {'host-1': 4, 'host-2': 8, 'host-3': 4}]
        mock_discovery = mock.Mock()
        mock_discovery.find_available_hosts_and_slots.side_effect = sequence(slots)

"
4	"macaw
sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita
lorikeet
coucal
bee eater
"
4	"tarantula
wolf spider, hunting spider
tick
centipede
black grouse
"
2	"model.add(Dense(num_classes, activation='softmax'))

# Horovod: adjust learning rate based on number of GPUs.
opt = keras.optimizers.Adadelta(lr * hvd.size())
"
4	"                 padding_type='SAME',
                 override_params=None,
                 is_test=False,
                 use_se=True):
"
2	"                            np=2, extra_conf=[conf.SPARK_CONF_ALWAYS_RESTART_FAILED_TASK,
                                              conf.SPARK_CONF_BLACKLIST_DISABLED])

        self.assertEqual(3, len(results))
"
2	"
    """"""
    Tests mpi_run with PYTHONPATH set in env.
"
1	"            # 1. Read a line from each file
            while True:
                keys = []
                values = []
                for f in files:
"
4	"
# 发送HTTP请求
data = {'images':[cv2_to_base64(cv2.imread(""/PATH/TO/IMAGE""))]}
headers = {""Content-type"": ""application/json""}
"
4	"            for index, data in enumerate(data_list):
                inputs['text_%s' % (index + 1)] = data
                outputs['emb_%s' % (index + 1)] = main_program.global_block(
                ).vars[prefix_name + emb_name_list[index]]
"
4	"rule, ruler
running shoe
safe
safety pin
"
3	"                f'max roundtrip time: {np.max(durations)}\n'
                f'mean roundtrip time: {np.mean(durations)}\n'
            )
"
4	"
                place = fluid.CPUPlace()
"
4	"coral reef
geyser
"
4	"African crocodile, Nile crocodile, Crocodylus niloticus
American alligator, Alligator mississipiensis
triceratops
"
1	"            label = np.append(label, label[0])
            return label
"
4	"        name='b7',
        is_test=is_test,
"
3	"            if self.num_dim and self.dtype:
                with gzip.open(abspath, 'rb') as fp:
                    result = np.frombuffer(fp.read(), dtype=self.dtype).reshape([-1, self.num_dim])
        except EOFError:
"
2	"              must be a `horovod.common.elastic.State` object used to synchronize state across
              workers.
    """"""
"
6	"            arg_types = OrderedDict((k, v) for k, v in zip(args, arg_types))
            return_type = return_type.split('#')[0].strip()
            out.append((arg_types, return_type))
"
1	"        logging.basicConfig(
            level=logging.DEBUG,
            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",
        )
    else:
"
1	"    def score_partial_(self, y, next_token, state, x):
        """"""Score interface for both full and partial scorer.

        Args:
"
2	"class SleepRequest(object):
    pass


"
4	"whiskey jug
whistle
"
6	"    model.eval()

    total_error = 0
    for data in loader:
        data = data.to(device)
"
2	"
class HorovodInternalError(RuntimeError):
    """"""Internal error raised when a Horovod collective operation (e.g., allreduce) fails.

"
2	"    queue: cpu
- label: ':docker: Build test-gpu-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0'
  plugins:
"
1	"
    ${cuda_cmd} --gpu ${ngpu} ${expdir}/train.log \
        asr_train.py \
        --config ${train_config} \
"
4	"返回预处理的图片均值，也就是 \[0.485, 0.456, 0.406\]。

```python
def get_pretrained_images_std()
"
2	"      run: test-mixed-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
"
4	"American chameleon, anole, Anolis carolinensis
whiptail, whiptail lizard
agama
frilled lizard, Chlamydosaurus kingi
"
4	"        """"""
        images_decode = [base64_to_cv2(image) for image in images]
        results = self.classify(images=images_decode, **kwargs)
        return results

"
4	"dough
meat loaf, meatloaf
"
4	"spotted salamander, Ambystoma maculatum
axolotl, mud puppy, Ambystoma mexicanum
bullfrog, Rana catesbeiana
"
4	"balloon
ballpoint, ballpoint pen, ballpen, Biro
Band Aid
banjo
bannister, banister, balustrade, balusters, handrail
"
1	"from typing import Collection
from typing import Dict
from typing import Iterable
from typing import Tuple
"
2	"        fn.assert_not_called()
        event.set()
        time.sleep(0.1)
        fn.assert_not_called()
"
2	"from horovod.torch.optimizer import DistributedOptimizer


def broadcast_parameters(params, root_rank):
"
4	"                    size=[dict_dim, 128],
                    padding_idx=dict_dim - 1,
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
"
2	"                                       [17 * size] + [17] * (dim - 1))))

            for i in range(size):
                rank_tensor = tf.slice(gathered,
"
4	"obelisk
oboe, hautboy, hautbois
"
4	"from efficientnetb0_imagenet.layers import conv2d, init_batch_norm_layer, init_fc_layer

__all__ = [
    'EfficientNet', 'EfficientNetB0_small', 'EfficientNetB0', 'EfficientNetB1',
"
4	"    """""" Calculate and round number of filters based on depth multiplier. """"""
    multiplier = global_params.width_coefficient
    if not multiplier:
        return filters
"
0	"    for v in face.verts:
        for i in range(prop.floor_count):
            cube = create_cube_without_faces(
                bm, (col_w, col_w, prop.floor_height),
                (v.co.x, v.co.y, v.co.z + (pos_h * (i+1)) + ((prop.floor_height / 2) * i)), bottom=True)
"
1	"    assert check_argument_types()
    if loader_type == ""text_int"":
        delimiter = "" ""
        dtype = int
    elif loader_type == ""text_float"":
"
4	"tripod
triumphal arch
trolleybus, trolley coach, trackless trolley
"
4	"barrel, cask
barrow, garden cart, lawn cart, wheelbarrow
baseball
basketball
bassinet
"
4	"
from __future__ import absolute_import
"
4	"# -*- coding:utf-8 -*-
# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
"
4	"            bn_act=None,
            padding_type=self.padding_type,
            bn_mom=self._bn_mom,
"
2	"        self._host_assignments = {}
        self._rank_assignments = {}
        self._world_size = 0
"
4	"                    'feature_map': name_prefix + feature_map.name
                }
                add_vars_prefix(context_prog, name_prefix)
"
2	"        :param env: dict representing environment variables
        :param extra_env: additional variables to be added to env
        """"""
        for key, value in extra_env.items():
"
4	"        return im_mean

"
4	"# limitations under the License.

from __future__ import absolute_import
from __future__ import division
"
2	"        # from wait_for_command_termination too quickly, so we are safe here to shutdown
        # clients have had enough time to connect to the service already
        #
        # the shutdown has to block on running requests (wait_for_command_exit_code)
"
2	"            nics = _driver_fn(all_host_names, local_host_names, settings, fn_cache=fn_cache)

            if settings.verbose >= 2:
                print('Interfaces on all the hosts were successfully checked.')
"
1	"        self.linear1 = nn.Linear(n_feat, n_feat * 2)
        self.linear2 = nn.Linear(n_feat, n_feat)
        self.linear_weight = nn.Linear(n_feat, self.wshare * 1 * self.kernel_size)
        nn.init.xavier_uniform(self.linear_weight.weight)
"
2	"                'Unable to find a set of common task-to-task communication interfaces: %s'
                % [(index, driver.task_addresses_for_tasks(index))
                   for index in range(num_hosts)])
        return nics
"
2	"    # Horovod: add Horovod Distributed GradientTape.
    if allreduce:
        tape = hvd.DistributedGradientTape(tape)
"
4	"red wolf, maned wolf, Canis rufus, Canis niger
coyote, prairie wolf, brush wolf, Canis latrans
dingo, warrigal, warragal, Canis dingo
dhole, Cuon alpinus
African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus
"
1	"        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
        weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
        weight_new = weight_new.view(B, H, T, T + k - 1).fill_(float(""-inf""))
"
2	"        def exec_command(slot_info, events):
            driver.record_ready(slot_info.hostname, slot_info.local_rank)
            return 1, time.time()

"
4	"        self.add_module_config_arg()
        self.add_module_input_arg()
        args = self.parser.parse_args(argvs)
"
2	"                        with pytest.raises(Exception) as e:
                            horovod.spark.run(fn, args=args, kwargs=kwargs,
                                              num_proc=num_proc, start_timeout=10,
                                              use_mpi=True, use_gloo=False,
                                              extra_mpi_args=extra_mpi_args, env=env,
"
2	"        return HostInfo(hostname, int(slots))


"
4	"lawn mower, mower
lens cap, lens cover
letter opener, paper knife, paperknife
"
4	"    w_end = w_start + size
    h_end = h_start + size
    img = img.crop((w_start, h_start, w_end, h_end))
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
1	"        input = ""卡尔普陪外孙玩滑梯。""
        output = [
            ""ka3"",
            ""er3"",
"
2	"        with mock.patch(""horovod.run.mpi_run._get_mpi_implementation_flags"", side_effect=mpi_impl_flags):
            with mock.patch(""horovod.run.mpi_run.safe_shell_exec.execute"", return_value=0) as execute:
"
4	"ptarmigan
ruffed grouse, partridge, Bonasa umbellus
prairie chicken, prairie grouse, prairie fowl
"
2	"                self.assertIsNotNone(mpi_flags)
                expected_command = ('mpirun '
"
2	"        self._worker_clients = {}

        self._worker_registry = WorkerStateRegistry(self, self._host_manager, reset_limit=reset_limit)
        self._results = ResultsRecorder()
"
4	"            model_filename=model_filename,
            params_filename=params_filename)

    @serving
"
2	"        sz = torch.IntTensor([0])
        broadcast_(sz, root_rank, name + '.sz')
"
4	"        """"""
        Run as a command.
"
1	"ngpu=0         # number of gpus (""0"" uses cpu, otherwise use gpu)
debugmode=1
verbose=1      # verbose option

# feature configuration
"
4	"    def encode(blocks_args):
        """"""
"
4	"silky terrier, Sydney silky
soft-coated wheaten terrier
West Highland white terrier
Lhasa, Lhasa apso
flat-coated retriever
"
2	"    state_dict = {
        'epoch': state.epoch,
        'batch': state.batch,
        'commits': state.commits,
        'hostname': hostname,
"
4	"    summary=""Chinese word embedding based on the SkipGram."",
    author=""baidu-nlp"",
    author_email="""",
    type=""nlp/semantic_model"")
class Word2vecSkipGram(hub.Module):
"
4	"                      num_groups=1,
                      padding_type=""SAME"",
                      conv_act=None,
                      bn_act='swish',
"
4	"        im_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3)
        return im_mean

"
4	"        else:
            bias_attr = False
    elif init == 'google':
        n = filter_size * filter_size * fan_out
"
1	"            None,
            ""g2p_en"",
"
2	"                self.skipTest(""MPICH is not testable"")

        self.do_test_run_with_controller_failure(controller, mode, run)

    def do_test_run_with_controller_success(self, controller, mode, run):
"
4	"            's%d%d' % (block.strides[0], block.strides[1]),
            'e%s' % block.expand_ratio,
            'i%d' % block.input_filters,
            'o%d' % block.output_filters
        ]
"
2	"    spark_job_group = 'horovod.spark.run.%d' % job_id.next_job_id()
    driver = driver_service.SparkDriverService(num_proc, max_np,
                                               fn, args, kwargs,
"
2	"
    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
"
4	"Chihuahua
Japanese spaniel
Maltese dog, Maltese terrier, Maltese
Pekinese, Pekingese, Peke
"
4	"```python
def get_expected_image_height()
```
"
2	"
    if settings.verbose >= 2:
"
4	"
    def test_common_apis(self):
        width = self.module.get_expected_image_width()
"
3	"from ..helper import yaml


"
4	"            input=input,
            num_filters=num_filters,
"
4	"                      num_filters,
                      stride=1,
                      num_groups=1,
                      padding_type=""SAME"",
"
4	"lens cap, lens cover
letter opener, paper knife, paperknife
"
4	"                 drop_connect_rate=0.2):
    """""" Get block arguments according to parameter and coefficients. """"""
"
4	"basset, basset hound
beagle
bloodhound, sleuthhound
"
4	"            use_cudnn=use_cudnn,
            name=conv_name,
            use_bias=use_bias)

        if use_bn == False:
"
2	"        self.task_addresses_for_task = task_addresses_for_task
        """"""Map of interface to list of (ip, port) pairs.""""""

"
3	"    width: 64px;
    height: 64px;
    fill: rgba(51, 51, 51, 0.5);
}
"
4	"        self.padding_type = padding_type
        self.use_se = use_se

"
1	"        use_bias (bool): Use bias term or not.

"
2	"                return CommandExitCodeResponse(terminated,
                                               self._command_exit_code if terminated else None)
            finally:
"
3	"
    Details: https://github.com/notAI-tech/deepsegment
    """"""

"
4	"            each['org_im'] = Image.open(im_path)
            each['org_im_width'], each['org_im_height'] = each['org_im'].size
"
2	"    queue: cpu
- label: ':spark: Spark Torch MNIST (test-cpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/pytorch_spark_mnist.py --num-proc 2 --work-dir /work --data-dir /data --epochs 3""
  plugins:
  - docker-compose#v2.6.0:
"
4	"                 input,
                 op_type,
"
1	"from espnet.nets.pytorch_backend.transformer.encoder import Encoder
from espnet.nets.pytorch_backend.transformer.mask import subsequent_mask
"
4	"    def mb_conv_block(self,
                      inputs,
                      block_args,
                      is_test=False,
                      drop_connect_rate=None,
"
4	"                    input_filters=block_args.output_filters, stride=1)
            for _ in range(block_args.num_repeat - 1):
                drop_connect_rate = self._global_params.drop_connect_rate
                if drop_connect_rate:
                    drop_connect_rate *= float(idx) / block_size
"
2	"    libsvm_path = os.path.join(args.data_dir, 'mnist.bz2')
    if not os.path.exists(libsvm_path):
"
2	"  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
"
2	"from horovod.run.runner import is_gloo_used, run_controller
from horovod.run.common.util import timeout, secret
"
4	"        return 224

    def get_pretrained_images_mean(self):
"
4	"import paddle.fluid as fluid
from efficientnetb1_imagenet.layers import conv2d, init_batch_norm_layer, init_fc_layer

__all__ = [
"
4	"diamondback, diamondback rattlesnake, Crotalus adamanteus
sidewinder, horned rattlesnake, Crotalus cerastes
trilobite
harvestman, daddy longlegs, Phalangium opilio
"
4	"

"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
4	"                                          '_blocks.' + str(idx) + '.')
                idx += 1

        return conv

"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
4	"chiffonier, commode
chime, bell, gong
china cabinet, china closet
Christmas stocking
"
4	"        if block_args.expand_ratio != 1:
            conv = self.conv_bn_layer(
                inputs,
                num_filters=oup,
"
6	"                k: sanitize(str(v))
                for k, v in self.propagate_type.items()
            }
        else:
"
4	"

```python
"
4	"        element['image'] = process_image(element['org_im'])
        yield element

"
2	"    # Split into training & validation.
    # Test set is in 2015, use the same period in 2014 from the training set as a validation set.
"
1	"            help=""Use multiple iterator mode"",
        )

"
4	"            num_squeezed_channels = max(
                1, int(block_args.input_filters * block_args.se_ratio))
            conv = self.se_block(conv, num_squeezed_channels, oup, name)

"
4	"
        total_num = len(all_data)
        loop_num = int(np.ceil(total_num / batch_size))
"
0	"        dummy.normal = Vector((0, 0, 1))
        x, y, z = btools.utils.local_xyz(dummy)
"
2	"import os
import sys
import time

duration = int(sys.argv[1])
"
1	"    kwargs = vars(args)
    split_scps(**kwargs)
"
2	"train_dataset = \
    datasets.MNIST('data-%d' % hvd.rank(), train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
"
4	"* use\_gpu (bool): 是否使用 GPU 来预测；
* top\_k (int): 返回预测结果的前 k 个。

"
4	"suspension bridge
swab, swob, mop
sweatshirt
"
2	"        if not _check_all_hosts_ssh_successful(remote_host_names, args.ssh_port, fn_cache=fn_cache):
            raise RuntimeError('could not connect to some hosts via ssh')
"
2	"    # Freeze the process to prevent it from spawning any new children.
    try:
        p.send_signal(signal.SIGSTOP)
"
2	">             if GPU_INFERENCE_ENABLED:
>                 from pyspark import TaskContext
>                 config = tf.ConfigProto()
"
4	"           groups=None,
           name=""conv2d"",
           norm=None,
           act=None,
           relufactor=0.0,
"
2	"parser.add_argument('--epochs', type=int, default=3,
                    help='number of epochs')
parser.add_argument('--epoch-wait', type=int, default=0,
                    help='number of seconds each epoch takes')
parser.add_argument('--logfile', default='/tmp/logfile.txt',
"
2	"                mean_dy = sum_dy / count_all_sum
                mean_dy_xmu = sum_dy_xmu / count_all_sum
"
4	"
    return blocks_args, global_params


"
2	"def generate_jsrun_rankfile(settings, path=None):
    """"""
    Generates rankfile to use with jsrun.
"
4	"        w_start = np.random.randint(0, width - size + 1)
        h_start = np.random.randint(0, height - size + 1)
"
4	"# limitations under the License.

import os
import time
"
2	"            self._assign_fn(var, value)

    def _load_var(self, var, value):
"
3	"    def _load_gzip(self, abspath):
        self.logger.info(f'loading index from {abspath}...')
        if not path.exists(abspath):
            self.logger.warning('numpy data not found: {}'.format(abspath))
            return None
"
4	"cardoon
mushroom
Granny Smith
"
1	"            ax.set_title(f""{key}"")
            ax.set_xlabel(""Output"")
            ax.set_ylabel(""Stop probability"")
            ax.set_ylim(0, 1)
            ax.grid(which=""both"")
"
1	"        raise NotImplementedError
    assert phoneme_tokenizer.text2tokens(input) == output


def test_token2text(phoneme_tokenizer: PhonemeTokenizer):
"
1	"                    ]
                    if idx + 1 == self.num_layers_applied_guided_attn:
                        break
                att_ws = torch.cat(att_ws, dim=1)  # (B, H*L, T_out, T_in)
                enc_dec_attn_loss = self.attn_criterion(att_ws, ilens, olens_in)
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: cpu
"
3	"
    def __str__(self):
        return self.__class__.__name__
"
4	"            bn_name='_bn1')

"
2	"                exec.assert_called_once()
                args, kwargs = exec.call_args
                executable, args, env = args
"
4	"            default=False,
            help=""whether use GPU or not."")
        self.arg_config_group.add_argument(
            '--batch_size',
"
4	"worm fence, snake fence, snake-rail fence, Virginia fence
wreck
yawl
yurt
"
2	"        hargs.nics = []
        hargs.verbose = 2
        hargs.disable_cache = True
"
2	"if __name__ == '__main__':
    args = parser.parse_args()
"
4	"## 服务部署

PaddleHub Serving可以部署一个在线图像识别服务。

## 第一步：启动PaddleHub Serving
"
4	"            expand_ratio=int(options['e']),
            id_skip=('noskip' not in block_string),
            se_ratio=float(options['se']) if 'se' in options else None,
"
5	"                        '(?P<dest_idx>[0-9]+))\\s*', move_s)

                    if not m:
                        continue

"
4	"                input=conv,
                act=bn_act,
                momentum=bn_mom,
"
2	"            # copy env so we do not leak env modifications
            env = copy.copy(env)
            # copy var over from os.environ
"
4	"        valid_names = ['b' + str(i) for i in range(8)]
        assert name in valid_names, 'efficient name should be in b0~b7'
        model_name = 'efficientnet-' + name
        self._blocks_args, self._global_params = get_model_params(
"
4	"        for im_path in paths:
            each = OrderedDict()
"
1	"
        for t in six.moves.range(1, lpz.size(0)):
"
4	"    w_end = w_start + size
    h_end = h_start + size
"
0	"
    # -- if the facemap already has a material assigned, assign the new faces to the material
"
2	"            batches_per_commit: Number of batches to complete between each commit (default: 1).
        """"""
        super(CommitStateCallback, self).__init__(tf.keras.backend, state, batches_per_commit)
"
1	"        for t in self.cleaner_types:
            if t == ""tacotron"":
                text = tacotron_cleaner.cleaners.custom_english_cleaners(text)
            elif t == ""jaconv"":
"
4	"                 is_test=False,
                 use_se=True):
        valid_names = ['b' + str(i) for i in range(8)]
        assert name in valid_names, 'efficient name should be in b0~b7'
"
4	"import os

"
4	"beacon, lighthouse, beacon light, pharos
beaker
bearskin, busby, shako
beer bottle
"
6	"        deg = deg.clamp_(1).view(-1, 1, 1)

"
4	"

"
6	"
    t = '(Tensor, Tensor, Size) -> Tensor'
    jit = torch.jit.script(conv.jittable(t))
    assert jit(x1, edge_index).tolist() == out.tolist()
    assert jit(x1, edge_index, size=(4, 4)).tolist() == out.tolist()
"
2	"        self.assertEqual(1, results[0]['rendezvous'])

        # either host with rank 0 got removed, or host with rank 1
"
4	"            if len(splits) >= 2:
                key, value = splits[:2]
"
4	"potter's wheel
power drill
"
4	"            args.append('noskip')
        return '_'.join(args)
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
    queue: cpu
"
4	"cliff dwelling
cloak
"
4	"                output_filters=round_filters(block_args.output_filters,
                                             self._global_params),
"
2	"                                             first_task_addresses, settings.key,
                                             settings.verbose)
            first_task_client.wait_for_command_termination()

"
4	"def EfficientNetB2(is_test=False,
                   padding_type='SAME',
                   override_params=None,
                   use_se=True):
    model = EfficientNet(
"
4	"sunscreen, sunblock, sun blocker
suspension bridge
swab, swob, mop
"
4	"                drop_connect_rate = self._global_params.drop_connect_rate
                if drop_connect_rate:
                    drop_connect_rate *= float(idx) / block_size
"
2	"
def run_fn(func, reset):
    @functools.wraps(func)
"
1	"

class NgramPartScorer(Ngrambase, PartialScorerInterface):
    """"""Partialscorer for ngram.""""""
"
4	"sulphur butterfly, sulfur butterfly
lycaenid, lycaenid butterfly
starfish, sea star
sea urchin
"
4	"

def reader(images=None, paths=None):
"
4	"                      bn_act='swish',
                      use_cudnn=True,
                      use_bn=True,
                      bn_mom=0.9,
"
4	"clumber, clumber spaniel
English springer, English springer spaniel
Welsh springer spaniel
cocker spaniel, English cocker spaniel, cocker
"
4	"        for block_arg in block_args_copy:
            block_arg = block_arg._replace(
                input_filters=round_filters(block_arg.input_filters,
"
1	"        weight = F.dropout(weight, self.dropout_rate, training=self.training)
        weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k
        weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)
"
4	"earthstar
hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa
bolete
ear, spike, capitulum
"
2	"    def test_shutdown_on_success(self):
        """"""Tests that shutdown event is triggered when one worker succeeds but the others are still working.""""""
        slots = {'host-1': 2, 'host-2': 2}
        discovery = FixedHosts(slots)

"
6	"                                               num_nodes=x[1].size(0))
            elif isinstance(edge_index, SparseTensor):
"
6	"                out = scatter(inputs, index, 0, None, dim_size, reduce='mean')
            elif aggregator == 'min':
                out = scatter(inputs, index, 0, None, dim_size, reduce='min')
            elif aggregator == 'max':
"
2	"- label: ':docker: Build test-cpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0'
  plugins:
  - docker-compose#6b0df8a98ff97f42f4944dbb745b5b8cbf04b78c:
      build: test-cpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0
      image-repository: 823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite
"
6	"    assert conv((x1, x1), adj.t()).tolist() == out1.tolist()

"
6	"    Args:
        remove_faces (bool, optional): If set to :obj:`False`, the
            :obj:`edge_index` tensor will not be removed.
    """"""
"
2	"    # key: cross_rank; value: local_size for this cross_rank
    cross_sizes = collections.defaultdict(int)
"
4	"def initial_type(name,
                 input,
"
4	"        batch_norm_epsilon=1e-3,
        dropout_rate=dropout_rate,
        drop_connect_rate=drop_connect_rate,
        num_classes=1000,
        width_coefficient=width_coefficient,
"
4	"stove
strainer
"
6	"
    def __init__(self, root, transform=None, pre_transform=None):
        super(WordNet18, self).__init__(root, transform, pre_transform)
        self.data, self.slices = torch.load(self.processed_paths[0])

"
1	"        --preprocess-conf ${preprocess_config} \
        --ngpu ${ngpu} \
        --backend ${backend} \
        --outdir ${expdir}/results \
"
2	"
                with override_env({rank_env: 0,
                                   local_rank_env: 1,
"
4	"horizontal bar, high bar
horse cart, horse-cart
"
4	"                         override_params=None,
                         use_se=False):
    model = EfficientNet(
        name='b0',
"
4	"curly-coated retriever
golden retriever
Labrador retriever
Chesapeake Bay retriever
"
4	"        cpu_config.disable_gpu()
        self.cpu_predictor = create_paddle_predictor(cpu_config)

        try:
"
4	"

def EfficientNetB2(is_test=False,
"
6	"        deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)
        deg_inv_sqrt = deg.pow_(-0.5)
        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)
"
4	"                        predicate=_if_exist)
                else:
"
4	"        override_params=override_params,
        use_se=use_se)
    return model
"
2	"    automatic: true
  agents:
    queue: cpu
- label: ':fire: Single PyTorch MNIST (test-cpu-openmpi-py3_6-tf1_6_0-keras2_1_2-torch0_4_1-mxnet1_4_1-pyspark2_3_2)'
  command: bash -c "" python /horovod/examples/pytorch_mnist.py --epochs 3""
"
4	"                phase=""train""):
        """"""context for transfer learning.

        Args:
"
6	"        num_workers (int): Number of workers to use for k-NN computation.
            Has no effect in case :obj:`batch` is not :obj:`None`, or the input
            lies on the GPU. (default: :obj:`1`)
"
2	"import mock
from parameterized import parameterized
import pytest
"
3	"
        with f:
"
2	"# limitations under the License.
# ==============================================================================
"
2	"    def __init__(self, *args, **kwargs):
        training_script = os.path.join(os.path.dirname(__file__), 'data/elastic_tensorflow_main.py')
        super(ElasticTensorFlowTests, self).__init__(training_script, *args, **kwargs)
        warnings.simplefilter('module')
"
1	"        postnet_filts (int, optional): Filter size of postnet.
        use_scaled_pos_enc (bool, optional):
"
2	"                        ""accumulate gradients locally."")
            assert not p.grad.requires_grad
            assert self._allreduce_delay[p] > 0
            handle, ctx = None, None
            self._allreduce_delay[p] -= 1
"
2	"                self.assertAllClose(w, np.ones_like(w))
            assert state.batch == 20
"
2	"
    Args:
"
4	"safety pin
saltshaker, salt shaker
"
6	"data.edge_type = data.edge_type[edge_mask]
data.train_idx = mapping[:data.train_idx.size(0)]
data.test_idx = mapping[data.train_idx.size(0):]

"
6	"            # propagate_type: (x: Tensor, edge_weight: OptTensor)
            out = self.propagate(edge_index, x=xs[-1], edge_weight=edge_weight,
                                 size=None)
            xs.append(out)
"
1	"            ""i"",
            ""w"",
"
4	"                pretrained=True,
                override_params=None,
                phase='train'):
        """"""context for transfer learning.

"
6	"        if isinstance(in_channels, int):
            in_channels = (in_channels, in_channels)

        self.in_channels_l = in_channels[0]

"
4	"           initial=""normal"",
           use_cudnn=True):
"
1	"        )

        # define projection layer
        if self.spk_embed_dim is not None:
            if self.spk_embed_integration_type == ""add"":
"
2	"        var.load(value, self.session)

    def _assign_var(self, var, value):
        var.assign(value)
"
4	"paddle, boat paddle
paddlewheel, paddle wheel
padlock
paintbrush
pajama, pyjama, pj's, jammies
"
4	"
import numpy as np


def base64_to_cv2(b64str):
"
4	"
        ops = block_string.split('_')
        options = {}
        for op in ops:
            splits = re.split(r'(\d.*)', op)
"
4	"Shetland sheepdog, Shetland sheep dog, Shetland
collie
Border collie
Bouvier des Flandres, Bouviers des Flandres
Rottweiler
"
1	"recog_set=""test""
# Available directories:
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
"
3	"    position: absolute;
    height: 300px;
    width: 75%;
    -webkit-transition: all .6s ease-in-out;
    transition: all .6s ease-in-out;
"
2	"        client.wait_for_command_termination(delay=0.1)
        terminated, exit_code = client.command_result()
        self.assertEqual(True, terminated)
        self.assertEqual(0, exit_code)
"
2	"# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"
4	"brass, memorial tablet, plaque
brassiere, bra, bandeau
breakwater, groin, groyne, mole, bulwark, seawall, jetty
"
1	"

@pytest.fixture
def csv_int(tmp_path):
"
2	"    opt = tf.keras.optimizers.Adam(lr=args.learning_rate, epsilon=1e-3)
    opt = hvd.DistributedOptimizer(opt)
    model.compile(opt, 'mae', metrics=[exp_rmspe])
    model_bytes = serialize_model(model)

"
4	"
class BlockDecoder(object):
    """""" Block Decoder for readability, straight from the official TensorFlow repository """"""

"
1	"    return phones


def pypinyin_g2p(text) -> List[str]:
    from pypinyin import pinyin
"
0	"            return {""FINISHED""}

        class DummyOpFail(bpy.types.Operator):
"
6	"    assert torch.allclose(conv1((x, x[:2]), adj.t(), adj.t()), out1[:2],
                          atol=1e-6)
    assert torch.allclose(conv2((out1, out1[:2]), edge_index, edge_index),
                          out2[:2], atol=1e-6)
"
1	"            self.bias = nn.Parameter(torch.Tensor(n_feat))

"
2	"                # we cannot know the exact value of that env variable
                # we test right handling of PYTHONPATH in test_mpi_run_*pythonpath* below
"
1	"            ys = ys[:, :max_olen]
            labels = labels[:, :max_olen]
"
4	"            name=name + '_weights', initializer=fluid.initializer.Constant(1.0))
        bias_attr = fluid.ParamAttr(
            name=name + '_offset',
            initializer=fluid.initializer.Constant(value=0.0))
        return fluid.layers.batch_norm(
"
1	"        batch_size = text.size(0)

"
4	"steel drum
stethoscope
stole
stone wall
stopwatch, stop watch
"
5	"            x += layout.XS
        x += layout.XS
"
6	"        self.adj_t = SparseTensor(
            row=data.edge_index[0], col=data.edge_index[1],
            value=torch.arange(self.E, device=data.edge_index.device),
            sparse_sizes=(N, N)).t()
"
1	"                fold_length=fold_length,
                sort_in_batch=sort_in_batch,
                sort_batch=sort_batch,
                chunk_length=chunk_length,
                chunk_shift_ratio=chunk_shift_ratio,
"
4	"    """"""
    Preprocess to yield image.

"
4	"# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
"
2	"                    help='number of epochs')
parser.add_argument('--epoch-wait', type=int, default=0,
                    help='number of seconds each epoch takes')
parser.add_argument('--logfile', default='/tmp/logfile.txt',
                    help='log file to record results (one line per epoch)')
"
2	"import torch.utils.data.distributed
import horovod.torch as hvd
import os
"
1	"    audio2 = np.random.randint(-100, 100, 16, dtype=np.int16)

    audio1 = audio1.astype(np.float64) / (np.iinfo(np.int16).max + 1)
    audio2 = audio2.astype(np.float64) / (np.iinfo(np.int16).max + 1)

"
6	"                self.node_norm, self.edge_norm = torch.load(path)
            else:
                self.node_norm, self.edge_norm = self.__compute_norm__()
                if save_dir is not None:  # pragma: no cover
                    torch.save((self.node_norm, self.edge_norm), path)
"
1	"
        # second linear layer
        x = self.linear2(x)
"
4	"                    vars=variable_names)
                for param in main_program.global_block().iter_parameters():
                    param.trainable = trainable

                place = fluid.CPUPlace()
"
6	"@pytest.mark.parametrize('requires_grad', [True, False])
def test_agnn_conv(requires_grad):
    x = torch.randn(4, 16)
"
4	"        param_attr, bias_attr = init_fc_layer(class_dim, '_fc')
        out = fluid.layers.fc(
            pool,
            class_dim,
"
1	"

def test_compatible_with_espnet1():
"
4	"            bn_mom=self._bn_mom,
            bn_eps=self._bn_eps,
            name=name,
            use_cudnn=False,
            conv_name=name + '_depthwise_conv',
"
1	"    echo ""X $base"" > ${align_dir}/data/spk2utt
    echo ""$base X"" > ${align_dir}/data/utt2spk
    echo ""$base $text"" > ${align_dir}/data/text
"
2	"    automatic: true
  agents:
    queue: cpu
- label: ':pytest: Run PyTests (test-cpu-openmpi-py3_6-tf2_0_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark2_4_0)'
"
1	"        g2p_type: str,
        non_linguistic_symbols: Union[Path, str, Iterable[str]] = None,
        space_symbol: str = ""<space>"",
        remove_non_linguistic_symbols: bool = False,
    ):
"
2	"
    def _create_id(self, hostname, local_rank):
"
5	"                if m:
                    FCS_VERSION = (int(m.group(1)), int(m.group(2)),
                                   int(m.group(3)))
                else:
                    FCS_VERSION = (0, 0, 0)
"
4	"        else:
            bn_name = name + bn_name
"
4	"French loaf
bagel, beigel
pretzel
cheeseburger
"
2	"       test-cpu-mpich-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0 \
       test-cpu-oneccl-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0 \
"
2	"            return self._task_host_hash_indices.copy()
        finally:
"
2	"        kwargs: Keyword arguments to pass to `fn`.
        num_proc: Number of Horovod processes.  Defaults to `spark.default.parallelism`.
        start_timeout: Timeout for Spark tasks to spawn, register and start running the code, in seconds.
"
1	"            else:
                t = line[0]
                tokens.append(t)
                line = line[1:]
"
2	"#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
"
4	"# -*- coding:utf-8 -*-
# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
"
2	"    def __init__(self, discovery, min_np, max_np, elastic_timeout, reset_limit, **kwargs):
        """"""
        :param discovery: object used to detect and manage available hosts
        :type discovery: horovod.run.elastic.discovery.HostDiscovery
        :param min_np: minimum number of processes
"
0	"                if line.startswith("":"")
              ))
    else :
        items = (item,)

"
4	"apron
ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin
assault rifle, assault gun
backpack, back pack, knapsack, packsack, rucksack, haversack
bakery, bakeshop, bakehouse
"
2	"  plugins:
  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
"
4	"            input=conv, pool_type='avg', global_pooling=True, use_cudnn=False)

        if not is_test and self._global_params.dropout_rate:
            pool = fluid.layers.dropout(
                pool,
"
4	"                label_list=self.label_list,
                top_k=top_k)
            res += out
"
2	"        first_event = exec_command.call_args_list[0][0][2]
        first_host = exec_command.call_args_list[0][0][1].hostname
"
2	"        thread.daemon = True
        thread.start()
"
4	"配置好服务端，以下数行代码即可实现发送预测请求，获取预测结果

```python
import requests
"
2	"# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
"
2	"  retry:
    automatic: true
  agents:
"
2	"    old = os.environ
    try:
        os.environ = env
"
1	"    assert dataset.has_name(""data1"")

    for key, data in dataset:
        if key == ""a"":
"
1	"    p2 = tmp_path / ""feats.ark""
    with kaldiio.WriteHelper(f""ark,scp:{p2},{p}"") as w:
"
4	"            bn_mom=self._bn_mom,
            padding_type=self.padding_type,
            bn_eps=self._bn_eps,
            name='',
            conv_name='_conv_stem',
"
2	"    :type settings: Horovod.run.common.util.settings.Settings
    :return: example: ['eth0', 'eth1']
    :rtype: list[string]
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
"
2	"        sz = tf.convert_to_tensor([t.shape[0]], dtype=tf.int32)
        to_numpy(broadcast(sz, root_rank, name + '.sz'))
    else:
        sz = tf.convert_to_tensor([0], dtype=tf.int32)
"
4	"muzzle
nail
neck brace
"
0	"        res = bpy.ops.btools_test.dummy_op_pass()
        self.assertEqual(res, {""FINISHED""})
        bpy.utils.unregister_class(DummyOpPass)

    def test_restricted_sizeoffset(self):
"
6	"    assert out2.size() == (2, 32)
    assert conv((x1, x2), edge_index, value, (4, 2)).tolist() == out1.tolist()
"
4	"sleeping bag
slide rule, slipstick
sliding door
slot, one-armed bandit
snorkel
"
4	"            stride=stride,
            groups=num_groups,
            act=conv_act,
            padding_type=padding_type,
            use_cudnn=use_cudnn,
"
4	"handkerchief, hankie, hanky, hankey
hard disc, hard disk, fixed disk
"
2	"        obj = cloudpickle.load(buf)

    return obj

"
2	"    :param args: function arguments
    :param stop: event to stop thread
    :type stop: threading.Event
    :param check_stop_interval_s: interval in seconds to check the stop event
    :type check_stop_interval_s: float
"
4	"from efficientnetb6_imagenet.processor import postprocess, base64_to_cv2
from efficientnetb6_imagenet.data_feed import reader
from efficientnetb6_imagenet.efficientnet import EfficientNetB6
"
1	"

@pytest.mark.parametrize(""shuffle"", [True, False])
def test_MultpleIterFactory(shuffle):
"
2	"- label: ':tensorflow: Test TensorFlow 2.0 MNIST (test-cpu-openmpi-py3_6-tf2_0_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""\$(cat /mpirun_command) python /horovod/examples/tensorflow2_mnist.py""
"
4	"    width, height = img.size
    size = target_size
    if center == True:
        w_start = (width - size) / 2
        h_start = (height - size) / 2
"
6	"    adj = adj.sparse_resize((4, 2))
    conv = MyConv((8, 16), 32)
    out1 = conv((x1, x2), edge_index, value)
"
4	"    elif norm_type == 'instance_norm':
        helper = fluid.layer_helper.LayerHelper(""instance_norm"", **locals())
        dtype = helper.input_dtype()
"
4	"    divisor = global_params.depth_divisor
    min_depth = global_params.min_depth
"
4	"aircraft carrier, carrier, flattop, attack aircraft carrier
airliner
airship, dirigible
altar
"
2	"# ? SPARK_CONF_REUSE_NODE_ALWAYS_FOR_SAME_TASK requires SPARK_CONF_REUSE_EXECUTOR_ALWAYS_FOR_SAME_TASK
SPARK_CONF_REUSE_NODE_ALWAYS_FOR_SAME_TASK = ('spark.blacklist.task.maxTaskAttemptsPerNode', SPARK_CONF_MAX_INT_MINUS_ONE)
SPARK_CONF_REUSE_NODE_ONCE_FOR_SAME_TASK = ('spark.blacklist.task.maxTaskAttemptsPerNode', '2')
"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
"
2	"    print('================')
    print('Data preparation')
    print('================')
"
4	"import cv2
import numpy as np
"
4	"        width_padding = right_padding
        if top_padding != bottom_padding or left_padding != right_padding:
            height_padding = top_padding + stride
            width_padding = left_padding + stride
"
2	"
  // Initialize concrete implementations.
  DoInitialization();
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
"
4	"                 - 3: There are three data to be feeded in the model, e.g. the module is used for text matching task (pair-wise).

"
4	"barn
barometer
barrel, cask
barrow, garden cart, lawn cart, wheelbarrow
baseball
"
4	"    def _drop_connect(self, inputs, prob, is_test):
        if is_test:
            return inputs
"
4	"#
#     http://www.apache.org/licenses/LICENSE-2.0
#
"
4	"        raise NotImplementedError(""norm tyoe: [%s] is not support"" % norm_type)

"
2	"  from tensorflow.python.keras.mixed_precision.experimental import device_compatibility_check
  device_compatibility_check.log_device_compatibility_check = lambda policy_name, skip_local: None

"
4	"        """"""
        Add the command config options.
"
4	"clog, geta, patten, sabot
cocktail shaker
"
4	"                bn_mom=self._bn_mom,
                bn_eps=self._bn_eps,
"
3	"
$("".wrap"").click(function () {
    $(""#first"").removeClass(""active"");
"
4	"mortar
mortarboard
mosque
mosquito net
motor scooter, scooter
"
6	"    x = torch.randn(8, 16)
    pos = torch.rand(8, 3)
    batch = torch.tensor([0, 0, 0, 0, 1, 1, 1, 1])
"
2	"
class SparkRendezvousServer(RendezvousServer):
"
3	"        results = []
        for idx, s in enumerate(self._segmenter.segment_long(text)):
            results.append(dict(
                text=s,
"
5	"            layout.createRoundText(self.s.talon, 'se', dx=layout.XS)
        x += layout.XS
"
4	"trilobite
harvestman, daddy longlegs, Phalangium opilio
"
1	"fi

dir=${download_dir}/${models}
"
2	"

"
4	"groenendael
malinois
"
2	"    store_states_csv = spark.read.csv('%s/store_states.csv' % args.data_dir, header=True)
    state_names_csv = spark.read.csv('%s/state_names.csv' % args.data_dir, header=True)
    google_trend_csv = spark.read.csv('%s/googletrend.csv' % args.data_dir, header=True)
    weather_csv = spark.read.csv('%s/weather.csv' % args.data_dir, header=True)
"
6	"    x1 = torch.randn(4, 8)
    x2 = torch.randn(2, 16)
    edge_index = torch.tensor([[0, 1, 2, 3], [0, 0, 1, 1]])
    row, col = edge_index
    value = torch.randn(edge_index.size(1))
"
2	"            self._run(discovery_schedule=discovery_schedule,
                      extra_conf=[conf.SPARK_CONF_ALWAYS_RESTART_FAILED_TASK,
                                  conf.SPARK_CONF_BLACKLIST_DISABLED])

    @mock.patch('horovod.run.elastic.driver.DISCOVER_HOSTS_FREQUENCY_SECS', 0.01)
"
4	"#     http://www.apache.org/licenses/LICENSE-2.0
#
"
4	"harmonica, mouth organ, harp, mouth harp
harp
harvester, reaper
"
4	"oxygen mask
packet
paddle, boat paddle
"
6	"    jit = torch.jit.script(conv.jittable())
    jit(x, edge_index)
    assert jit(x, edge_index).tolist() == out.tolist()

"
1	"            (""transformer"", ldconv_lconv2d_args),
            (""transformer"", ldconv_dconv2d_args),
            (""rnn"", rnn_args),
        )
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
"
2	"        .select(train_df['*'], *[prefix + col for prefix in ['Before', 'After'] for col in elapsed_cols])
    test_df = test_df \
        .join(elapsed, ['Date', 'Store']) \
        .select(test_df['*'], *[prefix + col for prefix in ['Before', 'After'] for col in elapsed_cols])

"
3	"
        with TimeContext(f'building {colored(self.canonical_name, ""green"")}', self.logger):
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
"
2	"  agents:
    queue: 2x-gpu-g4
- label: ':spark: Spark Keras Rossmann Estimator (test-mixed-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c ""OMP_NUM_THREADS=1 python /horovod/examples/keras_spark_rossmann_estimator.py --num-proc 2 --work-dir /work --data-dir file:///data --epochs 3 --sample-rate 0.01""
  plugins:
"
4	"def cv2_to_base64(image):
    data = cv2.imencode('.jpg', image)[1]
    return base64.b64encode(data.tostring()).decode('utf8')

"
4	"        name='b0',
        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
        use_se=use_se)
"
4	"            width_coefficient=w, depth_coefficient=d, dropout_rate=p)
    else:
"
2	"            self.assertEqual(1, len([line for line in stdout if line.startswith('Launching horovod task function: ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no 127.0.0.1 ')]), stdout)
            if mode == 'remote':
                self.assertEqual(1, len([line for line in stdout if line.startswith('Launching horovod task function: ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no localhost ')]))
            else:
"
2	"from horovod.spark.task.task_service import SparkTaskClient
from horovod.spark.runner import _task_fn
"
4	"Crock Pot
croquet ball
crutch
cuirass
"
2	"  timeout_in_minutes: 5
  retry:
"
4	"
def process_image(img):
    img = resize_short(img, target_size=256)
    img = crop_image(img, target_size=DATA_DIM, center=True)
    if img.mode != 'RGB':
"
4	"    'batch_norm_epsilon',
    'dropout_rate',
    'num_classes',
"
4	"thunder snake, worm snake, Carphophis amoenus
ringneck snake, ring-necked snake, ring snake
"
2	"

def get_simple_meta_from_parquet(store, label_columns, feature_columns, sample_weight_col,
                                 dataset_idx=None):

"
2	"        driver = ElasticDriver(mock.Mock(), mock_discovery, min_np=8, max_np=20)
        driver.wait_for_available_slots(min_np=16)
"
3	"
if False:
    import argparse

"
2	"            {'b': 2}
        ]
        host_manager = HostManager(mock_discovery)

        # Should be empty initially
"
1	"    audio2 = np.random.randint(-100, 100, 16, dtype=np.int16)
    audio1 = audio1.astype(np.float64) / (np.iinfo(np.int16).max + 1)
"
2	"    import tensorflow as tf
    from tensorflow.keras.layers import Input, Embedding, Concatenate, Dense, Flatten, Reshape, BatchNormalization, Dropout
    import tensorflow.keras.backend as K
"
4	"        """"""
        self.parser = argparse.ArgumentParser(
            description=""Run the {} module."".format(self.name),
            prog='hub run {}'.format(self.name),
            usage='%(prog)s',
"
1	"    p = tmp_path / ""wav.scp""
    soundfile.write(
        tmp_path / ""a.wav"",
        np.random.randint(-100, 100, (160000,), dtype=np.int16),
"
2	"                                               key, nics)

"
3	"        elif self.if_expression:
            self._compiled_if = compile(self.if_expression, '<string>', 'eval')
"
4	"            return inputs
        keep_prob = 1.0 - prob
        random_tensor = keep_prob + fluid.layers.uniform_random_batch_size_like(
            inputs, [-1, 1, 1, 1], min=0., max=1.)
"
4	"    res = b6.classify(images=test_image)
    print(res)

"
4	"king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica
American lobster, Northern lobster, Maine lobster, Homarus americanus
spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish
crayfish, crawfish, crawdad, crawdaddy
hermit crab
"
1	"                stats.update(enc_attn_loss=enc_attn_loss.item())
            # calculate for decoder
            if ""decoder"" in self.modules_applied_guided_attn:
                att_ws = []
                for idx, layer_idx in enumerate(
"
2	"        res = tiny_shell_exec.execute(command)

        if res:
"
4	"    @serving
    def serving_method(self, images, **kwargs):
        """"""
"
4	"        if isinstance(s, list) or isinstance(s, tuple):
            s = s[0]
        oup = block_args.input_filters * block_args.expand_ratio  # number of output channels

"
2	"            if slot_info.hostname == 'host-1':
                if slot_info.local_rank == 0:
                    return 1, time.time()

                driver.record_ready(slot_info.hostname, slot_info.local_rank)
"
1	"        self.build_funcs = list(build_funcs)
        self.seed = seed
        self.shuffle = shuffle

    def build_iter(self, epoch: int, shuffle: bool = None) -> Iterator:
"
4	"hummingbird
jacamar
"
4	"    'EfficientNetB2', 'EfficientNetB3', 'EfficientNetB4', 'EfficientNetB5',
    'EfficientNetB6', 'EfficientNetB7'
]
"
4	"        'efficientnet-b4': (1.4, 1.8, 380, 0.4),
        'efficientnet-b5': (1.6, 2.2, 456, 0.4),
        'efficientnet-b6': (1.8, 2.6, 528, 0.5),
        'efficientnet-b7': (2.0, 3.1, 600, 0.5),
    }
"
4	"    if center == True:
        w_start = (width - size) / 2
        h_start = (height - size) / 2
    else:
        w_start = np.random.randint(0, width - size + 1)
"
4	"streetcar, tram, tramcar, trolley, trolley car
stretcher
studio couch, day bed
stupa, tope
"
4	"French bulldog
Great Dane
"
4	"hair spray
half track
hammer
hamper
hand blower, blow dryer, blow drier, hair dryer, hair drier
"
2	"        with temppath() as t:
            with env(HOROVOD_TIMELINE=t, HOROVOD_TIMELINE_MARK_CYCLES='1'):
"
2	"
import horovod.torch as hvd

"
4	"gorilla, Gorilla gorilla
chimpanzee, chimp, Pan troglodytes
gibbon, Hylobates lar
"
2	"              type.
    Returns:
        The object that was broadcast from the `root_rank`.
"
4	"comic book
crossword puzzle, crossword
street sign
"
6	"

def test_my_conv():
    x1 = torch.randn(4, 8)
"
4	"    return conv

"
2	"  - docker-compose#v2.6.0:
      run: test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
"
4	"            input,
            param_attr=param_attr,
"
4	"

def efficientnet_params(model_name):
    """""" Map EfficientNet model name to parameter coefficients. """"""
"
2	"            super(RendezvousHandler, self)._put_value(scope, key, value)

        def _put_worker_addresses(self, host, local_rank, addresses, secret_key):
            driver.register_worker_server(host, local_rank, addresses, secret_key)
"
4	"Australian terrier
Dandie Dinmont, Dandie Dinmont terrier
"
1	"        if [ ${use_wordlm} = true ]; then
            recog_opts=""--word-rnnlm ${lmexpdir}/rnnlm.model.best""
        else
            recog_opts=""--rnnlm ${lmexpdir}/rnnlm.model.best""
        fi
"
2	"    parser.add_argument('--network-interface', action='store', dest='nics',
                        help='Network interfaces that can be used for communication separated by '
                             'comma. If not specified, Horovod will find the common NICs among all '
                             'the workers and use it; example, --network-interface ""eth0,eth1"".')
"
2	"

class TestSleepClient(network.BasicClient):
"
2	"# Results
img_sec_mean = np.mean(state.img_secs)
img_sec_conf = 1.96 * np.std(state.img_secs)
log('Img/sec per %s: %.1f +-%.1f' % (device, img_sec_mean, img_sec_conf))
"
4	"            images (list[numpy.ndarray]): data of images, shape of each is [H, W, C], color space must be BGR.
            paths (list[str]): The paths of images.
            batch_size (int): batch size.
"
6	"            forward_header=forward_header,
            forward_types=forward_types,
            forward_body=forward_body,
"
2	"                with mock.patch('horovod.run.runner.network.filter_local_addresses',
                                side_effect=lambda hosts: [host for host in hosts if host not in local_hosts]), \
"
2	"2. At least one member of the [technical steering committee](https://github.com/horovod/horovod/blob/master/CONTRIBUTING.md) must review and approve.
3. If any member of the technical steering committee requests changes, they must be addressed.

"
4	"
    Args:
        images (list[numpy.ndarray]): images data, shape of each is [H, W, C].
        paths (list[str]): paths to images.

"
1	"                raise RuntimeError(f""Not supported: type={t}"")

        return text

"
4	"                    name=""image"", shape=[3, 224, 224], dtype=""float32"")
                efficientnet_b6 = EfficientNetB6(
                    override_params=override_params)
                output, feature_map = efficientnet_b6.net(
                    input=image,
"
4	"
GlobalParams = collections.namedtuple('GlobalParams', [
    'batch_norm_momentum',
"
4	"dhole, Cuon alpinus
African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus
hyena, hyaena
"
2	"      return Status::OK();
    })
    .Doc(R""doc(
"
4	"handkerchief, hankie, hanky, hankey
hard disc, hard disk, fixed disk
harmonica, mouth organ, harp, mouth harp
harp
"
4	"
        Returns:
            inputs (dict): key is 'image', corresponding vaule is image tensor.
            outputs (dict): key is :
                'classification', corresponding value is the result of classification.
"
4	"chickadee
water ouzel, dipper
kite
bald eagle, American eagle, Haliaeetus leucocephalus
"
4	"

@moduleinfo(
"
4	"            all_data.append(yield_data)

        total_num = len(all_data)
        loop_num = int(np.ceil(total_num / batch_size))
"
4	"slide rule, slipstick
sliding door
slot, one-armed bandit
snorkel
"
4	"           filter_size=7,
           stride=1,
           stddev=0.02,
           padding=0,
"
4	"        width_padding = 0
        padding = [height_padding, width_padding]
    elif padding_type == ""DYNAMIC"":
"
2	"
                start = time.time()
                client.run_command('set -x; sleep {} && touch {}'.format(sleep, file), {})
                client.abort_command()
"
4	"        for block_string in string_list:
            blocks_args.append(BlockDecoder._decode_block_string(block_string))
        return blocks_args

"
1	"    i=0; for pid in ""${pids[@]}""; do wait ${pid} || ((++i)); done
    [ ${i} -gt 0 ] && echo ""$0: ${i} background jobs are failed."" && false
    echo ""Finished""
fi

"
4	"tile roof
toaster
"
2	"            '0.weight': torch.tensor([[v, v], [v, v]]),
            '0.bias': torch.tensor([v, v])
        })
"
2	"from horovod.run.elastic.worker import WorkerNotificationClient

"
4	"solar dish, solar collector, solar furnace
sombrero
"
5	"        if use_fc_solve_lib:
            self._setText(
                iter=fc_solve_lib_obj.get_num_times(),
                depth=0,
                states=fc_solve_lib_obj.get_num_states_in_collection(),
"
4	"           norm=None,
           act=None,
"
2	"        """"""
        super(UpdateEpochStateCallback, self).__init__(tf.keras.backend, state)

"
1	"        if key == ""b"":
            assert data[""data5""].shape == (150, 80,)

"
2	"                        self.assertEqual(expected, env)

    def do_test_spark_task_service_executes_command(self, client, file):
        self.assertFalse(os.path.exists(file))
        client.run_command('touch {}'.format(file), {})
"
2	"        # TODO(travis): also check for hosts removed from the blacklist in the future
        prev_host_slots = self._current_hosts.host_slots
        prev_host_assignment_order = self._current_hosts.host_assignment_order
        host_slots = self._discovery.find_available_hosts_and_slots()
"
3	"    from ..hubapi.docker import HubIO
    getattr(HubIO(args), args.hub)()

"
6	"    assert jit(x1, edge_index, size=(4, 4)).tolist() == out.tolist()

    t = '(Tensor, SparseTensor, OptTensor, Size) -> Tensor'
"
2	"  retry:
    automatic: true
"
4	"shoji
shopping basket
shopping cart
shovel
"
2	"epochs = 24
num_classes = 10

(mnist_images, mnist_labels), _ = \
    tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank())
"
1	"import logging
from pathlib import Path
from typing import Dict
from typing import List
"
4	"miniskirt, mini
minivan
missile
"
1	"    target = SoundScpReader(tmp_path / ""wav.scp"", normalize=True, dtype=np.float64)
    desired = {""abc"": (16, audio1), ""def"": (16, audio2)}
"
4	"
        param_attr, bias_attr = init_fc_layer(class_dim, '_fc')
"
2	"# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
4	"agaric
gyromitra
"
4	"            num_filters=num_filters,
            filter_size=filter_size,
            stride=stride,
            groups=num_groups,
"
1	"import logging
from typing import Callable
from typing import Collection
"
4	"football helmet
forklift
"
2	"
def benchmark_step(state):
    optimizer.zero_grad()
    output = model(data)
"
2	"  command: bash -c "" OMP_NUM_THREADS=1 \$(cat /mpirun_command) python /horovod/examples/mxnet_mnist.py""
  plugins:
  - docker-compose#v2.6.0:
"
4	"tench, Tinca tinca
goldfish, Carassius auratus
great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias
"
4	"                inputs = {
                    key: global_vars[value]
                    for key, value in inputs.items()
                }
                outputs = {
"
4	"                bn_act=None,
                bn_mom=self._bn_mom,
"
2	"  - docker-compose#v2.6.0:
      run: test-cpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
"
4	"        self.use_se = use_se

    def net(self, input, class_dim=1000, is_test=False):
"
2	"    state_dict = {
        'epoch': state.epoch,
        'batch': state.batch,
"
4	"ant, emmet, pismire
grasshopper, hopper
cricket
walking stick, walkingstick, stick insect
"
4	"            initializer=fluid.initializer.Constant(0.0),
            trainable=True)
        scale = helper.create_parameter(
"
4	"        Decodes a list of string notations to specify blocks inside the network.
        :param string_list: a list of strings, each string is a notation of block
"
2	"                                    r'-mca plm_rsh_agent ""[^""]+python[0-9.]* -m horovod.spark.driver.mpirun_rsh [^ ]+ [^ ]+"" '
                                    r'[^""]+python[0-9.]* -m horovod.spark.task.mpirun_exec_fn [^ ]+ [^ ]+'.format(
                    expected_np=expected_np,
"
1	"train_set=train
train_dev=test
"
2	"
    def forward(self, x):
        if ~self.mode:
"
1	"
        retval = self.chilidren[key]
        assert check_return_type(retval)
"
4	"
res (list\[dict\]): 分类结果，列表的每一个元素均为字典，其中 key 为识别动物的类别，value为置信度。

```python
"
1	"from espnet2.fileio.read_text import read_2column_text


"
4	"            label = label_list[index].split(',')[0]
            output_i[label] = float(result_i[index])
        output.append(output_i)
    return output
"
2	"            {'a': 2},
            {'a': 2, 'b': 2},
"
2	"  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
  retry:
"
4	"dugong, Dugong dugon
sea lion
Chihuahua
Japanese spaniel
Maltese dog, Maltese terrier, Maltese
"
4	"quill, quill pen
quilt, comforter, comfort, puff
"
2	"                             'Found duplicates: %s' % ', '.join(dups))

        all_param_ids = {id(v)
                         for param_group in self.param_groups
"
2	"    """"""
    Integration tests for horovod.run.
"
4	"envelope
espresso maker
face powder
feather boa, boa
"
2	"  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
"
2	"# Horovod: scale learning rate by lr_scaler.
optimizer = optim.SGD(model.parameters(), lr=args.lr * lr_scaler,
                      momentum=args.momentum)

"
4	"perfume, essence
Petri dish
photocopier
pick, plectrum, plectron
pickelhaube
"
4	"mosquito net
motor scooter, scooter
mountain bike, all-terrain bike, off-roader
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
"
3	"            if self.key_bytes and self.key_dtype:
                self.int2ext_key = np.frombuffer(self.key_bytes, dtype=self.key_dtype)

            if self.int2ext_key is not None and vecs is not None and vecs.ndim == 2:
                if self.int2ext_key.shape[0] != vecs.shape[0]:
"
4	"mailbox, letter box
maillot
maillot, tank suit
"
2	"    Actually tests that horovod.spark.run times out when it does not start up fast enough.
    """"""
    def do_test_timeout(self, use_mpi, use_gloo):
"
4	"grasshopper, hopper
cricket
walking stick, walkingstick, stick insect
cockroach, roach
"
4	"* inputs (dict): 计算图的输入，key 为 'image', value 为图片的张量；
* outputs (dict): 计算图的输出，key 为 'classification' 和 'feature_map'，其相应的值为：
    * classification (paddle.fluid.framework.Variable): 分类结果，也就是全连接层的输出；
"
2	"            # After sync, all values should match the root rank
            for w in self.evaluate(vars1):
                self.assertAllClose(w, np.ones_like(w))
            assert state.batch == 20
            assert state.epoch == 10
"
4	"mortar
mortarboard
mosque
mosquito net
"
2	"
        def exception(*args, **argv):
            raise Exception('Test Exception')

        with mock.patch(""horovod.run.mpi_run._get_mpi_implementation_flags"", side_effect=mpi_impl_flags):
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
"
6	"

def parse_types(func: Callable) -> List[Tuple[Dict[str, str], str]]:
"
4	"        h_start = np.random.randint(0, height - size + 1)
    w_end = w_start + size
"
4	"    elif padding_type == ""VALID"":
        height_padding = 0
        width_padding = 0
        padding = [height_padding, width_padding]
    elif padding_type == ""DYNAMIC"":
"
2	"  command: bash -c "" \$(cat /mpirun_command) python /horovod/examples/tensorflow_mnist.py""
  plugins:
  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
"
3	"            self.assertEqual(req.status.details[0].pod, 'r2')
            self.assertTrue(req.status.details[0].exception.startswith('ZeroDivisionError'))
"
2	"args.cuda = not args.no_cuda and torch.cuda.is_available()

"
2	"
        with spark_session('test_happy_run_elastic_fault_tolerant', max_failures=3):
"
1	"def test_text2tokens(word_tokenizer: WordTokenizer):
    assert word_tokenizer.text2tokens(""Hello World!! Ummm"") == [
"
4	"BlockArgs.__new__.__defaults__ = (None, ) * len(BlockArgs._fields)


def efficientnet_params(model_name):
"
4	"Irish water spaniel
kuvasz
schipperke
"
2	"        buf = io.BytesIO(t.numpy().tobytes())
        obj = cloudpickle.load(buf)

    return obj

"
4	"# See the License for the specific language governing permissions and
# limitations under the License.

"
4	"bison
ram, tup
"
4	"    'EfficientNet', 'EfficientNetB0_small', 'EfficientNetB0', 'EfficientNetB1',
    'EfficientNetB2', 'EfficientNetB3', 'EfficientNetB4', 'EfficientNetB5',
    'EfficientNetB6', 'EfficientNetB7'
]
"
4	"                             dirname,
                             model_filename=None,
"
2	"    from pyspark import SparkConf
    from pyspark.sql import SparkSession

    unknown_keys = set([prop for prop, _ in extra_conf]) \
        .difference(conf.SPARK_CONF_DEFAULT_VALUES.keys()) \
"
4	"golfcart, golf cart
gondola
gong, tam-tam
gown
"
1	"                self.non_linguistic_symbols = set(line.rstrip() for line in f)
        else:
            self.non_linguistic_symbols = set(non_linguistic_symbols)
"
2	"                    break
                logging.debug('%s: %s', log_name, line.strip())

    def _monitor_logfile(self):
"
6	"
data.num_nodes = node_idx.size(0)
data.edge_index = edge_index
"
4	"tench, Tinca tinca
goldfish, Carassius auratus
great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias
"
4	"corkscrew, bottle screw
cornet, horn, trumpet, trump
"
4	"                    class_dim=len(self.label_list),
                    is_test=is_test)

                name_prefix = '@HUB_{}@'.format(self.name)
"
4	"                name=name + ""_offset"",
                initializer=fluid.initializer.Constant(0.0))
        else:
            bias_attr = False
"
2	"        return res, hvd.rank()

    logging.getLogger().setLevel(logging.DEBUG)
    logging.basicConfig(format='%(asctime)-15s %(levelname)1.1s %(filename)s:%(lineno)d %(funcName)s() - %(message)s')

"
4	"
        fluid.io.save_inference_model(
            dirname=dirname,
"
2	"
    :param salt: extra information to include in the hash, ignores Falsy values
"
2	"#if HOROVOD_GPU_BROADCAST
REGISTER_KERNEL_BUILDER(
    Name(""HorovodSize"").Device(DEVICE_GPU).HostMemory(""size""),
    HorovodReturnScalarOp<int, common::horovod_size>);
#endif
"
4	"digital clock
digital watch
"
6	"
    mae = torch.cat(maes, dim=0)

    # Report meV instead of eV.
    mae = 1000 * mae if target in [2, 3, 4, 6, 7, 8, 9, 10] else mae
"
2	"from horovod.run.elastic.discovery import FixedHosts, HostManager
from horovod.run.elastic.driver import ElasticDriver
from horovod.run.elastic.rendezvous import create_rendezvous_handler
"
2	"from horovod.torch.mpi_ops import synchronize
from horovod.torch.mpi_ops import rank
"
5	"                 if bh_solve_lib_obj.ret_code_is_suspend(ret_code)
                 else 'unsolved'))
            self._setText(iter=bh_solve_lib_obj.get_num_times())
"
4	"        h_start = (height - size) / 2
    else:
        w_start = np.random.randint(0, width - size + 1)
"
4	"        )

    param_attr, bias_attr = initial_type(
        name=name,
"
3	"        :param kwargs:
        """"""
        super().__init__(*args, **kwargs)
        self.lang_code = lang_code
"
2	"    train_df = train_df.select(*(all_cols + ['Sales', 'Date'])).cache()
    test_df = test_df.select(*(all_cols + ['Id', 'Date'])).cache()

    # Build vocabulary of categorical columns.
"
2	"
                # Remove host hash earlier registered under this index.
                if req.index in self._task_index_host_hash:
                    earlier_host_hash = self._task_index_host_hash[req.index]
"
2	"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"
4	"    res = b7.classify(images=test_image)
    print(res)

"
2	"  agents:
    queue: 2x-gpu-g4
- label: ':spark: Spark Keras Rossmann Estimator (test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0)'
"
6	"    if len(matches) > 0:
        out = []
        args = list(signature.parameters.keys())
"
4	"        label_file = os.path.join(self.directory, ""label_list.txt"")
        with open(label_file, 'r', encoding='utf-8') as file:
            self.label_list = file.read().split(""\n"")[:-1]
        self.classification = self.classify
        self._set_config()
"
4	"    'batch_norm_momentum',
    'batch_norm_epsilon',
"
6	"        if char == '[':
            depth += 1
"
4	"from __future__ import print_function

import math
"
5	"        for label, min, max, avr, tot, top in ll:
            ttk.Label(frame, text=label
"
2	"      push-retries: 5
  - ecr#v1.2.0:
      login: true
"
2	"        """"""Tests two hosts, two slots each with second host failing before rendezvous completes.""""""
        slots = {'host-1': 2, 'host-2': 2}
        discovery = FixedHosts(slots)

        driver = ElasticDriver(mock.Mock(), discovery, min_np=2, max_np=4)
"
4	"import paddlehub as hub
import cv2
"
4	"        use_cudnn=use_cudnn,
        param_attr=param_attr,
        bias_attr=bias_attr)

    if need_crop:
"
0	"
        self.assertEqual(btools.utils.local_xyz(dummy), (Vector(),)*3)

        dummy.normal = Vector((1, 0, 0))
"
4	"junco, snowbird
indigo bunting, indigo finch, indigo bird, Passerina cyanea
robin, American robin, Turdus migratorius
bulbul
"
4	"                raise RuntimeError(
                    ""Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES as cuda_device_id.""
                )

"
1	"            # Is a text
            sub[""cc""]
"
2	"
    def expand_date(df):
        df = df.withColumn('Date', df.Date.cast(T.DateType()))
        return df \
            .withColumn('Year', F.year(df.Date)) \
"
4	"canoe
can opener, tin opener
cardigan
car mirror
"
1	"        logging.info(f""normalized log probability: {best.score / len(best.yseq):.2f}"")
        logging.info(f""total number of ended hypotheses: {len(nbest_hyps)}"")
"
4	"goblet
go-kart
golf ball
golfcart, golf cart
"
4	"                conv = self._drop_connect(conv, drop_connect_rate, is_test)
            conv = fluid.layers.elementwise_add(conv, inputs)

        return conv

"
4	"            name=name + ""_weights"",
            initializer=fluid.initializer.NormalInitializer(
                loc=0.0, scale=stddev))
        if use_bias == True:
            bias_attr = fluid.ParamAttr(
"
4	"pier
piggy bank, penny bank
"
4	"hammer
hamper
hand blower, blow dryer, blow drier, hair dryer, hair drier
hand-held computer, hand-held microcomputer
"
2	"        return F.batch_norm(
            input, self.running_mean, self.running_var, self.weight, self.bias,
            self.training or not self.track_running_stats, self.momentum, self.eps)
"
3	"      number=5, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
"
4	"                block_size += 1

"
4	"
* images (list\[numpy.ndarray\]): 图片数据，每一个图片数据的shape 均为 \[H, W, C\]，颜色空间为 BGR；
"
4	"                 name='b0',
                 padding_type='SAME',
                 override_params=None,
"
4	"

def softmax(x):
    orig_shape = x.shape
"
4	"# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
4	"                for param in context_prog.global_block().iter_parameters():
                    param.trainable = trainable
        return inputs, outputs, context_prog

"
2	"      login: true
  timeout_in_minutes: 30
"
2	"        thread = in_thread(fn, args=(1, 2))
        thread.join(1.0)
        self.assertFalse(thread.is_alive())
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
"
4	"    @serving
    def serving_method(self, images, **kwargs):
        """"""
"
0	"import bpy
import bmesh
import btools
import unittest
"
4	"            'e%s' % block.expand_ratio,
            'i%d' % block.input_filters,
            'o%d' % block.output_filters
        ]
"
4	"
        return conv

"
4	"letter opener, paper knife, paperknife
library
"
1	"        writer[""def""] = 16, audio2
        # Unsupported dimension
        with pytest.raises(RuntimeError):
            y = np.random.randint(-100, 100, [16, 1, 1], dtype=np.int16)
            writer[""ghi""] = 16, y
"
4	"            text_1 = fluid.layers.data(
                name=""text_1"",
                shape=[-1, max_seq_len, 1],
                dtype=""int64"",
"
4	"        Args:
            images (list[numpy.ndarray]): data of images, shape of each is [H, W, C], color space must be BGR.
            paths (list[str]): The paths of images.
            batch_size (int): batch size.
            use_gpu (bool): Whether to use gpu.
"
4	"
    if need_crop:
        conv = conv[:, :, 1:, 1:]
"
4	"birdhouse
boathouse
bobsled, bobsleigh, bob
"
4	"import cv2

classifier = hub.Module(name=""efficientnetb3_small_imagenet"")

result = classifier.classify(images=[cv2.imread('/PATH/TO/IMAGE')])
"
2	"  NCCLOpContext nccl_op_context_;
  HorovodGlobalState* global_state_;
};


"
4	"church, church building
cinema, movie theater, movie theatre, movie house, picture palace
cleaver, meat cleaver, chopper
"
2	"  plugins:
  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
"
4	"        cpu_config.disable_glog_info()
        cpu_config.disable_gpu()
        self.cpu_predictor = create_paddle_predictor(cpu_config)
"
2	"                         settings):
    """"""
"
4	"police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria
poncho
pool table, billiard table, snooker table
pop bottle, soda bottle
"
2	"                            [17] * dim, -100, 100, dtype=dtype)
                    summed = hvd.allreduce(tensor, average=False)
                    multiplied = tensor * (size-1)
"
4	"Brittany spaniel
clumber, clumber spaniel
English springer, English springer spaniel
"
2	"        event.set()
        thread.join(1.0)
        self.assertFalse(thread.is_alive())
        fn.assert_called_once()
"
4	"    main()

"
2	"        return LSFUtils.get_allocation_info()[""compute_node_cores""]

"
4	"plastic bag
plate rack
plow, plough
"
6	"from torch.nn import Parameter
from torch_scatter import scatter
from torch_sparse import SparseTensor, matmul, masked_select_nnz
"
2	"      run: test-cpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
"
3	"def download_data(targets, download_proxy=None):
    opener = urllib.request.build_opener()
    if download_proxy:
        proxy = urllib.request.ProxyHandler({'http': download_proxy, 'https': download_proxy})
"
4	"suit, suit of clothes
sundial
"
2	"            initial_lr: Initial learning rate at the start of training.

                .. warning:: Will be required in v0.21.0.
"
4	"
将模型保存到指定路径。

**参数**
"
2	"import io
import os
"
0	"
import tools
"
6	"        deg_in = 1. / self.adj_t.storage.rowcount()
        deg_out = 1. / self.adj_t.storage.colcount()
        prob = (1. / deg_in[row]) + (1. / deg_out[col])

"
4	"axolotl, mud puppy, Ambystoma mexicanum
bullfrog, Rana catesbeiana
tree frog, tree-frog
"
4	"baseball
basketball
bassinet
"
1	"        train: bool,
        num_iters_per_epoch: Optional[int],
"
6	"    assert jit1(x, adj.t(), adj.t()).tolist() == out1.tolist()
    assert jit2(out1, adj.t(), adj.t()).tolist() == out2.tolist()

"
4	"lionfish
puffer, pufferfish, blowfish, globefish
"
2	"
        rank_results = {}

"
4	"        out = fluid.layers.fc(
            pool,
            class_dim,
            name='_fc',
            param_attr=param_attr,
"
4	"        override_params=override_params,
        use_se=use_se)
    return model
"
4	"                    size=[dict_dim, 128],
                    padding_idx=dict_dim - 1,
                    dtype='float32',
                    param_attr=w_param_attrs)
"
4	"    def _depthwise_conv_norm(self, inputs, block_args, is_test, name=None):
        k = block_args.kernel_size
"
4	"vault
velvet
vending machine
vestment
"
1	"        # linear -> GLU -- -> lightconv -> linear
        #               \        /
"
4	"leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea
mud turtle
terrapin
box turtle, box tortoise
"
4	"
    @staticmethod
"
2	"        df = df.join(weather, ['State', 'Date'])

        # Fix null values.
        df = df \
            .withColumn('CompetitionOpenSinceYear', F.coalesce(df.CompetitionOpenSinceYear, F.lit(1900))) \
"
4	"            name=name + ""_weights"",
            initializer=fluid.initializer.NormalInitializer(
"
1	"-0.18418588	<s> you love
-0.3433284	I love coffee
-0.15267509	you love coffee
"
2	"    :param fn_cache: Cache storing the results of checks performed by horovod
    :type fn_cache: Horovod.run.util.cache.Cache
    :return: List of common interfaces
"
4	"white wolf, Arctic wolf, Canis lupus tundrarum
red wolf, maned wolf, Canis rufus, Canis niger
"
2	"    init()


def _broadcast_model(model, optimizer, backend):
    if _executing_eagerly():
"
4	"stove
strainer
streetcar, tram, tramcar, trolley, trolley car
stretcher
studio couch, day bed
"
4	"
    def get_padding(filter_size, stride=1, dilation=1):
"
4	"        x -= tmp
        x = np.exp(x)
        tmp = np.sum(x)
"
1	"            Dict: Statistics to be monitored.
            Tensor: Weight value.

"
1	"        help=""Specify g2p method if --token_type=phn"",
    )

"
2	"            import tensorflow.keras.backend as K

            # Do not use GPUs for prediction, use single CPU core per task.
            config = tf.ConfigProto(device_count={'GPU': 0})
"
4	"                name=name + ""_offset"",
                initializer=fluid.initializer.Constant(0.0))
        else:
            bias_attr = False

"
4	"                 dropout_rate=0.2,
                 drop_connect_rate=0.2):
    """""" Get block arguments according to parameter and coefficients. """"""
    blocks_args = [
"
4	"
    need_crop = False
    if padding_type == ""SAME"":
        top_padding, bottom_padding = cal_padding(input.shape[2], stride,
                                                  filter_size)
"
1	"-0.5104463	I love	-0.30103
-0.5104463	you love	-0.30103
-0.39019543	love coffee	-0.30103
"
4	"macaque
langur
"
4	"        x -= tmp
        x = np.exp(x)
        tmp = np.sum(x)
"
2	"  // Set flag to trigger update_cache_bits to remove empty
  // positions in cache_iters_ vector
"
2	"  retry:
    automatic: true
  agents:
    queue: cpu
- label: ':pytest: Run PyTests (test-cpu-gloo-py3_7-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark2_4_0)'
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
"
2	"# Copyright 2019 Uber Technologies, Inc. All Rights Reserved.
# Modifications copyright Microsoft
#
"
4	"                exe, self.pretrained_model_path, predicate=if_exist)

"
6	"                # Some ops add self-loops to `edge_index`. We need to do the
                # same for `edge_mask` (but do not train those).
"
4	"ruffed grouse, partridge, Bonasa umbellus
prairie chicken, prairie grouse, prairie fowl
"
4	"BlockArgs.__new__.__defaults__ = (None, ) * len(BlockArgs._fields)


def efficientnet_params(model_name):
    """""" Map EfficientNet model name to parameter coefficients. """"""
"
2	"
            state.batch += 1
            if state.batch % args.batches_per_commit == 0:
                state.commits += 1
"
2	"#
#     http://www.apache.org/licenses/LICENSE-2.0
"
4	"bakery, bakeshop, bakehouse
balance beam, beam
balloon
ballpoint, ballpoint pen, ballpen, Biro
"
2	"      config: docker-compose.test.yml
      push-retries: 5
"
4	"Italian greyhound
whippet
Ibizan hound, Ibizan Podenco
Norwegian elkhound, elkhound
"
4	"drilling platform, offshore rig
drum, membranophone, tympan
drumstick
"
4	"milk can
minibus
miniskirt, mini
"
6	"
def test_pna_conv():
    x = torch.randn(4, 16)
    edge_index = torch.tensor([[0, 0, 0, 1, 2, 3], [1, 2, 3, 0, 0, 0]])
    row, col = edge_index
"
2	"import torch

"
2	"    :param all_host_names: list of the host names
    :type all_host_names: list(string)
    :param remote_host_names: list of the remote host names.
    :type remote_host_names: list(string)
"
0	"    def setUpClass(cls):
        bpy.utils.register_class(btools.core.floorplan.FloorplanProperty)
        bpy.types.Scene.test_prop = bpy.props.PointerProperty(type=btools.core.floorplan.FloorplanProperty)

"
6	"        eps (float, optional): (Initial) :math:`\epsilon`-value.
            (default: :obj:`0.`)
"
4	"passenger car, coach, carriage
patio, terrace
"
4	"            padding_type=self.padding_type,
            bn_mom=self._bn_mom,
            bn_eps=self._bn_eps,
            name=name,
"
2	"        assert state.batch == 20
        assert state.epoch == 10

        # Partially modify then commit
        model1.load_state_dict(model2.state_dict())
"
2	"    def _handle_worker_exit(self, slot_info, exit_code, timestamp):
        if not self.has_rank_assignment(slot_info.hostname, slot_info.local_rank):
"
4	"class EfficientNetB4TestCase(TestCase):
    def setUp(self):
"
4	"thatch, thatched roof
theater curtain, theatre curtain
"
4	"missile
mitten
mixing bowl
mobile home, manufactured home
Model T
"
4	"Angora, Angora rabbit
hamster
porcupine, hedgehog
fox squirrel, eastern fox squirrel, Sciurus niger
"
2	"- label: ':docker: Build test-cpu-gloo-py3_8-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark3_0_0'
  plugins:
  - docker-compose#6b0df8a98ff97f42f4944dbb745b5b8cbf04b78c:
      build: test-cpu-gloo-py3_8-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark3_0_0
"
1	"                ]

            # check whether to finish generation
"
1	"            --recog-json ${feat_recog_dir}/split${nj}utt/data.JOB.json \
            --result-label ${expdir}/${decode_dir}/data.JOB.json \
            --model ${expdir}/results/${recog_model}  \
            ${recog_opts}
"
2	"                                                start_timeout=5, verbose=2)
                self.assertListEqual([([0, 4, 0, 4, 1, 4, 0, 4], 0),
                                      ([0, 4, 0, 4, 1, 4, 0, 4], 1)], res)


"
4	"                   override_params=None,
                   use_se=True):
    model = EfficientNet(
        name='b6',
        is_test=is_test,
"
1	"        :param int y: blank symbol index
        :return: best alignment results
"
0	"        unit=""LENGTH"",
        description=""Width of extruded border"",
"
2	"        print(jsrun_command)

    # Execute the jsrun command.
"
2	"    __test__ = False

    def __init__(self, training_script, *args, **kwargs):
"
2	"    def init(self, rendezvous_addr=None, rendezvous_port=None,
             nic=None, hostname=None, local_rank=None):
        with self._lock:
            if self._service:
                return
"
2	"epoch_to_hosts = {epoch: hosts for epoch, hosts in discovery_schedule if epoch is not None}
default_hosts = discovery_schedule[-1][1] if discovery_schedule else []

exit_schedule = json.loads(args.exit_schedule) if args.exit_schedule else {}
"
2	"            assert (hvd.allreduce(ts1.grad, name='ts1.grad') - ts2.grad).abs().sum() < 1e-6

"
2	"            self._driver.stop()
            return

        # Check that all processes failed, indicating that processing should stop
        if self.count(FAILURE) == self._size:
"
4	"
            idx += 1
            if block_args.num_repeat > 1:
"
2	"    auto ready_event = std::shared_ptr<common::ReadyEvent>(RecordReadyEvent(context));
    auto hvd_context = std::make_shared<TFOpContext>(context);
    auto enqueue_result = EnqueueJoin(
"
4	"    'width_coefficient',
    'depth_coefficient',
    'depth_divisor',
"
4	"lynx, catamount
leopard, Panthera pardus
snow leopard, ounce, Panthera uncia
jaguar, panther, Panthera onca, Felis onca
lion, king of beasts, Panthera leo
"
2	"      const std::vector<TensorTableEntry>& entries, Timeline& timeline,
      const std::function<void()>& error_check_callback) {
"
4	"groom, bridegroom
scuba diver
rapeseed
daisy
yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum
"
4	"    img = img.crop((w_start, h_start, w_end, h_end))
    return img


def process_image(img):
"
1	"import numpy as np

"
1	"        eprenet_conv_chans: int = 256,
        eprenet_conv_filts: int = 5,
        dprenet_layers: int = 2,
        dprenet_units: int = 256,
"
1	"

class TextCleaner:
    """"""Text cleaner.

"
4	"    data = np.fromstring(data, np.uint8)
    data = cv2.imdecode(data, cv2.IMREAD_COLOR)
    return data


"
4	"                }
                outputs = {
"
4	"conch
snail
slug
sea slug, nudibranch
"
4	"        batch_norm_epsilon=1e-3,
        dropout_rate=dropout_rate,
        drop_connect_rate=drop_connect_rate,
"
4	"        self.classification = self.classify
        self._set_config()

"
2	"        self.assertEqual(1, gloo_exec_command_fn.call_count)
        _, _, _, call_env = gloo_exec_command_fn.call_args[0]
        self.assertEqual(env or {}, call_env)
        self.assertEqual({}, gloo_exec_command_fn.call_args[1])
"
1	"        lmdict=${dict}
        mkdir -p ${lmdatadir}
"
4	"        'r2_k5_s22_e6_i24_o40_se0.25',
        'r3_k3_s22_e6_i40_o80_se0.25',
"
0	"
    def test_local_to_global(self):
        X = Vector((1, 0, 0))
        Y = Vector((0, 1, 0))
"
2	"
    :param event: event that triggers func
"
1	"#!/bin/bash

"
1	"        '123  '
        >>> get_human_readable_count(1234)  # (one thousand)
"
4	"hog, pig, grunter, squealer, Sus scrofa
wild boar, boar, Sus scrofa
warthog
"
2	"
        # the settings should not contain the key
"
6	"def point_pair_features(pos_i: Tensor, pos_j: Tensor, norm_i: Tensor,
                        norm_j: Tensor) -> Tensor:
"
2	"    def size(self):
        return self._size

    def last_rendezvous(self):
        return self._rendezvous_id
"
4	"
```python
def get_pretrained_images_std()
```

"
1	"        #### use CPU for decoding
        ngpu=0

"
4	"

"
1	"                batch_type=args.batch_type,
                train=False,
                multiple_iterator=False,
"
4	"

def initial_type(name,
"
4	"book jacket, dust cover, dust jacket, dust wrapper
menu
plate
guacamole
"
4	"    def setUp(self):
        self.module = hub.Module(name='efficientnetb2_imagenet')
        self.test_images = [
            ""../image_dataset/classification/animals/dog.jpeg"",
            ""../image_dataset/keypoint_detection/girl2.jpg""
"
4	"        out_size = max(filter_size - stride, 0)
    else:
        out_size = max(filter_size - (img_size % stride), 0)
    return out_size // 2, out_size - out_size // 2

"
2	"            for w1, w2 in zip(model1.get_weights(), model1_weights):
                self.assertAllClose(w1, w2)
            assert state.batch == 20
            assert state.epoch == 10
"
4	"    def _initialize(self):
        """"""
        initialize with the necessary elements
"
6	"
    url = 'https://pytorch-geometric.com/datasets/benchmarking-gnns'
"
2	"
        self._check_input_dim(input)

"
3	"lgpl-3.0: GNU Lesser General Public License v3.0
isc: ISC
lppl-1.3c: LaTeX Project Public License v1.3c
ms-pl: Microsoft Public License
"
2	"      login: true
  timeout_in_minutes: 5
"
2	"class GetTaskToTaskAddressesRequest(object):
    def __init__(self, task_index, all_task_addresses):
"
2	"            ElasticDriver._discover_hosts = wrapped_discover_hosts
            driver = ElasticDriver(mock.Mock(), discovery, min_np=2, max_np=4)
            with pytest.raises(RuntimeError):
                driver.wait_for_available_slots(min_np=2)
"
2	"        self._assign_fn = self._assign_var if _IS_TF2 else self._load_var
        self._save_model()

        bcast_obj = broadcast_object_fn(session=session) if not _executing_eagerly() else broadcast_object
"
2	"
    Returns:
        List of results returned by running `fn` on each rank.
    """"""

"
4	"ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus
sloth bear, Melursus ursinus, Ursus ursinus
mongoose
meerkat, mierkat
"
4	"            if drop_connect_rate:
                drop_connect_rate *= float(idx) / block_size
            conv = self.mb_conv_block(conv, block_args, is_test,
                                      drop_connect_rate,
                                      '_blocks.' + str(idx) + '.')
"
1	") -> Dict[str, List[Union[float, int]]]:
    """"""Read a text file indicating sequences of number

"
6	"
    t = '(OptPairTensor, SparseTensor, OptTensor, Size) -> Tensor'
"
3	"        :param metric: The distance metric to use. `braycurtis`, `canberra`, `chebyshev`, `cityblock`, `correlation`,
                        `cosine`, `dice`, `euclidean`, `hamming`, `jaccard`, `jensenshannon`, `kulsinski`,
                        `mahalanobis`,
                        `matching`, `minkowski`, `rogerstanimoto`, `russellrao`, `seuclidean`, `sokalmichener`,
"
3	"        """"""Get all docs in the request, filtered by ``if`` when the expression contains ``doc``

        Always use ``self.docs`` instead of ``self.req.docs`` to access filtered docs
        """"""
"
4	"basketball
bassinet
bassoon
"
2	"      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
1	"        ""a speech recognition model on one CPU or GPU"",
        config_file_parser_class=configargparse.YAMLConfigFileParser,
        formatter_class=configargparse.ArgumentDefaultsHelpFormatter,
"
1	"        init_type: str = ""xavier_uniform"",
        init_enc_alpha: float = 1.0,
        init_dec_alpha: float = 1.0,
        use_masking: bool = False,
"
4	"water buffalo, water ox, Asiatic buffalo, Bubalus bubalis
bison
ram, tup
bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis
"
2	"        addr = network.get_driver_ip(common_intfs)
        port = rendezvous.start(handler)
        nic = list(common_intfs)[0]
"
1	"        assert n_feat % wshare == 0
        self.wshare = wshare
        self.use_kernel_mask = use_kernel_mask
        self.dropout_rate = dropout_rate
        self.kernel_size = int(kernel_size_str.split(""_"")[lnum])
"
4	"            name='_fc',
            param_attr=param_attr,
            bias_attr=bias_attr)
        return out, pool

"
4	"volleyball
waffle iron
"
2	"
        try:
            self._reset_count += 1
"
4	"        tmp = np.sum(x, axis=1)
        x /= tmp.reshape((x.shape[0], 1))
    else:
        tmp = np.max(x)
"
4	"from paddle.fluid.core import PaddleTensor, AnalysisConfig, create_paddle_predictor
from paddlehub.module.module import moduleinfo, runnable, serving
"
4	"    @serving
    def serving_method(self, images, **kwargs):
        """"""
        Run as a service.
"
1	"            else:
                if args.dtype != ""float32"":
                    raise NotImplementedError(
                        f""`--dtype {args.dtype}` is only available with `--api v2`""
"
2	"  timeout_in_minutes: 15
  retry:
    automatic: true
  agents:
"
1	"from typing import Collection

from jaconv import jaconv
import tacotron_cleaner.cleaners
"
1	"        modules_applied_guided_attn=modules_applied_guided_attn,
    )
"
4	"dishwasher, dish washer, dishwashing machine
disk brake, disc brake
"
2	"import os
import unittest
import warnings

from elastic_common import BaseElasticTests
"
4	"
def cal_padding(img_size, stride, filter_size, dilation=1):
    """"""Calculate padding size.""""""
    if img_size % stride == 0:
        out_size = max(filter_size - stride, 0)
"
2	"        if exit_code is None or exit_code != 0:
            raise RuntimeError('executed command returned non-zero exit code: {}'.format(exit_code))

    def _run(self, discovery_schedule=None, exit_schedule=None, hosts=None,
"
2	"# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
"
1	"def test_SoundScpWriter(tmp_path: Path):
    audio1 = np.random.randint(-100, 100, 16, dtype=np.int16)
    audio2 = np.random.randint(-100, 100, 16, dtype=np.int16)
"
2	"    automatic: true
  agents:
    queue: 4x-gpu-g4
"
1	"            idim=idim,
            attention_dim=adim,
            attention_heads=aheads,
"
2	"                        env = sorted([line.strip() for line in f.readlines()])
                        expected = [
                            'HADOOP_TOKEN_FILE_LOCATION=path',
                            'HOROVOD_SPARK_WORK_DIR={cwd}'.format(cwd=os.getcwd()),
                            'PYTHONPATH=pypath',
"
4	"volcano
ballplayer, baseball player
groom, bridegroom
scuba diver
"
2	"- label: ':fire: Single PyTorch MNIST (test-cpu-mpich-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c "" python /horovod/examples/pytorch_mnist.py --epochs 3""
"
2	"            if slot_info.rank == 0:
                return 0, time.time()
"
4	"fire engine, fire truck
fire screen, fireguard
flagpole, flagstaff
flute, transverse flute
folding chair
"
4	"
def reader(images=None, paths=None):
    """"""
    Preprocess to yield image.

"
2	"        thread.start()  # wait for command exit code
        client.run_command('sleep 2', {})  # execute command
        time.sleep(0.5)  # give the thread some time to connect before shutdown
        service.shutdown()  # shutdown should wait on request to finish
        duration = time.time() - start
"
2	"                            if work_dir_env_set:
                                self.assertEqual(test_dir, os.getcwd(), msg)
                            else:
                                self.assertEqual(tmp_path, os.getcwd(), msg)
"
4	"        if use_gpu:
            try:
                _places = os.environ[""CUDA_VISIBLE_DEVICES""]
                int(_places[0])
"
1	"

if __name__ == ""__main__"":
    main(sys.argv[1:])
"
4	"Mexican hairless
timber wolf, grey wolf, gray wolf, Canis lupus
white wolf, Arctic wolf, Canis lupus tundrarum
red wolf, maned wolf, Canis rufus, Canis niger
"
4	"tiger shark, Galeocerdo cuvieri
hammerhead, hammerhead shark
electric ray, crampfish, numbfish, torpedo
"
2	"  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-mpich-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
"
6	"    def __init__(self, in_channels: int, out_channels: int, K: int = 1,
                 cached: bool = False, add_self_loops: bool = True,
                 bias: bool = True, **kwargs):
"
6	"import re
import inspect
import pyparsing as pp
from itertools import product
"
4	"# limitations under the License.

from __future__ import absolute_import
"
2	"# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"
2	"        ])
        # config properties once set will survive session.stop() and
        # SparkSession.config(conf=config).getOrCreate(), so we have to make sure
        # we overwrite their value if not in extra_conf
        more_conf = conf.SPARK_CONF_DEFAULT_VALUES.copy()
"
0	"        btools.utils.create_cube_without_faces(
            self.bm, Vector((1, 1, 1)))
        self.assertEquals(len(self.bm.faces), 6)
        self.assertEquals(len(self.bm.verts), 8)

"
4	"        'r2_k5_s22_e6_i24_o40_se0.25',
        'r3_k3_s22_e6_i40_o80_se0.25',
"
4	"

def norm_layer(input, norm_type='batch_norm', name=None):
    if norm_type == 'batch_norm':
        param_attr = fluid.ParamAttr(
"
1	"        if self.loss_type == ""L1+L2"":
            loss = l1_loss + mse_loss + bce_loss
"
4	"Dandie Dinmont, Dandie Dinmont terrier
Boston bull, Boston terrier
miniature schnauzer
giant schnauzer
standard schnauzer
"
4	"coffeepot
coil, spiral, volute, whorl, helix
"
2	"    queue: cpu
- label: ':tensorflow: Test Keras MNIST (test-cpu-openmpi-py3_6-tf1_14_0-keras2_2_4-torch1_2_0-mxnet1_4_1-pyspark2_4_0)'
  command: bash -c "" \$(cat /mpirun_command) python /horovod/examples/keras_mnist_advanced.py""
"
2	"
        assert len(res) == 4
        for name, (exit_code, timestamp) in res.items():
            assert exit_code == 1, name

"
2	"
  Status Execute(std::vector<TensorTableEntry>& entries,
"
2	"        raise Exception(
            'horovod does not find the jsrun command.\n\n'
"
4	"            is_test = True
        elif phase in [""train""]:
            is_test = False
        else:
            raise ValueError(
"
2	"  timeout_in_minutes: 5
  retry:
"
2	"# Copyright 2019 Uber Technologies, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
"
2	"
        super(self.__class__, self).step()

        # compute delta = curr - start
"
4	"space heater
space shuttle
spatula
speedboat
"
4	"wooden spoon
wool, woolen, woollen
"
2	"                    self.assertEqual({0: 2, 2: 1}, driver.get_ranks_to_indices())

                with override_env({rank_env: 1,
"
2	"    history, best_model_bytes = \
        horovod.spark.run(train_fn, args=(model_bytes,), num_proc=args.num_proc, verbose=2)[0]
"
1	"
        path_name_type_list = copy.deepcopy(path_name_type_list)
        self.preprocess = preprocess

"
2	"  - docker-compose#6b0df8a98ff97f42f4944dbb745b5b8cbf04b78c:
      build: test-gpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0
      image-repository: 823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite
      cache-from: test-gpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0:823773083436.dkr.ecr.us-east-1.amazonaws.com/buildkite:SLUG-test-gpu-openmpi-py3_6-tfhead-kerashead-torchhead-mxnethead-pyspark2_4_0-latest
      config: docker-compose.test.yml
"
4	"toilet tissue, toilet paper, bathroom tissue

"
4	"```python
def save_inference_model(dirname,
"
2	"          oneccl_env=""${oneccl_env}:echo:'/mpirun_command_ofi':>:/mpirun_command:&&""
       else
          oneccl_env=""${oneccl_env}:echo:'/mpirun_command_mpi':>:/mpirun_command:&&""
"
4	"                         padding_type='SAME',
                         override_params=None,
                         use_se=False):
"
4	"partridge
African grey, African gray, Psittacus erithacus
"
2	"def get_torch_rocm_macros():
    try:
        from torch.utils.cpp_extension import COMMON_HIPCC_FLAGS
        pattern = re.compile(r'-D(\w+)=?(\w+)?')
"
2	"#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
4	"                print(
                    var.name,
                    os.path.exists(
                        os.path.join(self.pretrained_model_path, var.name)))
"
1	"from pathlib import Path
import string

"
4	"        ]
        self.true_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3).tolist()
        self.true_std = np.array([0.229, 0.224, 0.225]).reshape(1, 3).tolist()
"
4	"# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
"
4	"velvet
vending machine
vestment
"
4	"sorrel
zebra
hog, pig, grunter, squealer, Sus scrofa
"
4	"cardigan
car mirror
carousel, carrousel, merry-go-round, roundabout, whirligig
carpenter's kit, tool kit
"
4	"        epsilon = 1e-5
        mean = fluid.layers.reduce_mean(input, dim=[2, 3], keep_dim=True)
        var = fluid.layers.reduce_mean(
            fluid.layers.square(input - mean), dim=[2, 3], keep_dim=True)
        if name is not None:
"
4	"moving van
muzzle
nail
neck brace
"
4	"from paddle.fluid.core import PaddleTensor, AnalysisConfig, create_paddle_predictor
from paddlehub.module.module import moduleinfo, runnable, serving
from paddlehub.common.paddle_helper import add_vars_prefix

from efficientnetb7_imagenet.processor import postprocess, base64_to_cv2
"
4	"        """"""
        self.arg_input_group.add_argument(
            '--input_path', type=str, help=""path to image."")

"
4	"            images=test_images, use_gpu=True, top_k=2)
        for res in results_4:
"
4	"guenon, guenon monkey
patas, hussar monkey, Erythrocebus patas
baboon
"
4	"paddlewheel, paddle wheel
padlock
paintbrush
pajama, pyjama, pj's, jammies
"
4	"import numpy as np


def base64_to_cv2(b64str):
    data = base64.b64decode(b64str.encode('utf8'))
"
2	"
log('Model: %s' % args.model)
log('Batch size: %d' % args.batch_size)
device = 'GPU' if args.cuda else 'CPU'
log('Number of %ss: %d' % (device, hvd.size()))
"
4	"        tmp = tmp / fluid.layers.sqrt(var + epsilon)
        tmp = fluid.layers.elementwise_add(tmp, offset, axis=1)
        return tmp
    else:
"
3	"    $("".pay"").addClass(""active"");
    $("".pay > .icon"").addClass(""active"");
    $("".choose"").removeClass(""active"");
    $("".wrap"").removeClass(""active"");
"
4	"    if len(x.shape) > 1:
        tmp = np.max(x, axis=1)
        x -= tmp.reshape((x.shape[0], 1))
        x = np.exp(x)
        tmp = np.sum(x, axis=1)
"
4	"snowmobile
snowplow, snowplough
soap dispenser
"
2	"        driver.register_task_to_task_addresses(next_task_index, task_to_task_addresses)

    for index in driver.task_indices():
        in_thread(notify_and_register, (index,))
"
1	"                    ] + ctc_weight * torch.from_numpy(
                        ctc_scores - hyp[""ctc_score_prev""]
                    )
"
1	"        self.fd.write(f""{key} {value}\n"")

"
4	"warplane, military plane
washbasin, handbasin, washbowl, lavabo, wash-hand basin
washer, automatic washer, washing machine
water bottle
water jug
"
4	"https://github.com/PaddlePaddle/PaddleClas

### 依赖

paddlepaddle >= 1.6.2
"
1	"    def forward(self, query, key, value, mask):
        """"""Forward of 'Dynamic Convolution'.

"
4	"                      bn_name=None):
        conv = conv2d(
            input=input,
            num_filters=num_filters,
            filter_size=filter_size,
"
2	"
    if hvd.rank() == 0:
            print(args.x_max, args.op, args.learning_rate, hvd.size(), step)

if __name__ == ""__main__"":
"
2	"
    def test_shutdown_during_request_basic(self):
        sleep = 2.0
        key = secret.make_secret_key()
"
4	"
import paddle.fluid as fluid


"
4	"        Args:
            images (list[numpy.ndarray]): data of images, shape of each is [H, W, C], color space must be BGR.
            paths (list[str]): The paths of images.
            batch_size (int): batch size.
"
4	"                 - 3: There are three data to be feeded in the model, e.g. the module is used for text matching task (pair-wise).


"
4	"            handle_id = iter_id * batch_size
            for image_id in range(batch_size):
                try:
"
0	"    if prop.add_border:
        # -- inset top face
        bmesh.ops.inset_region(
            bm, faces=top_face, thickness=prop.border, use_even_offset=True
"
3	"

def get_exist_path(directory, s):
    r = os.path.join(directory, s)
    if os.path.exists(r):
"
2	"        os.environ = old

"
1	"            raise NotImplementedError(
                f""--num-encs {args.num_encs} > 1 is not supported in --api v2""
            )
    else:
        raise ValueError(""Only chainer and pytorch are supported."")
"
4	"                exe = fluid.Executor(place)
                # pretrained
                if pretrained:

                    def _if_exist(var):
"
1	"-1.0598761	<unk>	0
0	<s>	-0.30103
-1.0598761	</s>	0
-0.8305393	I	-0.30103
"
2	"
def write(s):
    with open(filename_tmp, 'w') as f:
"
4	"Chesapeake Bay retriever
German short-haired pointer
vizsla, Hungarian pointer
"
2	"        named_parameters: A mapping between parameter names and values. Used for naming of
                          allreduce operations. Typically just ``model.named_parameters()``.
        compression: Compression algorithm used during allreduce to reduce the amount
                     of data sent during the each parameter update step.  Defaults to
                     not using compression.
"
6	"                          atol=1e-6)
    assert torch.allclose(jit2((out1, out1[:2]), adj.t(), adj.t()), out2[:2],
"
2	"img_sec_conf = 1.96 * np.std(state.img_secs)
log('Img/sec per %s: %.1f +-%.1f' % (device, img_sec_mean, img_sec_conf))
log('Total img/sec on %d %s(s): %.1f +-%.1f' %
"
4	"def EfficientNetB4(is_test=False,
                   padding_type='SAME',
                   override_params=None,
                   use_se=True):
    model = EfficientNet(
"
1	"            )
            self.acc = sum(map(lambda x: x[0] * x[1], zip(acc, ys_out_len))) / sum(
"
2	"        )

"
1	"                        MultiHeadedAttention(
                            attention_heads, attention_dim, attention_dropout_rate
                        ),
                        positionwise_layer(*positionwise_layer_args),
                        dropout_rate,
"
2	"        return task.fn_result()
    finally:
        # we must not call into shutdown too quickly, task clients run a command
        # and want to wait on the result, we have told task service not to return
"
4	"        offset_param = fluid.ParamAttr(
            name=offset_name,
"
4	"garbage truck, dustcart
gasmask, respirator, gas helmet
gas pump, gasoline pump, petrol pump, island dispenser
"
2	"      run: test-mixed-openmpi-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
"
4	"grand piano, grand
greenhouse, nursery, glasshouse
grille, radiator grille
grocery store, grocery, food market, market
"
4	"        """"""
        self.arg_config_group.add_argument(
            '--use_gpu',
"
2	"                             '%s' % ', '.join(str(id) for id in unnamed_param_ids))

"
3	"        req = urllib.request.Request(args.index_data_url, method=""HEAD"")
        response = urllib.request.urlopen(req, timeout=5)
        self.assertEqual(response.status, 200)
"
2	"#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
"
4	"            width_coefficient=w, depth_coefficient=d, dropout_rate=p)
    else:
"
2	"            return

        # Check for failures, and add them to the blacklisted hosts list
        failures = self.get(FAILURE)
        for host, slot in failures:
"
4	"cheetah, chetah, Acinonyx jubatus
brown bear, bruin, Ursus arctos
American black bear, black bear, Ursus americanus, Euarctos americanus
"
4	"    blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25',
        'r2_k3_s22_e6_i16_o24_se0.25',
"
2	"      run: test-cpu-openmpi-py3_6-tf2_0_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
"
2	"                    spark_task_service(index=0, key=key) as (task0, _, _), \
                    spark_task_service(index=1, key=key) as (task1, _, _), \
                    spark_task_service(index=2, key=key) as (task2, _, _):
                self.assertIsNone(task0.fn_result())
"
4	"        output.append(output_i)
    return output

"
4	"        name='b0',
        is_test=is_test,
"
4	"shopping cart
shovel
"
4	"def context(trainable=True, pretrained=True)
```

**参数**

"
4	"            filter_size=1,
            bn_act='swish',
"
4	"dam, dike, dyke
desk
desktop computer
dial telephone, dial phone
"
4	"                    exe.run(startup_prog)
                # trainable
                for param in context_prog.global_block().iter_parameters():
                    param.trainable = trainable
"
2	"    if key in exit_schedule:
        ranks_to_exit = exit_schedule[key]
        if start_rank in ranks_to_exit:
"
1	"            hyp[""ctc_score_prev""] = 0.0
            if ctc_weight != 1.0:
"
6	"    assert out1.size() == (4, 32)
    assert torch.allclose(conv(x, adj1.t()), out1, atol=1e-6)
    out2 = conv(x, edge_index, value)
    assert out2.size() == (4, 32)
"
2	"REGISTER_KERNEL_BUILDER(
    Name(""HorovodLocalRank"").Device(DEVICE_GPU).HostMemory(""local_rank""),
"
1	"            None
            if postnet_layers == 0
            else Postnet(
                idim=idim,
"
4	"                label_list=self.label_list,
                top_k=top_k)
            res += out
        return res
"
4	"import json
import cv2
"
4	"brain coral
flatworm, platyhelminth
nematode, nematode worm, roundworm
conch
snail
"
2	"    automatic: true
  agents:
"
4	"## 命令行预测

```
"
2	"
REGISTER_OP(""HorovodJoin"")
    .Doc(R""doc(
Perform an join on a tensor,
)doc"");
"
4	"
        # Check stride
        assert (('s' in options and len(options['s']) == 1) or
                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))

"
2	"            str((1, 0)): [0, 1, 2, 3],
        }

        message = 'Horovod detected that one or more processes exited with non-zero status'
        with pytest.raises(RuntimeError, match=message):
"
4	"        'r1_k3_s11_e6_i192_o320_se0.25',
    ]
"
4	"redbone
borzoi, Russian wolfhound
Irish wolfhound
Italian greyhound
"
4	"        if top_padding != bottom_padding or left_padding != right_padding:
            height_padding = top_padding + stride
            width_padding = left_padding + stride
            need_crop = True
        padding = [height_padding, width_padding]
"
2	"                                    'may need to increase the --start-timeout '
                                    'parameter if you have too many servers.')
    settings = elastic_settings.ElasticSettings(discovery=discover_hosts,
                                                min_np=args.min_np or args.np,
"
2	"                'is_sparse_vector_only': False,
                'shape': None,  # Only used by SparseVector columns
                'intermediate_format': constants.NOCHANGE,
                'max_size': None  # Only used by SparseVector columns
"
4	"            text_1 = fluid.layers.data(
                name=""text_1"",
                shape=[-1, max_seq_len, 1],
"
4	"    for element in component:
        element['image'] = process_image(element['org_im'])
        yield element

"
4	"        with fluid.program_guard(context_prog, startup_prog):
            with fluid.unique_name.guard():
                image = fluid.layers.data(
"
6	"    jit = torch.jit.script(conv.jittable(t))
    assert jit(x, edge_index).tolist() == out1.tolist()
    assert jit(x, edge_index, value).tolist() == out2.tolist()

"
2	">         # 1. Standalone Cluster
>         # - Must configure spark.worker.* configs as below.
>         #
>         # 2. YARN
>         # - Requires YARN 3.1 or higher to support GPUs
"
4	"Madagascar cat, ring-tailed lemur, Lemur catta
indri, indris, Indri indri, Indri brevicaudatus
"
2	"        if settings.nics:
            # If args.nics is provided, we will use those interfaces. All the workers
            # must have at least one of those interfaces available.
"
4	"wolf spider, hunting spider
tick
centipede
black grouse
"
2	"  plugins:
  - docker-compose#v2.6.0:
"
0	"floorplan = btools.core.floorplan
builder = floorplan.floorplan.Floorplan

"
1	"        # Configurations used in the first paper are in
        # egs/{csj, librispeech}/asr1/conf/tuning/ld_conv/
        parser.add_argument(
"
4	"```shell
$ hub serving start -m efficientnetb6_imagenet
"
2	"                print('host changes: {} -> {}'.format(current_hosts, next_hosts))
                start = int(time.time())
"
2	"        # Cache that provides the store
        self.cache_lock = threading.Lock()
        self.cache = {}

"
6	"

"
2	"                    help='disables CUDA training')

args = parser.parse_args()
"
2	"        _mkdir_p(settings.output_filename)

    driver = ElasticDriver(rendezvous, settings.discovery,
"
4	"half track
hammer
hamper
"
4	"            padding_type=self.padding_type,
            bn_eps=self._bn_eps,
            name='',
"
6	"    assert conv.__repr__() == 'CGConv((8, 16), dim=0)'
    out = conv((x1, x2), edge_index)
    assert out.size() == (2, 16)
"
4	"Cardigan, Cardigan Welsh corgi
toy poodle
"
4	"# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
"
4	"            name=name,
            conv_name=name + '_project_conv',
            bn_name='_bn2')
"
4	"        is_test=is_test,
        padding_type=padding_type,
        override_params=override_params,
        use_se=use_se)
"
4	"        op_type='conv',
        fan_out=num_filters,
"
2	"            nics = settings.nics
        else:
            # Find the set of common, routed interfaces on all the hosts (remote
"
2	"            procs = spark_context.range(0, numSlices=settings.max_np if settings.elastic else settings.num_proc)
            # We assume that folks caring about security will enable Spark RPC encryption,
            # thus ensuring that key that is passed here remains secret.
            mapper = _make_mapper(driver.addresses(), settings, use_gloo, is_elastic)
"
4	"combination lock
computer keyboard, keypad
confectionery, confectionary, candy store
container ship, containership, container vessel
"
5	"        # import pdb
        # pdb.set_trace()
"
2	"  command: bash -c "" \$(cat /mpirun_command) python /horovod/examples/pytorch_mnist.py""
  plugins:
"
4	"warplane, military plane
washbasin, handbasin, washbowl, lavabo, wash-hand basin
"
2	"- label: ':pytest: Run PyTests (test-cpu-mpich-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0)'
  command: bash -c "" cd /horovod/test && (echo test_*.py | sed 's/[a-z_]*tensorflow2[a-z_.]*//g' | sed 's/test_interactiverun.py//g' | sed 's/test_spark_keras.py//g' | sed 's/test_spark_torch.py//g' | sed 's/test_spark.py//g' | sed 's/test_run.py//g' | xargs -n 1 \$(cat /mpirun_command) pytest -v --capture=no) && pytest --forked -v --capture=fd test_spark.py test_run.py""
  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-mpich-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
"
2	"

class WorkerNotificationClient(network.BasicClient):
    def __init__(self, addresses, key, verbose, match_intf=False):
"
3	"
        :param text: the raw text
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
"
4	"from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
"
0	"

def get_mod_name(filepath):
    """"""Return the module name and the root path of the given python file path.""""""
"
2	"# Copyright 2019 Uber Technologies, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
"
3	"            _volumes = {os.path.dirname(os.path.abspath(readme_path)): {'bind': '/workspace'}}
            _env = get_default_login()
"
1	"#!/usr/bin/env python3
# encoding: utf-8

"
4	"    return model


def EfficientNetB3(is_test=False,
                   padding_type='SAME',
"
4	"# limitations under the License.

"
2	"                    help='number of batches per benchmark iteration')
parser.add_argument('--num-iters', type=int, default=10,
                    help='number of benchmark iterations')
"
4	"kimono
knee pad
knot
"
2	"from horovod.run.elastic import constants
from horovod.run.runner import parse_args, _run_elastic

"
2	"    m3 = 10.
    m2 = 5.
    m1 = -20.
    m0 = -5.
    return m3*x*x*x + m2*x*x + m1*x + m0
"
4	"clumber, clumber spaniel
English springer, English springer spaniel
Welsh springer spaniel
cocker spaniel, English cocker spaniel, cocker
"
2	"#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
"
4	"import paddlehub as hub


class EfficientNetB3TestCase(TestCase):
"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
"
2	"  retry:
    automatic: true
  agents:
"
1	"if [ ! -f ""${wav}"" ]; then
    echo ""No such WAV file: ${wav}""
    exit 1
fi
"
4	"American black bear, black bear, Ursus americanus, Euarctos americanus
ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus
sloth bear, Melursus ursinus, Ursus ursinus
mongoose
meerkat, mierkat
"
3	"    $(""#second"").removeClass(""active"");
    $(""#third"").addClass(""active"");
    $(""#fourth"").removeClass(""active"");
})
"
2	"              workers.
    """"""
    from tensorflow.python.framework.errors_impl import UnknownError

"
4	"#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
"
2	"  timeout_in_minutes: 5
  retry:
"
3	"                    c.embedding.CopyFrom(array2pb(np.random.random([embed_dim])))
                    c.chunk_id = c_id
                    c.doc_id = j
                    c_id += 1
"
4	"
        return conv

    def se_block(self, inputs, num_squeezed_channels, oup, name):
"
4	"Bernese mountain dog
Appenzeller
EntleBucher
boxer
"
6	"    def message(self, x_j: Tensor, x_norm_i: Tensor, x_norm_j: Tensor,
                index: Tensor, ptr: OptTensor,
                size_i: Optional[int]) -> Tensor:
"
4	"def get_pretrained_images_std()
```

返回预处理的图片标准差，也就是 \[0.229, 0.224, 0.225\]。
"
6	"
    t = '(Tensor, Tensor, OptTensor, Size) -> Tensor'
"
4	"        return output

    def _expand_conv_norm(self, inputs, block_args, is_test, name=None):
        # Expansion phase
        oup = block_args.input_filters * block_args.expand_ratio  # number of output channels
"
2	"                    if len(self._discovery_schedule) == 1:
                        del self._discovery_schedule[None]

"
2	"                    .apply(tf.data.experimental.unbatch()) \
                    .shuffle(int(train_rows / hvd.size())) \
                    .batch(args.batch_size) \
                    .map(lambda x: (tuple(getattr(x, col) for col in all_cols), tf.log(x.Sales)))

"
4	"            self.label_list = file.read().split(""\n"")[:-1]
        self.classification = self.classify
"
4	"    elif act == 'leaky_relu':
        conv = fluid.layers.leaky_relu(
            conv, alpha=relufactor, name=name + '_leaky_relu')
    elif act == 'tanh':
"
4	"                 dropout_rate=0.2,
                 drop_connect_rate=0.2):
    """""" Get block arguments according to parameter and coefficients. """"""
"
4	"airliner
airship, dirigible
"
4	"measuring cup
medicine chest, medicine cabinet
megalith, megalithic structure
"
3	"class FastICAEncoder(BaseNumericEncoder):
    """"""
    :class:`FastICAEncoder` encodes data from an ndarray in size `B x T` into an ndarray in size `B x D`.

"
2	"            (None, ['localhost:2', '127.0.0.1:2']),
        ]

"
2	"# Copyright 2020 Uber Technologies, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
"
4	"
        return BlockArgs(
            kernel_size=int(options['k']),
"
4	"walking stick, walkingstick, stick insect
cockroach, roach
"
4	"studio couch, day bed
stupa, tope
submarine, pigboat, sub, U-boat
suit, suit of clothes
sundial
"
3	"            rate = failed / success
            self.assertTrue(rate < 0.1)
"
4	"sombrero
soup bowl
space bar
space heater
"
2	"void NCCLOpContext::AsyncErrorCheck() {
  ncclResult_t nccl_async_err;
  auto nccl_err = ncclCommGetAsyncError(*nccl_comm_, &nccl_async_err);
  if (nccl_err != ncclSuccess) {
    throw std::logic_error(std::string(""ncclGetAsyncError failed: "") + ncclGetErrorString(nccl_err));
"
2	"       fi
    else
       oneccl_env=""""
"
4	"def get_expected_image_width()
```
"
6	"from typing import Union, Tuple, Callable
from torch_geometric.typing import OptTensor, OptPairTensor, Adj, Size

"
4	"        results = self.classify(images=images_decode, **kwargs)
        return results

    @runnable
    def run_cmd(self, argvs):
"
2	"                           '2. Run distributed '
                           'training script using the standard way provided by your'
                           ' MPI distribution (usually mpirun, srun, or jsrun).\n'
"
2	"    automatic: true
  agents:
"
1	"        if key == ""a"":
            assert all((data[""data8""]) == np.array([1.4, 3.4], dtype=np.float32))
        if key == ""b"":
            assert all((data[""data8""]) == np.array([0.9, 9.3], dtype=np.float32))
"
2	"  plugins:
  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
"
1	"        return loss, stats, weight

"
4	"        use_se=use_se)
    return model


"
2	"        time.sleep(sleep / 2.0)
        service.shutdown()
        duration = time.time() - start
        print('shutdown completed in {} seconds'.format(duration))
"
6	"                 pre_transform=None, pre_filter=None, use_node_attr=False,
                 use_edge_attr=False, cleaned=False):
        self.name = name
        assert self.name in self.names

"
4	"mixing bowl
mobile home, manufactured home
"
4	"studio couch, day bed
stupa, tope
submarine, pigboat, sub, U-boat
suit, suit of clothes
"
4	"                         override_params=None,
                         use_se=False):
    model = EfficientNet(
"
2	"            self._lock.acquire()
            try:
                return TaskIndexByRankResponse(self._ranks_to_indices[req.rank])
            finally:
"
4	"        'efficientnet-b1': (1.0, 1.1, 240, 0.2),
        'efficientnet-b2': (1.1, 1.2, 260, 0.3),
        'efficientnet-b3': (1.2, 1.4, 300, 0.3),
"
4	"                name=name,
                conv_name=name + '_expand_conv',
"
2	"- label: ':fire: Single PyTorch MNIST (test-cpu-gloo-py3_8-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark3_0_0)'
  command: bash -c "" python /horovod/examples/pytorch_mnist.py --epochs 3""
  plugins:
"
4	"            description=""Run the {} module."".format(self.name),
            prog='hub run {}'.format(self.name),
            usage='%(prog)s',
"
4	"            input_filters=int(options['i']),
            output_filters=int(options['o']),
"
2	"  timeout_in_minutes: 5
  retry:
    automatic: true
"
1	"
    if linenum + 1 < num_splits:
"
3	"
        if ref_indexer:
            # copy the header info of the binary file
            self.num_dim = ref_indexer.num_dim
"
4	"Yorkshire terrier
wire-haired fox terrier
Lakeland terrier
"
4	"miniature schnauzer
giant schnauzer
standard schnauzer
"
2	"      run: test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
"
1	"    parser.add_argument(""--ngpu"", type=int, default=0, help=""Number of GPUs"")
    parser.add_argument(
"
2	"      run: test-cpu-gloo-py3_8-tf2_2_0-keras2_3_1-torch1_5_0-mxnet1_5_0-pyspark3_0_0
      config: docker-compose.test.yml
"
1	"            if len(args.train_shape_file) != 0:
                train_key_file = args.train_shape_file[0]
            else:
"
4	"**参数**

* trainable (bool): 计算图的参数是否为可训练的；
"
4	"steel arch bridge
steel drum
"
4	"gown
grand piano, grand
"
2	"  command: bash -c "" \$(cat /mpirun_command) python /horovod/examples/tensorflow_mnist_eager.py""
  plugins:
  - docker-compose#v2.6.0:
      run: test-cpu-mpich-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_5_0-pyspark2_4_0
      config: docker-compose.test.yml
"
2	"
    task = task_service.SparkTaskService(index, settings.key, settings.nics,
"
4	"crash helmet
crate
crib, cot
Crock Pot
croquet ball
"
4	"DATA_DIM = 224
img_mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))
img_std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))
"
2	"  command: bash -c ""cd /horovod/test && (echo test_*.py | sed 's/[a-z_]*tensorflow2[a-z_.]*//g' | sed 's/test_interactiverun.py//g' | sed 's/test_spark_keras.py//g' | sed 's/test_spark_torch.py//g' | sed 's/test_spark.py//g' | sed 's/test_run.py//g' | xargs -n 1 horovodrun -np 2 -H localhost:2 --gloo pytest -v --capture=no) && pytest --forked -v --capture=fd test_spark.py test_run.py""
  plugins:
  - docker-compose#v2.6.0:
      run: test-gpu-openmpi-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
      config: docker-compose.test.yml
"
4	"from __future__ import division

import ast
"
4	"                name=name + '_offset',
                initializer=fluid.initializer.Uniform(low=-bound, high=bound))
        else:
            bias_attr = False
    elif init == 'google':
"
2	"  timeout_in_minutes: 5
  retry:
"
2	"    automatic: true
  agents:
"
4	"Tibetan terrier, chrysanthemum dog
silky terrier, Sydney silky
soft-coated wheaten terrier
West Highland white terrier
Lhasa, Lhasa apso
"
4	"    resized_height = int(round(img.size[1] * percent))
    img = img.resize((resized_width, resized_height), Image.LANCZOS)
    return img
"
4	"
**参数**

* trainable (bool): 计算图的参数是否为可训练的；
* pretrained (bool): 是否加载默认的预训练模型。
"
2	"
    def _set_attrs(self):
        for attr, value in self._saved_state.items():
            setattr(self, attr, value)

"
2	"    # we need the driver's PATH and PYTHONPATH in env to run mpirun,
    # env for mpirun is different to env encoded in mpirun_command
    for var in ['PATH', 'PYTHONPATH']:
        if var not in env and var in os.environ:
"
0	"        mod = os.path.splitext(mod)[0]

    return mod, dir
"
2	"        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
"
4	"            idx += 1
            if block_args.num_repeat > 1:
                block_args = block_args._replace(
                    input_filters=block_args.output_filters, stride=1)
            for _ in range(block_args.num_repeat - 1):
"
4	"        conv = fluid.layers.sigmoid(conv, name=name + '_sigmoid')
    elif act == 'swish':
        conv = fluid.layers.swish(conv, name=name + '_swish')
    elif act == None:
"
2	"  - docker-compose#v2.6.0:
      run: test-gpu-gloo-py3_6-tf1_15_0-keras2_3_1-torch1_4_0-mxnet1_4_1-pyspark2_4_0
"
4	"electric locomotive
entertainment center
envelope
espresso maker
face powder
"
4	"wall clock
wallet, billfold, notecase, pocketbook
wardrobe, closet, press
warplane, military plane
washbasin, handbasin, washbowl, lavabo, wash-hand basin
"
2	"      config: docker-compose.test.yml
      pull-retries: 3
  - ecr#v1.2.0:
      login: true
  timeout_in_minutes: 5
"
3	"zlib: zLib License

"
4	"dishrag, dishcloth
dishwasher, dish washer, dishwashing machine
disk brake, disc brake
"
2	"    automatic: true
  agents:
    queue: cpu
"
1	"            attention_dim=adim,
            attention_heads=aheads,
"
4	"abacus
abaya
academic gown, academic robe, judge's robe
"
2	"            self.assertEqual(0, results[epoch]['rank'])
            self.assertEqual(0, results[epoch]['start_rank'])
            self.assertEqual(5, results[epoch]['size'])
            self.assertEqual(1, results[epoch]['rendezvous'])
"
2	"        cls = type(optimizer.__class__.__name__, (optimizer.__class__,),
                   dict(_DistributedAdasumOptimizer.__dict__))
        return cls(optimizer.param_groups, named_parameters, compression, backward_passes_per_step)

"
4	"# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"
0	"
        verts = context.object.data.vertices
"
2	"      login: true
  timeout_in_minutes: 5
  retry:
    automatic: true
  agents:
"
4	"
        return BlockArgs(
"
4	"Arctic fox, white fox, Alopex lagopus
grey fox, gray fox, Urocyon cinereoargenteus
tabby, tabby cat
tiger cat
Persian cat
"
4	"                dtype='float32',
                param_attr=w_param_attrs)
            emb_1_name = emb_1.name
            data_list = [text_1]
"
4	"killer whale, killer, orca, grampus, sea wolf, Orcinus orca
dugong, Dugong dugon
sea lion
"
2	"  }
}
#else
void server_affinity_set(int affinity) {
"
2	"
import functools
import queue
"
4	"import paddlehub as hub


"
1	"        for i in range(n_utts):
            text.append(utts[i] + "" "" + txts[i])
            utt2spk.append(utts[i] + "" Bruce\n"")
            wav.append(utts[i] + "" "" + str(i) + "".wav\n"")

"
4	"    resized_height = int(round(img.size[1] * percent))
    img = img.resize((resized_width, resized_height), Image.LANCZOS)
    return img
"
3	"  - toy-example

"
6	"    assert jit((x1, None), adj.t()).tolist() == out2.tolist()
    jit.fuse = False
"
